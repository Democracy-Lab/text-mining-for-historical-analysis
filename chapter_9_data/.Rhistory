ggplot(top_words, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top 10 Words in DOJ Blog Entries",
x = "Word",
y = "Frequency") +
theme_minimal()
library(rvest)
library(tidyverse)
library(xml2)
rss_url <- "https://feeds.bbci.co.uk/news/rss.xml"
rss_page <- read_xml(rss_url)
articles <- tibble(
title = rss_page %>%
xml_find_all("//item/title") %>% xml_text(),
link = rss_page %>%
xml_find_all("//item/link") %>% xml_text())
print(head(articles, 5))
library(tidyverse)
library(tidytext)
data(stop_words)
# Unnest tokens, remove stop words, count words, and select the top 20
top_words <- tidy_egils_saga %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 20)
head(tidy_egils_saga)
library(httr)
library(fs)
library(pdftools)
library(tidyverse)
# Define the URL with the PDF to be downloaded
url <- "https://sagadb.org/files/pdf/egils_saga.en.pdf"
# Define the directory it will be downloaded to on one's computer
# The file will be downloaded to folder named "tmha_data"
destfile <- path("tmha_data", "egils_saga.en.pdf")
# Create directory if it doesn't exist
dir_create(path_dir(destfile))
# Download the PDF file
GET(url, write_disk(destfile, overwrite = TRUE))
# Extract text from the PDF
pdf_text_raw <- pdf_text(destfile)
# Convert into a tibble with one row per page
tidy_egils_saga <- tibble(page = seq_along(pdf_text_raw),
text = pdf_text_raw)
head(tidy_egils_saga)
library(tidyverse)
library(tidytext)
data(stop_words)
# Unnest tokens, remove stop words, count words, and select the top 20
top_words <- tidy_egils_saga %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 20)
# Visualize top 20 words
top_words %>% ggplot(aes(x = reorder(word, n), y = n)) +
geom_col() +
coord_flip() +
labs(title = "Top 20 Words in Egil's Saga (after removing stop words)",
x = "Word",
y = "Count") +
theme_minimal()
tidy_egils_saga_with_metadata <- tidy_egils_saga %>%
mutate(author = "Snorri Sturluson",
book = "Egil's Saga",
date = "1893")
library(gt)
gt(head(tidy_egils_saga_with_metadata))
library(rvest)
library(dplyr)
url <- "https://github.com/stephbuon/text-mining-for-historical-analysis"
page <- read_html(url)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(httr)
library(jsonlite)
# Define the JSON API endpoint
url <- "https://www.dallasopendata.com/resource/7h2m-3um5.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
animal_shelter_data <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
# Show just the first five columns
head(animal_shelter_data[, 1:5])
json_list <- fromJSON(json_content)
subset_list <- json_list[1:2, ]
cat(prettify(toJSON(subset_list, auto_unbox = TRUE)))
nrow(json_list)
url <- "https://www.dallasopendata.com/resource/7h2m-3um5.json"
# Pagination settings
limit <- 1000
# Will store the output from each iteration
animal_shelter_data <- tibble()
# Run for two iterations: first with offset = 0, then with offset = 1000.
# This means the code will first collect rows 1–1000,
# and then collect rows 1001–2000 on the second iteration.
for (i in 0:1) {
offset <- i * limit
# Make paginated request
response <- GET(url, query = list(`$limit` = limit, `$offset` = offset))
if (status_code(response) != 200) {
stop("Request failed. Status code: ", status_code(response)) }
# Parse the response
data_subset <- fromJSON(content(response, "text"), flatten = TRUE)
# Save collected data
animal_shelter_data <- bind_rows(animal_shelter_data, as_tibble(data_subset)) }
library(usdoj)
blog_entries <- doj_blog_entries(n_results = 10, search_direction = "DESC")
head(blog_entries$teaser, 5)
library(tidyverse)
library(tidytext)
# Tokenize, remove stop words, and count word frequency
top_words <- blog_entries %>%
select(body) %>%
unnest_tokens(word, body) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 10)
# Plot the top words
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top 10 Words in DOJ Blog Entries",
x = "Word",
y = "Frequency") +
theme_minimal()
library(rvest)
library(tidyverse)
library(xml2)
rss_url <- "https://feeds.bbci.co.uk/news/rss.xml"
rss_page <- read_xml(rss_url)
articles <- tibble(
title = rss_page %>%
xml_find_all("//item/title") %>% xml_text(),
link = rss_page %>%
xml_find_all("//item/link") %>% xml_text())
print(head(articles, 5))
library(httr)
library(fs)
library(pdftools)
library(tidyverse)
# Define the URL with the PDF to be downloaded
url <- "https://sagadb.org/files/pdf/egils_saga.en.pdf"
# Define the directory it will be downloaded to on one's computer
# The file will be downloaded to folder named "tmha_data"
destfile <- path("tmha_data", "egils_saga.en.pdf")
# Create directory if it doesn't exist
dir_create(path_dir(destfile))
# Download the PDF file
GET(url, write_disk(destfile, overwrite = TRUE))
# Extract text from the PDF
pdf_text_raw <- pdf_text(destfile)
# Convert into a tibble with one row per page
tidy_egils_saga <- tibble(page = seq_along(pdf_text_raw),
text = pdf_text_raw)
head(tidy_egils_saga)
library(tidyverse)
library(tidytext)
data(stop_words)
# Unnest tokens, remove stop words, count words, and select the top 20
top_words <- tidy_egils_saga %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 20)
# Visualize top 20 words
top_words %>% ggplot(aes(x = reorder(word, n), y = n)) +
geom_col() +
coord_flip() +
labs(title = "Top 20 Words in Egil's Saga (after removing stop words)",
x = "Word",
y = "Count") +
theme_minimal()
tidy_egils_saga_with_metadata <- tidy_egils_saga %>%
mutate(author = "Snorri Sturluson",
book = "Egil's Saga",
date = "1893")
library(gt)
gt(head(tidy_egils_saga_with_metadata))
library(rvest)
library(dplyr)
url <- "https://github.com/stephbuon/text-mining-for-historical-analysis"
page <- read_html(url)
# Convert to character (raw HTML), split into lines, and preview a few lines
html_preview <- page %>%
as.character() %>%
str_split("\n") %>%
unlist() %>%
str_trim() %>%
.[1:5]
# Optionally truncate long lines to fit the page
html_preview <- str_trunc(html_preview, width = 100)
# Print nicely
cat(paste(html_preview, collapse = "\n"))
library(hansardr)
data("speaker_metadata_1840")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0051P0_728")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0051P0_18193")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194") %>%
mutate(speaker = str_replace_all(speaker, "'", ""))
library(stringr)
# Messy string
messy_string <- " <p>We!!!   can clean   messy--<b> text using </b>.. ,    regular expressions </p> "
# Clean it with a complex regex pipeline that
# removes all HTML tags
# removes all non-word and non-space characters
# replaces multiple spaces with a single space
# trims leading/trainign spaces
clean_string <- messy_string |>
str_remove_all("<[^>]+>") |>
str_replace_all("[^\\w\\s]", "") |>
str_replace_all("\\s+", " ") |>
str_trim()
print(clean_string)
library(stringr)
# Remove HTML tags
messy_string <- gsub("<[^>]+>", "", messy_string)
# Replace multiple punctuation marks with a space (put hyphen first or last to avoid range error)
messy_string <- gsub("[-!.,]+", " ", messy_string)
# Collapse multiple spaces and trim
text <- str_squish(messy_string)
print(text)
my_number = "42"
typeof(my_number)
my_number + 2
my_number <- as.numeric(my_number)
my_number + 2
library("hansardr")
data("hansard_1800")
typeof("hansard_1800$text")
knitr::include_graphics("pipe.png")
library(tidyverse)
data("msleep")
msleep_subset <- msleep %>%
select(name, conservation, sleep_total) %>%
slice(1:10)
print(msleep_subset)
msleep_subset %>%
mutate(is_carnivore = str_detect(conservation, "domesticated"))
msleep_subset %>%
mutate(is_missing = is.na(conservation))
library(tidyverse)
# Replace NA in ""conservation" with the string "missing"
msleep_subset <- msleep %>%
select(name, conservation) %>%
mutate(conservation = replace_na(conservation, "missing"))
# View the updated data
msleep_subset %>%
slice(1:10)
continents_population <- list('South America', 644.54, 'Africa', 1305, 'Europe', 745.64, 'Asia', 4587)
typeof(continents_population)
for (item in continents_population) {
print(typeof(item))}
### Code doesn't show; only table
library(gt)
# Create a data frame with the structure
text_structures <- tibble::tibble(
Structure = c(
"Matrix", "List", "Data Frame", "Tibble", "S3 Object", "S4 Object"
),
Dimensions = c(
"2D", "1D", "2D", "2D", "Varies", "Varies"
),
Homogeneous = c(
"Yes", "No", "No", "No", "No", "No"
),
`Key Use for Text Mining` = c(
"Numeric structure for word embeddings or term-document counts",
"Flexible storage for mixed or uniform data—e.g., texts, dates, names",
"Standard tabular format for digitized records or structured text metadata",
"Tidyverse-friendly data frames optimized for large data and printing",
"Used in packages like `quanteda`, `stm` with custom print/access behavior",
"Formal, slot-based system used in `tm`, `topicmodels`, with accessor functions"
),
Example = c(
"`matrix(rnorm(300), nrow = 100, ncol = 3)`",
"`list(1800, \"text\", \"author\")`",
"`data.frame(name = ..., date = ...)`",
"`tibble(year, text)`",
"`class(tokens_object) → \"tokens\"`",
"`class(corpus_object) → \"Corpus\", \"VCorpus\"`"
)
)
# Generate the gt table
text_structures %>%
gt() %>%
tab_header(
title = "Common R Data Structures in Text Mining"
) %>%
cols_label(
Structure = "Structure",
Dimensions = "Dimensions",
Homogeneous = "Homogeneous",
`Key Use for Text Mining` = "Key Use for Text Mining",
Example = "Example"
) %>%
fmt_markdown(columns = everything())
library(quanteda)
library(tidyverse)
library(hansardr)
data("hansard_1800")
texts <- hansard_1800$text
# Create a tokens object from the text column
tokens_hansard <- tokens(texts)
# Perform a KWIC search on the tokens object
kwic_result <- kwic(tokens_hansard, pattern = "corn", window = 5)
gt(head(kwic_result))
slice(tokens_hansard, 1:5)
tokens_tibble <- tibble(
doc_id = rep(names(tokens_hansard), lengths(tokens_hansard)),
token = unlist(tokens_hansard, use.names = FALSE))
slice(tokens_tibble, 1:5)
library(gt)
library(tibble)
library(dplyr)
# --- Quanteda ---
quanteda_tbl <- tibble(
Task = c(
"Convert dfm to tibble",
"Get token list",
"Combine with metadata",
"Tidy version"
),
`What to do` = c(
"`dfm %>% convert(to = \"data.frame\") %>% as_tibble()`",
"`tokens %>% as.list()`",
"`docvars(dfm)` gives doc-level covariates",
"`Use quanteda.textmodels::convert()` or `tidytext::tidy()` if applicable"
)
)
print(quanteda_tbl %>%
gt() %>%
tab_header(title = "✅ Quanteda (tokens, dfm, etc.)") %>%
fmt_markdown(columns = everything()))
cat("### \n\n", sep = "\n")
# --- tm ---
tm_tbl <- tibble(
Task = c(
"Convert Corpus",
"DTM to tidy format",
"Combine with meta"
),
`What to do` = c(
"`sapply(corpus, as.character)` or `content(corpus)`",
"`tidy(DTM)` from `tidytext`",
"`meta(corpus)` or `tm_map()`"
)
)
tm_tbl %>%
gt() %>%
tab_header(title = "✅ tm (Corpus, DocumentTermMatrix)") %>%
fmt_markdown(columns = everything())
# --- text2vec ---
text2vec_tbl <- tibble(
Task = c(
"Convert DTM",
"Vocabulary to tibble",
"Embeddings to tibble"
),
`What to do` = c(
"`as.matrix(dtm) %>% as_tibble()` or `Matrix::summary(dtm)`",
"`vocab$term_stats %>% as_tibble()`",
"`as_tibble(embeddings)`"
)
)
print(text2vec_tbl %>%
gt() %>%
tab_header(title = "✅ text2vec (itoken, dtm, etc.)") %>%
fmt_markdown(columns = everything()))
# --- topicmodels / stm ---
stm_tbl <- tibble(
Task = c(
"Tidy topics + terms",
"Tidy docs + topics",
"STM effects"
),
`What to do` = c(
"`tidy(model, matrix = \"beta\")` (from `broom`)",
"`tidy(model, matrix = \"gamma\")`",
"`Use estimateEffect()` then extract manually"
)
)
print(stm_tbl %>%
gt() %>%
tab_header(title = "✅ topicmodels / stm (LDA/STMs)") %>%
fmt_markdown(columns = everything()))
# --- Sparse matrices ---
sparse_tbl <- tibble(
Task = c(
"Convert to dense",
"Extract non-zeros",
"Tidy-friendly format"
),
`What to do` = c(
"`as.matrix(sparse_mat)`",
"`Matrix::summary(sparse_mat)`",
"`Use reshape2::melt()` or `pivot_longer()`"
)
)
print(sparse_tbl %>%
gt() %>%
tab_header(title = "✅ Sparse matrices (e.g., dgCMatrix)") %>%
fmt_markdown(columns = everything()))
# --- Tidyverse Tools ---
tidyverse_tools_tbl <- tibble(
`Tool/Package` = c(
"tidytext",
"textrecipes",
"tidylo, textdata",
"unnest_tokens()",
"broom / broom.mixed"
),
Use = c(
"Makes many text objects tidy (DTMs, LDA, etc.)",
"Tidy preprocessing (e.g., tokenization, tf-idf)",
"Additional tidy tools for text and lexicons",
"From `tidytext`, best for converting raw text to tokens",
"Tidy output from models (including `topicmodels`)"
)
)
print(tidyverse_tools_tbl %>%
gt() %>%
tab_header(title = "Helpful Tidyverse-Compatible Tools") %>%
fmt_markdown(columns = everything()))
# Knit in the document directory (helps Pandoc find local files)
knitr::opts_knit$set(root.dir = dirname(knitr::current_input()))
knitr::opts_chunk$set(echo = TRUE)
# Knit in the document directory (helps Pandoc find local files)
knitr::opts_knit$set(root.dir = dirname(knitr::current_input()))
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir if we are actually knitting a file (not running interactively)
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble.tex exists right next to this Rmd (safety net)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}"
), "preamble.tex")
}
# Sanitize curly quotes and stray latex macros in all printed output
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)      # ’ ‘ → '
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)     # “ ” → "
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)     # latex macro → '
if (is.function(hook_out)) hook_out(x, options) else x
})
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
