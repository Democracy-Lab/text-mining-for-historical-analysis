library(rvest)
library(dplyr)
url <- "https://github.com/stephbuon/text-mining-for-historical-analysis"
page <- read_html(url)
# Convert to character (raw HTML), split into lines, and preview a few lines
html_preview <- page %>%
as.character() %>%
str_split("\n") %>%
unlist() %>%
str_trim() %>%
.[1:5]
# Optionally truncate long lines to fit the page
html_preview <- str_trunc(html_preview, width = 100)
# Print nicely
cat(paste(html_preview, collapse = "\n"))
library(hansardr)
data("speaker_metadata_1840")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0051P0_728")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0051P0_18193")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194") %>%
mutate(speaker = str_replace_all(speaker, "'", ""))
library(stringr)
# Messy string
messy_string <- " <p>We!!!   can clean   messy--<b> text using </b>.. ,    regular expressions </p> "
# Clean it with a complex regex pipeline that
# removes all HTML tags
# removes all non-word and non-space characters
# replaces multiple spaces with a single space
# trims leading/trainign spaces
clean_string <- messy_string |>
str_remove_all("<[^>]+>") |>
str_replace_all("[^\\w\\s]", "") |>
str_replace_all("\\s+", " ") |>
str_trim()
print(clean_string)
library(stringr)
# Remove HTML tags
messy_string <- gsub("<[^>]+>", "", messy_string)
# Replace multiple punctuation marks with a space (put hyphen first or last to avoid range error)
messy_string <- gsub("[-!.,]+", " ", messy_string)
# Collapse multiple spaces and trim
text <- str_squish(messy_string)
print(text)
my_number = "42"
typeof(my_number)
my_number + 2
my_number <- as.numeric(my_number)
my_number + 2
library("hansardr")
data("hansard_1800")
typeof("hansard_1800$text")
knitr::include_graphics("pipe.png")
library(tidyverse)
data("msleep")
msleep_subset <- msleep %>%
select(name, conservation, sleep_total) %>%
slice(1:10)
print(msleep_subset)
msleep_subset %>%
mutate(is_carnivore = str_detect(conservation, "domesticated"))
msleep_subset %>%
mutate(is_missing = is.na(conservation))
library(tidyverse)
# Replace NA in ""conservation" with the string "missing"
msleep_subset <- msleep %>%
select(name, conservation) %>%
mutate(conservation = replace_na(conservation, "missing"))
# View the updated data
msleep_subset %>%
slice(1:10)
continents_population <- list('South America', 644.54, 'Africa', 1305, 'Europe', 745.64, 'Asia', 4587)
typeof(continents_population)
for (item in continents_population) {
print(typeof(item))}
### Code doesn't show; only table
library(gt)
# Create a data frame with the structure
text_structures <- tibble::tibble(
Structure = c(
"Matrix", "List", "Data Frame", "Tibble", "S3 Object", "S4 Object"
),
Dimensions = c(
"2D", "1D", "2D", "2D", "Varies", "Varies"
),
Homogeneous = c(
"Yes", "No", "No", "No", "No", "No"
),
`Key Use for Text Mining` = c(
"Numeric structure for word embeddings or term-document counts",
"Flexible storage for mixed or uniform data—e.g., texts, dates, names",
"Standard tabular format for digitized records or structured text metadata",
"Tidyverse-friendly data frames optimized for large data and printing",
"Used in packages like `quanteda`, `stm` with custom print/access behavior",
"Formal, slot-based system used in `tm`, `topicmodels`, with accessor functions"
),
Example = c(
"`matrix(rnorm(300), nrow = 100, ncol = 3)`",
"`list(1800, \"text\", \"author\")`",
"`data.frame(name = ..., date = ...)`",
"`tibble(year, text)`",
"`class(tokens_object) → \"tokens\"`",
"`class(corpus_object) → \"Corpus\", \"VCorpus\"`"
)
)
# Generate the gt table
text_structures %>%
gt() %>%
tab_header(
title = "Common R Data Structures in Text Mining"
) %>%
cols_label(
Structure = "Structure",
Dimensions = "Dimensions",
Homogeneous = "Homogeneous",
`Key Use for Text Mining` = "Key Use for Text Mining",
Example = "Example"
) %>%
fmt_markdown(columns = everything())
library(quanteda)
library(tidyverse)
library(hansardr)
data("hansard_1800")
texts <- hansard_1800$text
# Create a tokens object from the text column
tokens_hansard <- tokens(texts)
# Perform a KWIC search on the tokens object
kwic_result <- kwic(tokens_hansard, pattern = "corn", window = 5)
gt(head(kwic_result))
slice(tokens_hansard, 1:5)
tokens_tibble <- tibble(
doc_id = rep(names(tokens_hansard), lengths(tokens_hansard)),
token = unlist(tokens_hansard, use.names = FALSE))
slice(tokens_tibble, 1:5)
library(gt)
library(tibble)
library(dplyr)
# --- Quanteda ---
quanteda_tbl <- tibble(
Task = c(
"Convert dfm to tibble",
"Get token list",
"Combine with metadata",
"Tidy version"
),
`What to do` = c(
"`dfm %>% convert(to = \"data.frame\") %>% as_tibble()`",
"`tokens %>% as.list()`",
"`docvars(dfm)` gives doc-level covariates",
"`Use quanteda.textmodels::convert()` or `tidytext::tidy()` if applicable"
)
)
print(quanteda_tbl %>%
gt() %>%
tab_header(title = "✅ Quanteda (tokens, dfm, etc.)") %>%
fmt_markdown(columns = everything()))
cat("### \n\n", sep = "\n")
# --- tm ---
tm_tbl <- tibble(
Task = c(
"Convert Corpus",
"DTM to tidy format",
"Combine with meta"
),
`What to do` = c(
"`sapply(corpus, as.character)` or `content(corpus)`",
"`tidy(DTM)` from `tidytext`",
"`meta(corpus)` or `tm_map()`"
)
)
tm_tbl %>%
gt() %>%
tab_header(title = "✅ tm (Corpus, DocumentTermMatrix)") %>%
fmt_markdown(columns = everything())
# --- text2vec ---
text2vec_tbl <- tibble(
Task = c(
"Convert DTM",
"Vocabulary to tibble",
"Embeddings to tibble"
),
`What to do` = c(
"`as.matrix(dtm) %>% as_tibble()` or `Matrix::summary(dtm)`",
"`vocab$term_stats %>% as_tibble()`",
"`as_tibble(embeddings)`"
)
)
print(text2vec_tbl %>%
gt() %>%
tab_header(title = "✅ text2vec (itoken, dtm, etc.)") %>%
fmt_markdown(columns = everything()))
# --- topicmodels / stm ---
stm_tbl <- tibble(
Task = c(
"Tidy topics + terms",
"Tidy docs + topics",
"STM effects"
),
`What to do` = c(
"`tidy(model, matrix = \"beta\")` (from `broom`)",
"`tidy(model, matrix = \"gamma\")`",
"`Use estimateEffect()` then extract manually"
)
)
print(stm_tbl %>%
gt() %>%
tab_header(title = "✅ topicmodels / stm (LDA/STMs)") %>%
fmt_markdown(columns = everything()))
# --- Sparse matrices ---
sparse_tbl <- tibble(
Task = c(
"Convert to dense",
"Extract non-zeros",
"Tidy-friendly format"
),
`What to do` = c(
"`as.matrix(sparse_mat)`",
"`Matrix::summary(sparse_mat)`",
"`Use reshape2::melt()` or `pivot_longer()`"
)
)
print(sparse_tbl %>%
gt() %>%
tab_header(title = "✅ Sparse matrices (e.g., dgCMatrix)") %>%
fmt_markdown(columns = everything()))
# --- Tidyverse Tools ---
tidyverse_tools_tbl <- tibble(
`Tool/Package` = c(
"tidytext",
"textrecipes",
"tidylo, textdata",
"unnest_tokens()",
"broom / broom.mixed"
),
Use = c(
"Makes many text objects tidy (DTMs, LDA, etc.)",
"Tidy preprocessing (e.g., tokenization, tf-idf)",
"Additional tidy tools for text and lexicons",
"From `tidytext`, best for converting raw text to tokens",
"Tidy output from models (including `topicmodels`)"
)
)
print(tidyverse_tools_tbl %>%
gt() %>%
tab_header(title = "Helpful Tidyverse-Compatible Tools") %>%
fmt_markdown(columns = everything()))
# Knit in the document directory (helps Pandoc find local files)
knitr::opts_knit$set(root.dir = dirname(knitr::current_input()))
knitr::opts_chunk$set(echo = TRUE)
# Knit in the document directory (helps Pandoc find local files)
knitr::opts_knit$set(root.dir = dirname(knitr::current_input()))
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir if we are actually knitting a file (not running interactively)
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble.tex exists right next to this Rmd (safety net)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}"
), "preamble.tex")
}
# Sanitize curly quotes and stray latex macros in all printed output
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)      # ’ ‘ → '
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)     # “ ” → "
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)     # latex macro → '
if (is.function(hook_out)) hook_out(x, options) else x
})
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
library(gt)
library(tibble)
library(dplyr)
md_col <- function(tbl, col) {
# Safely convert a single column's markdown to gt::md()
col <- rlang::ensym(col)
tbl %>% mutate(!!col := gt::md(!!col))
}
# --- Quanteda ---
quanteda_tbl <- tibble(
Task = c(
"Convert dfm to tibble",
"Get token list",
"Combine with metadata",
"Tidy version"
),
`What to do` = c(
"`dfm %>% convert(to = \"data.frame\") %>% as_tibble()`",
"`tokens %>% as.list()`",
"`docvars(dfm)` gives doc-level covariates",
"`Use quanteda.textmodels::convert()` or `tidytext::tidy()` if applicable"
)
)
quanteda_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "Quanteda (tokens, dfm, etc.)") %>%
cols_label(`What to do` = "What to do")
# --- tm ---
tm_tbl <- tibble(
Task = c(
"Convert Corpus",
"DTM to tidy format",
"Combine with meta"
),
`What to do` = c(
"`sapply(corpus, as.character)` or `content(corpus)`",
"`tidy(DTM)` from `tidytext`",
"`meta(corpus)` or `tm_map()`"
)
)
tm_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "tm (Corpus, DocumentTermMatrix)") %>%
cols_label(`What to do` = "What to do")
# --- text2vec ---
text2vec_tbl <- tibble(
Task = c(
"Convert DTM",
"Vocabulary to tibble",
"Embeddings to tibble"
),
`What to do` = c(
"`as.matrix(dtm) %>% as_tibble()` or `Matrix::summary(dtm)`",
"`vocab$term_stats %>% as_tibble()`",
"`as_tibble(embeddings)`"
)
)
text2vec_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "text2vec (itoken, dtm, etc.)") %>%
cols_label(`What to do` = "What to do")
# --- topicmodels / stm ---
stm_tbl <- tibble(
Task = c(
"Tidy topics + terms",
"Tidy docs + topics",
"STM effects"
),
`What to do` = c(
"`tidy(model, matrix = \"beta\")` (from `broom`)",
"`tidy(model, matrix = \"gamma\")`",
"`Use estimateEffect()` then extract manually"
)
)
stm_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "topicmodels / stm (LDA/STMs)") %>%
cols_label(`What to do` = "What to do")
# --- Sparse matrices ---
sparse_tbl <- tibble(
Task = c(
"Convert to dense",
"Extract non-zeros",
"Tidy-friendly format"
),
`What to do` = c(
"`as.matrix(sparse_mat)`",
"`Matrix::summary(sparse_mat)`",
"`Use reshape2::melt()` or `pivot_longer()`"
)
)
sparse_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "Sparse matrices (e.g., dgCMatrix)") %>%
cols_label(`What to do` = "What to do")
# --- Tidyverse Tools ---
tidyverse_tools_tbl <- tibble(
`Tool/Package` = c(
"tidytext",
"textrecipes",
"tidylo, textdata",
"unnest_tokens()",
"broom / broom.mixed"
),
Use = c(
"Makes many text objects tidy (DTMs, LDA, etc.)",
"Tidy preprocessing (e.g., tokenization, tf-idf)",
"Additional tidy tools for text and lexicons",
"From `tidytext`, best for converting raw text to tokens",
"Tidy output from models (including `topicmodels`)"
)
)
tidyverse_tools_tbl %>%
md_col(Use) %>%
gt() %>%
tab_header(title = "Helpful Tidyverse-Compatible Tools")
library(tidyverse)
census <- tibble(
name = c("Mary Jenkins", "Samuel Price", "Hannah Lee", "Charles Morton", "Unnamed Child"),
occupation = c("washerwoman", NA, "seamstress", "laborer", NA),
age = c(32, 44, 29, 51, 1))
cenus
census
census %>%
mutate(occupation = str_detect(conservation, "washerwoman"))
census %>%
mutate(is_washerwoman = str_detect(occupation, "washerwoman"))
census %>%
mutate(is_missing = is.na(occupation))
library(tidyverse)
# Replace NA in ""conservation" with the string "missing"
census <- census %>%
mutate(occupation = replace_na(occupation, "missing"))
# View the updated data
census %>%
slice(1:10)
