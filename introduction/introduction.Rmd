---
title: "Introduction"
output: pdf_document
geometry: "left=1in,right=1in,top=2in,bottom=1in"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
```

# Preface

We live in an age of text. Text structures government reports, policy documents, business communications, news media, and personal correspondence. Text is not merely a record of our world; text also shapes the world we leave behind, and is the vehicle for our desires and needs. Laws are made through written language; identities are negotiated in digital conversations; decisions are justified through written reports; debates are transcribed onto records. The instructions that guide our technologies, the terms that govern our contracts, and the narratives that influence public opinion are all encoded in text. Even our memories and emotions—diary entries, social media posts, messages to loved ones—are inscribed in language. Text is not only a product of human expression, but is also increasingly generated by Artificial Intelligence (AI). As the volume and variety of text continue to grow, so does the importance of techniques like text mining for making sense of it all.

Historical analysis, meanwhile, is among the most ancient forms of analysis. Since the first aboriginals located the story of their people in the cosmos, since the Sumerian scribes who kept lists of kings and battle records, from the time that Herodotus and Thucydides peered into the causes of human affairs and Sima Qian inspected the documentation behind myth and fact, humans have used the analysis of past stories to locate themselves in time and space. Parsing the understanding of the past characteristic of ten years ago from what we believe today is a powerful rubric for understanding, whether we apply it to history, law, or medicine. Understanding how values, beliefs, politics, and culture are changing in the present is often the closest we ever come to predicting the future. 

Text mining refers to the process of breaking digitalized documents into words and phrases. When we use text mining for historical analysis, we prioritize analyzing texts over time, counting those words, phrases and linguistic patterns that help us to understand how ideas, politics, and culture changed.

Without the analytical framework of historical thinking, text mining is rarely a useful practice. It mimics analysis by dividing words into categories and frequency, but the results are often wanting. Having stripped language of its context, intention, and complexity, text mining without historical analysis rarely produces an analytic worthy of a second glance. 

The problem of text-based data is increasingly relevant in the age of AI. AI technologies lower the barrier for performing computational text analysis, allowing researchers to explore different methodologies and their effect on a textual corpus without needing extensive programming knowledge. AI tools can quickly process large amounts of historical text, summarize content, and even assist in identifying linguistic patterns that might be difficult to detect through traditional close reading.

However, the use of AI chatbots also raises critical questions about accuracy and interpretability, issues that we aim to address in this book. Even in a world of AI, discerning the truth of facts about political and cultural change requires a grounding in the original texts and how they changed over time, which AI on its own is not sufficient to deliver. 

If you ask ChatGPT for an assessment of how U.S. president Donald Trump changed politics, ChatGPT will summarize the reflections of many media and pundits large and small. But the chatbot is unlikely to perform a statistical analysis of the political questions that were silenced and those that were raised, based on the words of the speeches and press releases from the White House, unless the analyst explicitly asks the agent for help executing text mining for historical analysis. 

Chatbots generate text probabilistically rather than through an understanding of historical nuance. This means chatbots can introduce errors or fabricate sources. Since LLMs are trained on contemporary and often skewed datasets their outputs may reflect present-day assumptions and biases rather than faithfully representing historical contexts. Chatbots are known to generate content--false content as well as factual content--with a highly confident tone (Toney-Wails 2024, Hicks et. al 2024).

Because of their propensity to fabricate truths, AI cannot replace the value of text mining for historical analysis. On the contrary, a basic introduction to R, like the one offered in this book, is more relevant than ever. Even if readers do not intend to become expert programmers, understanding the steps executed in a data processing pipeline, as well as how to read and adapt code, can help them move beyond treating scripts as opaque black boxes. It can provide the insight needed for an analyst to question to what purpose an online text mining app serves. Without this critical insight, the data processing steps behind these apps are opaque and we cannot interrogate how the steps behind data processing impress upon our analysis of history. This foundational knowledge of programming, we therefore suggest, fosters critical engagement with computational methods, empowering scholars to question their assumptions about the creation on historical knowledge. 

What the AI gives us by default -- a summary of pundits -- is not objective truth; no matter how accurate it is, summaries of hearsay are always a reflection of rumor. That hearsay may be informative, but it is never above contestation.  It's only through the comparison of many texts over time that we arrive at the solid ground of fact -- statements about shared events that we have all experienced together.  When we perform text mining, our purpose is to analyze the fact of historical change over time, producing objective analyses of who said what to whom, how political values and beliefs changed, of what voices were raised and what voices are silenced, in a way that transcends the bias of party, race, gender, nation, political alliance, or theory. At the end of the day, text mining for historical analysis answers questions about politics and bias by counting words over time. Those counts may not tell us everything we need to reach an interpretation of the past, but in number, they offer the safety of an objective fact of history. Text mining for historical analysis represents a practice of discerning fact that cushions against the hypothetical, hallucination-based, fact-free world of research created by the abuse of technologies like ChatGPT.

At the same time, ChatGPT and other AI programs offer students of computation an important opportunity to accelerate their learning in the experimental zones where new fields emerge. Because students relatively new to a subject can use an AI to edit their code, to make suggestions, or even to provide general summaries of historical context, today's AI offers to speed the creation of new fields. 

Text mining for historical analysis is among the fields that can profit the most from the kind of acceleration afforded by AI. Every week, we hear from humanists and social scientists who trained on traditional methods but have realized that a chatbot can digitalized the handwritten manuscripts in their files, and they wonder about the next steps to take. Every week, we talk to journalists, medics, and data workers in the public sector who are overwhelmed by data and wonder how to move towards bigger questions.  Our students trained in Computer Science learn a rigorous approach to code without a set of practices for how to apply it.  The fingers of the hand are reaching for knowledge, but they cannot find each other, let alone act in unison.

It is for these many audiences – all engaged in addressing text in different ways -- that we have written a book that articulates best practices from computer science, history, data science, as well as the critically-informed use of AI.  Our approach is fundamentally hybrid: it prescribes a balanced diet of history, critical thinking, programming, and alongside with specific engagements with chatbots.  Yet the book presumes no preliminary knowledge of any field.  We begin with the basics, both with history and with programming.  We envision a future paradigm in which AI and data analysis combine as a mode through which we generate knowledge and perform interpretation, one through which analysts can explore patterns and engage critically with text. 

## The Origins of Text Mining for Historical Analysis  

This book originated in the classroom. Over several years of teaching hybrid practices to students at our respective universities, we concluded that it was vital that analysts wishing to learn these techniques have access to a deliberate, encompassing series of chapters that applies the techniques of computational text analysis to the concerns of History. We shared drafts of the manuscript and the accompanying code with our colleagues and students, and these drafts have received the accolades of several cohorts of users for their transparency.  

Many excellent books teach programming (Lubanovic 2019, Matthes 2023, Zumel 2019). The same is true for historical analysis (Wineburg 2021, Fischer 1970). There are also excellent introductions available for the digital humanities (Davidson 2012), as well as omnibus guides for how computation has affected the field of history (Salmi, Crymble, Milligan 2022) and even starter guides for learning text analysis for literary or historical study (Silge and Robinson 2017, Blaney et al 2021). But none of these provides an approach where questions about change over time structure an approach to text, with the idea of elevating patterns over time. 

However, few resources exist that identify and explain the skills of the analyst in the zone where text meets change over time.  That change over time enhances the analyses of text mining has long been plain to computational literary theorists like Ted Underwood, who has highlighted the significance of framing questions over "distant horizons" like the changing trends in the literary canon (Underwood 2019). Meanwhile, in continental Europe, the pursuit of historical study through text mining has generated new journals, conferences, centers and coordinating institutions, as well as a bevy of excellent scholarship that leans heavily on mathematical modeling and word embeddings to produce powerful new analyses of historical events (Ros, Wevers, Eijnatten, Ihalainen).  This book's companion volume, Guldi's _The Dangerous Art of Text Mining_ (2023), consolidates much of current research into a theory and method of text mining for historical analysis. At the same time, as with the published works of most of the theorists in the field, _The Dangerous Art_ offers no starting point to students who wish to take code into their own hands. 

The current volume offers a practical, programming-oriented introduction to text mining for historical analysis.  It is designed to stand on its own. At the same time, it reflects back on conversations about the nature of change over time, strategies for modeling the bias of archives, for studying events, and for foregrounding different temporal relationship, that are laid out in _The Dangerous Art._

Our goal is to equip readers with diverse backgrounds and interests with strategies tailored to a range of research needs. To further support this aim, we have included additional material in the conclusion and appendices, offering guidance on alternative code libraries. We provide this guidance in an appendix and not the body of the _Text Mining for Historical Analysis_ because it is more than a code cookbook; it is also a book where approaches to historical thinking are explained alongside functions and algorithms as a key part of the method. 

## Historical Thinking

Text mining for historical analysis is necessarily a hybrid zone -- that is to say, it is not only interdisciplinary, but it has been forged out of long-running partnerships where computationalists and historians read each other’s work, collaborate on infrastructures and processes, and share ideas, growing into a new living thing -- much as experiments with hybrid corn in the twentieth century produced new strains of life that change the way we live.  Following in the footsteps of computional linguistics and computational biology, digital history is emerging as a hybrid zone somewhere between the information sciences, social sciences, and humanities.  The crucial outside contribution in our field comes from the social sciences and humanities – realms where humans have reasoned over centuries about the conditions under which ideas, politics, and culture change.  

In Guldi’s _Dangerous Art_, she breaks down thinking about the past into several “categories of temporal experience” that have been studied by philosophers of History.  These categories of temporal experience include the event (what happened), period (when it happened and how long it lasted), agency (who or what caused it), causality (why it happened and what followed from it), and memory (how it has been remembered or forgotten over time).  Guldi then pairs each category of temporal experience with algorithms from Natural Language Processing, showing how multiple algorithms can help the analyst to understand the multidimensional nature of historical change. 

The case studies we present are mainly from British History because of our extensive experience in the field. To make its case for the usefulness of text mining and historical analysis as a means of understanding changing politics, culture, and ideas, this volume, like _The Dangerous Art,_ performs analyses on one of the indexical databases that have the most to tell us about the modern era -- Hansard's parliamentary debates of Great Britain, a database that holds reflections on the merits of the free market, the moral imperative to abolish slavery, and the rights of women -- even when the institution in question was sometimes slow or recalcitrant in its uptake of those arguments.  This book also gives examples taken from the study of the United States debates of Congress from the _Congressional Record_ which will be of interest to readers from American History; an appendix explains to instructors how the entire book might be used in an American History course. 

Despite their focus on political speeches, the methods in this book are intended to extend to other questions across historic domains, and even into other fields in which analysts value the concept of change over time. Scholars of different types of historic text--such as legal texts, hip-hop lyrics, or records on global affairs--will therefore discover that computational historical thinking can be generalized to other studies. 

## The Challenging Skill of Detecting the Unexpected

There are many software packages that allow analysis to import datasets and quickly perform text mining, such as in the form of sentiment analysis, word counts, and topic models. A greater conceptual introduction to computation, however, can elevate these analyses from engaging with one-dimensional computational models to forming a deeper understanding of human problems. This deeper understanding, we believe, is the foundation of performing more meaningful analysis.

Because this book integrates both computational and historical thinking, it contends with the assumptions and stances traditionally asserted within the humanities: that meaningful research argues for surprising or unexpected insights, that it should challenge existing ideas, or reveal new perspectives. Within this framework, argumentation—particularly the pursuit of novelty—has long been valorized as the hallmark of scholarly contribution. This has resulted in a dichotomy within the humanities between work that focuses on method and our interactions, as well as argumentative work that might not consider how the mode in which we engage text mediates interpretation (Robertson 2016, 2021). 

We do not disagree that this form of knowledge production is valuable, but here we do not privilege argument over other forms of insight. This book navigates that tension, suggesting that computational methods need not always produce radically new claims to be valuable; they can also deepen our grasp of well-known phenomena, provide detail and nuance for known patterns within the archives, and enhance interpretive practices without necessarily overturning established narratives.  At their best, computational methods provide a way of moving from cherry-picked research questions to a method of working where the analyst consults theory, secondary sources, primary sources, and computational work on the way to identifying the most relevant research question. 

What does this look like in action? In the text, we walk the analyst through an example of using text mining to identify the most relevant speakers to the debate over the abolition of slavery, the most relevant words, and some of the most relevant passages where the speakers use those words.  As we note in our explanation, the investigation need not end there; each of the individual speakers is attached to many other archives that could tell us more about the stories they’re connected to. Some of the events they reference happened outside of Britain in the Caribbean colonies, and there are still other archives attached to these records. Computational methods can lead us to specify which stories made it to the floor of parliament during the debates about abolition, but they don’t tell us which of these stories to follow next – and bringing those stories home will often occasion leaving data-driven archives behind. 

Among the skills for producing insight from computational historical analysis is the discernment of "validation" from "discovery." An initial attempt to run code often returns results that replicate what is already known about the historical record. ACreating a topic model of Victorian novels may result in a selection of topics that looks familiar to readers of Victorian novels. This is not a failed study. Instead, it demonstrates that the analytic approach (in this case, topic modeling) is capable of replicating a specific mode of humanistic inquiry. The ability to replicate known findings is key to a scientific appreciation for the mode of analysis. The interplay between validation and discovery ensures that computational techniques not only reinforce existing understandings but also open new avenues for historical interpretation.

As a rule of thumb, we do not prioritize discovery over validation or vice-versa; rather, we look to discovery as an ultimate step after a series of visualizations designed to validate different aspects of theory and secondary research. Researchers might colloquially refer to an "80/20" rule of validation to discovery, where 80% of work with data is expected to recapitulate already known features of the data, and only 20% of the work leads to new discoveries. 

Some of the analyses taught in this book will not lead to earth-shattering discoveries to expert historians of Britain's parliament; these analytics are explained in the text as instances of validation, and we carefully walk the analyst through the next steps that might lead from validation to discovery. The promise of this kind of work is a process of interrogation, whereby writing code leads to more precise questions about language, which are in turn driven by broader reading. Those questions ultimately lead to data-driven analysis which may constitute new findings, whether in British history or in the analyst's own domain of research. As we mentioned above, we anticipate and hope the content taught here initiates additional inquiry into vast historical domains, and that _Text Mining for Historical Analysis_ might serve as the basis--or foundation--for much larger questions. 

To support a practice of discovery, the most important skills for computational historical analysis is "iteration," a practice long been theorized in scientific as well as humanistic inquiry as a key method of working with data. The origins of such an approach might be traced back to the 1970s, where scientific scholars, like John Tukey, formalized an approach called "Exploratory Data Analysis" (EDA), which involves a series of steps that move from initial data visualizations to those focused on answering a specific question. Such an approach is applicable to computational historical analysis (Tukey 1977). We will show that users can transition from a high-level overview of a time period and into a close reading of the text itself. The importance of taking an iterative approach to research has been emphasized by other fields, too. Political scientists, like Justin Grimmer and his collaborators, stress the importance of iterative engagement by moving between theory-based lines of questioning (also termed "induction") and data-based lines of questioning (called "deduction") in a cycle referred to as "abductive logic" (Grimmer 2022). In her article, "Critical Search," Guldi outlines an iterative process where the researcher moves from reading historical overviews to working with data, to reading critical theory to engage with these questions, to finding data insights, to then adjusting the algorithm to align with the scholar’s questions, to testing those insights against the additional literature to see whether and how scholars have engaged these episodes (Guldi 2018).In a more concrete example of iterative research: scholars like Lauren Tilton have examined how EDA can serve as an impetus to hone a research problem from a timeline of all the photographs in a section of the Library of Congress to the timing of content in photographs taken by two prominent photographers (Arnold 2023, Wexler 2014). 

Our book models an iterative approach to analysis. For example, when examining how speech patterns varied across decades, we show that slicing and re-slicing the data—based on slightly different research questions—and applying different metrics to the corpus can yield entirely different results than applying a single statistical measure once. A one-time application of a method may overlook the multiplicity of narratives and insights embedded in the data. While no single approach can capture this complexity, using multiple methods helps illuminate the richness and layered meanings within the corpus.

Another key to producing insight is growing curious about data processing, algorithms, and the many ways in which they can be applied. _Text Mining for Historical Analysis_ encourages a curiosity in the following exercises by walking readers through the production of an analysis, step-by-step, and then recreating a new analysis by making minor modifications to the existing code. For instance, we may first explain a computational method and then guide readers through iterative applications, demonstrating its versatility and encouraging exploration through small modifications. Both of us have proposed the value of using different algorithms to investigate the same substantive question because each algorithm and each adjustment to algorithm parameters will produce different, subjective results, and some of these results may be more or less meaningful for historical analysis. In short, there is no one "best" algorithm for any investigation. A more strategic approach leverages several related algorithms to investigate the hidden dimensions of difference latent in the data. This approach argues that a singular algorithm cannot produce a "definitive" interpretation of history. As an example, multiple times throughout the book we will compare the effect of using different algorithms designed to measure similar phenomenon, eschewing the claim that there is one "best" algorithm for any subject in favor of a genuine curiosity about patterns hidden in the data that may produce surprise and insight. 

In this book, we seek to challenge assumptions about how computers and historical analysis can intersect, suggesting that the two can exist in a mature, mutually-enhancing, reciprocal, and truly hybrid process. Our broader goal is to demonstrate a theoretical and methodological framework for thinking through historical problems computationally. It extends a line of thought on how digital technologies are reshaping not just what we think, but how we think (Ramsay 2011, Hayles 2012, Kirschenbaum 2012). Our goal is to propose a way of how to think, to think of ourselves in relation to technologies and data. 

We hope that such a way of thinking initiates a form of engagement that may produce genuine surprise and meaning for those familiar with the historical events. For those less familiar with the events of British history, we hope that learning computational historical analysis provides not only a deeper understanding of the past but also a critical framework for interpreting historical narratives and patterns in general. 

## Our Approach to Programming

For many analysts new to the domain of computational historical analysis, it is hard to know where to get started if their aim is to analyze text data. They may find themselves searching instructions online, all the while asking themselves: "should I begin with cleaning my data or learning the basics of command-line instructions? Should I start by counting words? Should I commit to Python or R?" So many choices may appear at once. 

Analysts need a structured and focused program of study—one that equips them with practical skills for analyzing text and understanding change over time, rather than a collection of disconnected lessons that may not align with their research concerns. To meet this need, we have organized the core challenges faced by text analysts into a coherent series of chapters, each designed to build on the previous one. Our approach ensures that scholars can develop a functional understanding of text mining for historical analysis without unnecessary distractions.

Many of the concepts demonstrated in this book include detailed explanations and step-by-step guidance tailored for readers who have never programmed before, as well as those with some programming experience who are new to R or its application for historical analysis. To support this range of learners, the early chapters provide structured, guided introductions to key programming concepts, which are then built upon progressively throughout the book. 

We have selected R as the primary programming language for this book because it offers a balance between ease of use and analytical capability. Compared to other languages such as Python--which often require a baseline familiarity with complex syntax, multiple data structures, and strict data management--R enables researchers to engage in text analysis more quickly and with fewer technical barriers. This makes R particularly well-suited for historians and other researchers in the humanities. By minimizing technical overhead we can concentrate on mastering computational methods that directly advance our research goals.

Throughout the book we try to communicate basic concepts originating from computer science and history and define key terms. A glossary of terms from both history and computer science are also provided in the appendix.

## The Longevity of the R Programming Language 

Readers may question the long-term viability of the R ecosystem. These questions may stem from concerns about the long-term stability of the R packages used in this book, the instructions we provide for interacting with the RStudio interface, or the rapid advancements of generative AI as a primary means of generating code (as opposed to reading or writing code ourselves). These questions are especially important in the context of a book that has technical content, where technological updates can render examples or instructions as less applicable. 

Software packages can change, and it is possible that RStudio could change its look or feel. We agree understand these concerns, which is why we have taken several steps to ensure the durability and reliability of the instructions in this book. First, we have chosen R and RStudio deliberately for their longevity and wide adoption. RStudio’s user interface has remained largely consistent for over a decade, and we expect our instructions to remain accurate for many years to come, regardless of changes to operating systems or software updates.

In respect to the R packages used throughout the book, some are widely adopted and have become "stable." This means they are well-tested and unlikely to change significantly. We avoid relying on "bleeding-edge" or experimental versions, which are more likely to introduce changes that can "break" our code. Other packages, like `hansardr`, were created by Buongiorno, who us committed to maintaining the package into the future. We also welcome reader feedback for future updates. 

Finally, we acknowledge a broader concern: in an era of fast-moving technological change--especially around generative AI--how can readers trust that a book like this will remain useful? Ultimately, _Text Mining for Historical Analysis_ is not a code cookbook. It introduces and demonstrates foundational concepts and methodologies for computational historical thinking that transcend the use of a singular tool. In doing this, our book also teaches readers how to interpret and engage R. Our book extensively uses Hadley Wickham's `tidyverse`--a set of R packages that share an underlying philosophy and grammar of data processing and visualization (2019). However, the particular functions we use from the `tidyverse`—along with our selective method of employing functions, the process for ordering them in a larger code body, and the way we pair them with other R libraries like quanteda—demonstrates our methodological approach to structuring a reproducible and interpretable text analysis pipeline for the field of digital history. 

In short, while no software is immune to change, we have made every effort to ensure that _Text Mining for Historical Analysis_ offers a conceptual understanding that will remain relevant, and that exercises in this book are built on a foundation of stable, well-supported tools. We believe this will keep the book relevant and useful to readers for many years to come.

## The Data in this Book

In this book, we explore where data can be collected from and how to prepare it for analysis. While there are many ways to represent and store textual data, _Text Mining for Historical Analysis_ primarily demonstrates methods for structuring and processing data in a tabular format. This approach has several key advantages. First, tabular data is simple to read, making it more accessible for historians and analysts. Second, a wide range of existing tools and code libraries--especially in R--are designed specifically for processing tabular data. These existing tools allows for efficient data processing, visualization, and analysis. Our approach to writing code is highly flexible, enabling researchers to apply similar analytical techniques across diverse datasets. For example, just as we analyze patterns in the British Parliamentary Debates, analysts could use the same tabular methods to examine legislative discussions in Hong Kong, legal battles involving peasants in Peru, or diplomatic correspondences between nations. The ability to apply a common data processing framework to different historical sources not only makes computational text analysis more accessible but also enables meaningful comparisons across historical contexts. 

However, representing data in a tabular format is not the only way to approach text mining and there are also disadvantages. One key drawback is that tabular formats impose a structured, spreadsheet-like organization onto text, which can sometimes oversimplify complex linguistic and historical relationships. For instance, historical documents often contain nested narratives or variations in spelling and formatting that may not fit neatly into a structured table. Additionally, while tabular formats work well for tracking word frequencies, metadata, and referential shifts over time, they may struggle to capture more nuanced textual features such as syntactic relationships, semantic shifts, or rhetorical structures. We attempt to address some of these limitations in _Text Mining for Historical Analysis_, as we believe these points further emphasize the importance of using multiple metrics to analyze a dataset. 

The analyses in this book primarily concern Hansard, which contains the official record of the debates of Britain's House of Lords and House of Commons, 1803-1899. It is an interesting data set because the debates cover the transformations of the Industrial Revolution, the abolition of the trans-Atlantic slave trade by the British navy, the struggle for women's rights, and many other debates of general interest to readers of modern history. 

One advantage of exploring the British parliamentary debates, as we do for most of the volume, is that we have jointly worked for at least a decade on the datasets presented in this book. Our familiarity allows us to model an engagement with issues of data quality and historical inquiry -- something that would be more difficult to organize in a book that moved from dataset to dataset at random.  Deep familiarity with a dataset, its historical context, and the kinds of historical questions that might arise when looking at a dataset is one of the prerequisites for serious analysis of change over time. 

As we will explore in the chapters that follow, Hansard is in many ways an imperfect data set. The Hansard data dates from a moment in history before the advent of modern technologies of transcription, including shorthand and electronic recording. Instead, it was compiled from press reports and notes, highly edited for what journalists assumed would be of interest to contemporary readers, and sometimes paraphrased to keep pace with the speaker.  Nonetheless, even in the early versions of the reports, a great deal of data about the social, economic, and political realities facing parliament remain preserved.  Though far from a verbatim transcript of the records of parliament, it is nevertheless an important historical source that has served generations of historians, many of them working with concerns social and cultural as well as political and intellectual. 

Part of what computational text analysis can accomplish is an investigation into the record of bias left by an earlier age. The Hansard dataset is an artifact from an era where the vote was limited to a tiny portion of the aristocracy (before 1832) and the middle class (to 1867).  The record is also almost exclusively male, as women could not vote in Britain before 1918.  Britain of course controlled a global empire, and many of the speakers expressed disdainful attitudes towards their Roman Catholics, Indians, Africans, and Irish subjects; indeed, historians have reasoned that it was the British willingness to fabricate arguments about racial superiority and the necessity of violence that allowed the empire to exist.  The techniques in this volume allow the student of history to examine British attitudes about gender, race, and class for themselves.

## Working With Your Own Data

What if you're not a historian of nineteenth-century British Parliament? Computational historical thinking can still be applied to many other datasets. New sources are continually becoming available, and researchers studying virtually any historical period may find datasets to which the following methods apply—with only minor adjustments, such as aligning column names for speaker, date, and speech content. Toward the end of the book, we will shift our focus to the Congressional Records of twentieth-century America. What about when an analyst is ready to model their own data? Where might one acquire the data and how does one structure it for analysis? In the final chapter, we will expand our scope even further, exploring the concept of "data" more broadly by demonstrating how to collect, clean, and analyze data from diverse sources and formats, including large-scale collections hosted by modern and historic government websites. One of the great delights of text mining is that a set of code can be applied to a new dataset with an adjustment of few lines of code. 

For contemporary records or certain kinds of archival material that has already been digitized by libraries and archives, the process of assembling a dataset may be relatively straightforward. Exercises in the Programming Historian give a straightforward set of steps to scraping the records from an archival digitization project into a database of a kind that can be examined with the processes in this book. 

The landscape of data is swiftly changing. We already hear about historians who have had great success in digitizing photographs from the archives of typescript or even handwritten manuscripts by using large language models (LLMs). Such practices  stand to dramatically lower the cost of digitization. This horizon will doubtlessly change over the near horizon.

## AI: Working With Chatbots as an Opportunity for Critical Thinking

In this book, we will explore the use of LLM-based chatbots for computational historical analysis through three major applications: brainstorming, writing code, and generating analysis. We suggest chatbots can support brainstorming by proposing alternative ways to visualize results or by identifying different angles that may not have been considered by the analyst. One practical use of a chatbot, as shown in this book, is to help generate a list of "stop words"—or, common words to be removed from a text in order to support a more meaningful analysis. Although a human analyst can create a list of stop words by hand, doing so is often tedious and time-consuming. AI can speed up this process, allowing researchers to devote more time to interpreting historical patterns and insights. However, generating something like a stop words list with a chatbot does not eliminate the need for critical judgment (Schofield 2017). Analysts must still carefully review the suggested stop words, since chatbots may mistakenly flag words that hold historical significance. Words that appear unimportant in a general linguistic context may carry unique meaning in specific historical periods or texts. Without oversight, an automatically generated stop word list could erase crucial evidence, skew results, or reinforce modern biases. Thus, chatbots must be used with discretion to ensure they serve historical inquiry. 

Another way LLMs can support computational historical analysis is by generating code that perform data processing and data visualization steps, rather than using a LLM to execute the analysis. This approach introduces more control over the LLM for analysis, reducing the risk of fabricated "reasoning" or hallucinated results. The generated code reflects the analyst's logic, but the LLM itself does not engage in reasoning steps. Used this way, AI chatbots can facilitate the set up required for historical analysis and reduce the technical barriers that often accompany computational text analysis. For instance, a chatbot can quickly generate a script to clean data by removing punctuation, normalizing spelling variations, or structuring messy data into a more usable format. Even though these capabilities can save time and lower the barrier to computational text analysis, analysts need strategies for assessing chatbot-generated code with a critical eye. LLMs do not "understand" coding best practices or the goals of an intended analysis. Nor do chatbots "understand" research contexts or the historical record. They generate responses based on patterns in their training data. As a result, the code may contain errors or the chatbot may use methods or derive interpretations that are inappropriate for historical analysis, a problem we demonstrate in _Text Mining for Historical Analysis_ by prompting an AI chatbot to provide an interpretation of 19th-century Parliamentary speech. 

Another risk is this: chatbots may produce code that is fully functional but subtly distorts the data, potentially leading to misleading conclusions. This is why we think it is essential for researchers to have the skills to review and modify generated code in order to ensure it aligns with their research goals and the nature of their data. In this book, we demonstrate how to critically assess chatbot-generated code with the aim of providing readers with a mode of critical thinking needed to make sure automation supports--rather than compromises--rigorous historical analysis.

A final approach we demonstrate in using AI chatbots in _Text Mining for Historical Analysis_ is the generation of textual analyses. On the surface, this capability holds promise: a chatbot might quickly summarize historical texts, suggest connections between events and ideological movements, or offer an overview of unfamiliar material. Such tools could be particularly helpful for researchers constrained by time or new to a topic, providing a broad starting point for deeper inquiry.

However, due to the probabilistic nature of LLMs, they do not “understand” history—they generate outputs based on patterns learned from contemporary data, and they are far less likely to have encountered 19th-century speech or rhetoric during training. As a result, their interpretations can introduce serious distortions. In Chapter 5, for example, we show how an LLM misrepresents a parliamentarian’s remarks: instead of recognizing the parliamentarian's critical stance towards women, the model reframes his speech as supportive, attributing to the speaker a progressive view that is not present in the historical record. This reflects a tendency for LLMs to introduce present-day biases—especially overly positive readings—into contentious remarks.

Because of these limitations, we argue that chatbot-generated summaries should be treated as exploratory rather than authoritative. Their output requires critical human interpretation and verification against primary and secondary sources. Without awareness of historical context, LLMs risk omitting key nuances, misrepresenting positions, or fabricating plausible-sounding but inaccurate information. Their value, therefore, lies less in producing final interpretations and more in supporting initial exploration.  

## Exploratory Output versus Publishable Tables

Before a dataset becomes something we interpret, it must first become something we trust. In a computational workflow, this often means examining partial or provisional views of the data while it is still "in motion" or being processed. Throughout the book, we use functions like `head()` within the data processing stages as they let us glimpse the structure or content of an object as it is being transformed, helping us confirm that our filtering, joining, tokenizing, or reshaping behaved as expected. These quick textual previews operate as a form of diagnostic seeing — a way of checking whether the data is in the condition we think it is.

Once the data is cleaned, ordered, and conceptually meaningful, our goal shifts from verifying to communicating. At this point, simply printing the first six rows is no longer sufficient. We instead use tools like `kable()` to convert a dataframe into a typographically legible, reader-facing table. The formatting is not cosmetic but rhetorical: choices about table alignment, column labels, and captioning help guide interpretation. These decisions transform raw rows into evidence. Where `head()` asks "does the data processing look right?", `kable()` asks "what should the reader see — and understand — here?"

This distinction matters for interpretive work in the digital humanities because even seemingly small formatting decisions shape how patterns are perceived: which numbers look large or small, which words appear central or marginal, which contrasts emerge as meaningful. Exploratory output supports the researcher's reasoning process; published tables communicate the conclusion of that reasoning to others.

## Book Organization 

Introducing computational historical analysis is challenging because it requires balancing close reading with quantitative methods. An introduction to the domain must bring together traditional approaches to historical interpretation with computational approaches, showing how the two can complement and enrich each other. Our goal is to equip readers with the ability to engage in both modes of analysis. As a result, the chapters in this book serve different purposes. Early chapters introduce the R programming language and critical thinking skills. However, the book does not follow a strictly linear progression. Some chapters emphasize close reading and the process of historical inquiry—tracing how a scholarly question evolves into both qualitative and quantitative analysis—while others focus more on quantitative methods themselves: how they work, what they measure, and how they can be meaningfully applied to historical research. Here we provide an overview of how the rest of the book is organized. 

Chapter 1 introduces foundational concepts in text mining by focusing on the relationship between word usage and context. It begins with a key principle from linguistics—that “you shall know a word by the company it keeps,” as articulated by J.R. Firth. In this chapter, we consider how this insight informs historical inquiry. Rather than treating words as isolated units, the chapter demonstrates how examining the language that surrounds a given term can reveal broader cultural and political patterns. Using a curated version of the 19th-century British Parliamentary Debates, made available through our `hansardr` R package, the chapter guides readers through the process of loading, exploring, and analyzing historical text data. Readers learn to work with tidy data structures in R, apply functions from the tidyverse (2019) and tidytext (2016) libraries, and perform basic operations such as tokenization, keyword filtering, and frequency counts. These initial techniques are used to examine how terms like “India,” “foreigner,” and “outsider” were discussed in mid-nineteenth-century parliamentary speech. The chapter also introduces methods for identifying collocates and using Keywords in Context (KWIC) to trace subtle variations in meaning and usage over time. In addition to practical instruction, the chapter encourages critical reflection on the limitations of computational approaches, emphasizing that word frequency and proximity, while useful, are best understood as preliminary tools—methods that can guide and supplement but not replace close reading and interpretive analysis. In this way, the chapter offers both a computational foundation and a methodological orientation, preparing readers to approach historical texts as data while remaining attentive to the interpretive work that such analysis entails.

Chapter 2 builds on the foundations of text mining to explore how political actors use language to reference the past, frame contemporary events, and justify change. Readers will investigate how discourses surrounding memory, legitimacy, and reform are shaped by the invocation of historical events in parliamentary speech—such as how British politicians referenced the Abolition of Slavery in 1833 or drew on imagery from the Glorious Revolution during the debates that preceded the Reform Acts of 1832 and 1867. In doing so, the chapter introduces new analytical techniques including the use of n-grams (or, multi-word phrases), joins between textual data and metadata, and the application of a “controlled vocabulary” to systematically track mentions of key historical events. Working with metadata on speaker identity and speech dates, readers will learn how to merge datasets using `left_join()` and explore patterns in parliamentary language surrounding public protests, social class, suffrage, and more. As in previous chapters, the focus is not only on counting but on reading: bigram analysis provides clues that prompt further inquiry into the political and rhetorical context in which phrases like “the will of the people” or “public meeting” were deployed. The chapter emphasizes that text mining is an iterative, interpretive process, one that combines quantitative methods with close reading to support historically grounded questions. Ultimately, this chapter models how historians and analysts can use text-as-data methods to investigate the shifting contours of political discourse and collective memory—and better understand how appeals to the past help shape the political possibilities of the present.

Chapter 3 TBD - Jo

Chapter 4 introduces two quantitative approaches for analyzing textual data—distinctiveness and distance—as a way of uncovering patterns in language use that may not be apparent through close reading alone. Distinctiveness measures, such as term frequency-inverse document frequency (TF-IDF), help identify words that are especially characteristic of a given speaker, time period, or document. Readers will apply TF-IDF to explore which terms were most prominent in the speeches of specific individuals, such as Gladstone and Disraeli, and how the vocabulary of political debate shifts across decades. The chapter also introduces the concept of distance between corpora, defined through statistical measures such as Partial Jensen-Shannon Divergence (JSD). Where distinctiveness isolates what is unique within a single corpus, distance allows analysts to compare the overall similarity between corpora—for example, by measuring how the language of the 1850s differs from that of the 1840s. In the course of working through these methods, readers will gain experience with core text processing techniques in R, including tokenization, filtering low-frequency terms, combining datasets, and managing memory in larger-scale analyses. The chapter also addresses common sources of noise in historical corpora, such as OCR errors, and shows how choices in data cleaning and preprocessing have an impact the outcome of quantitative analysis. By the end of the chapter, readers will not only understand how to compute TF-IDF and JSD, but also how to interpret the results historically—asking not just which words are distinct or distant, but why. The goal is to develop a reflective practice in which algorithmic methods inform, rather than replace, humanistic judgment.

Chapter 5 explores how meaning is constructed through grammatical relationships between words. Drawing on tools from computational linguistics, the chapter differentiates between lexical analysis, which classifies words by type (e.g., nouns, verbs, adjectives), and syntactic analysis, which examines how those words function within larger sentence structures. In particular, it focuses on how syntactic analysis and text mining can be used to analyze the Hansard debates by identifying grammatical subjects (those who perform actions) and objects (those who are acted upon), thereby uncovering how agency is distributed in historical discourse. By examining who is positioned as the subject and who as the object, analysts can trace how power, action, and responsibility are assigned—or withheld—across different social groups. For example, while a lexical search might reveal how frequently the term “woman” appears in a corpus, syntactic analysis allows us to ask whether women are represented as political agents enacting change or as passive recipients of legislation and discourse. To support this inquiry, the chapter introduces two complementary techniques: dependency parsing using the spacyr package, and the use of generative AI to identify and extract grammatical patterns. Using these methods, readers will practice identifying subject-verb-object relationships and analyzing adjective-noun pairings that reflect underlying cultural assumptions. One example explored in the chapter is the prominence of “marry” as the most frequent adjective modifying “woman” in the 1870 parliamentary debates—a linguistic pattern that both reflects and reinforces Victorian ideals about gender, marriage, and morality. Such patterns reveal how legislative language encoded normative roles for women and constructed narrow frameworks for acceptable feminine identity.

Chapter 6 TBD - Jo

Chapter 7 

Appendix A: 

Appendix B: 

Appendix C: 

