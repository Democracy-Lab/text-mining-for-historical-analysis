---
title: "First Time Set Up"
output: pdf_document
geometry: "left=1in,right=1in,top=2in,bottom=1in"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
install.packages("tidyverse")
install.packages("kableExtra")
library(tidyverse)
library(kableExtra)
```

# Preface

Text is everywhere around us, in government and corporate reports, in newspapers and emails and blog entries, in the output of Artificial Intelligence. There are many ways of learning to code that will cause unnecessary delay to the user who cares about text as a route to understanding competing  ideas, culture, politics, and arguments.  For the user who principally approaches code as a means to a serious reckoning with these topics, many introductions to Python and R are unnecessarily slow. They begin with exercises in creating a virtual "calculator" and dwell on datasets composed of numbers, not words. The scholar who wants to understand text but begins with those other textbooks is liable to bemoan lost hours irrelevant to their studies. In an undertaking at the fringe of an established field, which initial instructor one meets is a parameter with enormous consequences. 

At the time of writing, few resources offer to take a novice all the way to competency through a guided series of lessons tailored to scholars’ actual concerns. Over several years of teaching students of History and Computer Science to text mine for historical analysis, we came to the conclusion that it was vital that scholars wishing to learn these techniques have access to a deliberate, encompassing, soup-to-nuts series of lessons that apply the techniques of textual analysis to the concerns of History. We shared drafts of the manuscript and the accompanying code with our students, and these drafts have received the accolades of several cohorts of users for their transparency.  

This book introduces text mining for historical analysis using the R programming language. Many of the exercises in this book are based on code samples from Jo Guldi’s _The Dangerous Art of Text Mining_ (2023) that have been rewritten with the purpose of offering an practical point of entry into the practice of digital history for those with no previous acquaintance with the basic methods of programming.

The methods in this book are intended to be generalizable to other questions beyond history and other fields beyond British history in particular; the student of hip hop, the law-school student, or the analyst undertaking a profile of the crises recently faced by a sector of industry will therefore find many tools in this book that can be generalized to other studies. 

The intended readership of _Text Mining for Historical Analysis_ are analysts, scholars, or students interested in learning about the methods used in digital history. Many of the coding exercises include material and explanation geared towards those who have never programmed before, or who have some programming experience but are new to R and its historical application. The first two chapters offer guided approaches to the programming concepts that are built on throughout this book. To help readers with different needs approach the book, we have added material in the conclusion and appendices to help readers with diverse goals reach strategies appropriate to different levels of skills, for example readers who are anxious to create their own dataset and apply code later.  

More than just a code cookbook, however, _Text Mining for Historical Analysis_ exemplifies the tenets of “critical search,” a framework for analysis that insists on performing strategic reading and implementing multiple methods of measurement. Any search through text illuminates various dimensions of a corpus while obfuscating others, and critical searching can enhance the analyst’s sense of the text measured by considering how certain vantage points share different information.

This framework for analysis also encourages analysts to use quantitative measurements side-by-side with close reading techniques. Contextual, historical knowledge should inform our assessment of the linguistic trends belonging to historical structures of meaning such as discourses and contested concepts. 

It is our hope that _Text Mining for Historical Analysis_ supplies its readers with the tools and perspective that incite a discerning sense of discovery and that enable readings of history that share the insight uniquely offered by computational analysis. 

## The Challenging Skill of Detecting the Unepected

Data analysts who know how to deploy sentiment analysis, word count, and topic modeling often propose simplistic computational models for humanistic problems. Such slapdash applications of technical skill reduce the dimensionality of a research question in a way that produces answers that experts familiar with the problem will find trivial. Meaningful research, by contrast, must draw out unexepected results from the data. The challenge of "surprising" research -- which is as meaningful in a political or corporate context as it is in a humanistic one -- is that to find what is genuinely unexpected, the analyst must already have a relatively deep familiarity with what researchers in the field expect to know. 

This book introduces several approaches that support a practice of text mining designed to create insight. The exercises in coding in this book attempt to model a form of engagement most likely to produce genuine surprise and meaning for experts familiar with the case. One crucial skill set of a researcher who works with data is the ability to familiarize themselves with the many different sources and conversations that form the context for a dataset.  In traditional history classes, instructors typically introduce students to background reading of what professional historians have said about a subject (secondary sources) as well as the original text of the debates (primary sources), as well as more general insights from nearby fields in the humanities and social sciences about grand themes such as the operation of democracy, the problems of gender and race, or the challenges of understanding technology (insights that typically go by the name of 'theory'). 

Amongst the most important skills for an insight-producing practice of text mining is "iteration," a practice long been theorized in the digital humanities – and before that in computational work – as a key method of working with data.  In the 1970s, computational scholars formalized an approach called “Exploratory Data Analysis” (or EDA) as a series of steps from moving from initial data visualizations to data visualizations focused around a specific question.  In one recent book, political scientist Brandon Stewart and his collaborators stress the importance of iterative engagements with the data, moving between theory-based lines of questioning ("induction") and data-based lines of questioning ("deduction") in a cycle that the political scientists refer to as "abductive logic."  Scholars like Lauren Tilton have examined how EDA can work in the history classroom, for example as an impetus to hone a research problem from a timeline of all the photographs in a section of the Library of Congress to the timing of content in photographs taken by two prominent photographers. The coding exercises that follow model iterativity in their approach. In modeling a dozen different approaches to how speech differed before and after the Reform Act of 1832, we show how cutting and recutting the same data according to slightly adjusted lines of questioning ultimately produces much better results than a single application of a statistical measure. 

Another skill for producing insight from text mining is an appreciation of the dynamics of "validation" and "discovery." At times, it is important for scholars to understand the function of code that replicates what they already know.  A topic model of Victorian novels that results in a selection of topics that look familiar to readers of Victorian novels is a good sign – it shows that the computer indeed identifies the patterns that generations of scholarly readers discovered.  Any method that only validates earlier discoveries is useless; to be useful for scholarly discovery and innovation, a method must also be capable of contributing to these new discoveries. As a rule of thumb, we don’t prioritize discovery over validation; rather, we look to discovery as an ultimate step after a series of visualizations designed to validate different aspects of theory and secondary research. Researchers sometimes refer to an “80/20” rule of validation to discovery sometimes used in the sciences, where 80% of work with data is expected to recapitulate already known features of the data, and only 20% of the work leads to new discoveries. In keeping with this set of expectations, we will lead readers through a process where some 80% of the coding exercises given here will not lead to earth-shattering discoveries about the history of Britain’s parliament, or any other textual database to which the code may later be applied. The promise of this kind of work is a process of interrogation, whereby coding leads to precise questions about language driven by broader reading. Those precise questions ultimately lead to data-driven analysis which in fact constitute new findings in 20% of the cases. This textbook will take us through validation to discovery, but it preaches and demonstrates patient engagement with the process as a principle of productive textual analysis. 

Another key to producing insight is the skill of growing curious about algorithms and the many ways in which they can be applied. 	In The Dangerous Art of Text Mining, one of us attempted to formalize the process of iteration in digital humanities work through a process we call “Critical Search.”  Critical Search outlines a series of “ideal” methods (in the sense that they offer a possible although not universal set of iterations), where the scholar moves from reading historical overviews of the period and debates in secondary sources to working with data, to reading critical theory to engage with the most pressing questions around the period, to finding new insights in the data, to adjusting the algorithm used or the algorithm’s parameters to fit the scholar’s questions, to testing those insights against the secondary literature to see whether previous scholars have engaged these episodes, to reading in the primary sources to engage the literature, returning to each of these.  

A Critical Search approach to algorithms is modeled in the exercises that follow by a careful explanation of each technical tool, followed by iterative applications of each tool to explore its use in different ways.  In *The Dangerous Art of Text Mining,* one of us has extensively argued the case for using different algorithms to investigate the same substantive question because each algorithm and each adjustment to algorithm parameters will produce different results. In short, there is no one "best" algorithm for any investigation. A more intelligent approach leverages several related algorithms to investigate the hidden dimensions of difference latent in the data.  In case studies about the use of distinctiveness measures, we will apply the same algorithm to study individual behavior and temporal difference. We will compare the effect of using different algorithms designed to measure similar phenomenon, eschewing the claim that there is one "best" algorithm for any subject in favor of a genuine curiosity about patterns hidden in the data that may produce surprise and insight. 

A Critical Search approach to text is modeled in exercises that move from data-driven overviews of top words to an inquiry about the speakers of those words and their use of the words in context. The exercises that follow model moving through the data from quantitative overviews back to the original words in context. Great insight almost never comes by simply reading a word cloud of top words; it comes after moving between data-driven visualizations and actual words in context over several iterations. In the exercises that follow, we have set out to model how researchers produce insight by putting actual texts in dialogue with quantitative visualizations.  Data modeling is a crucial part of the process, but it is not the sole location where interventions and thinking are required. 

Engaging in background reading from the humanities and social sciences is a crucial means  of sharpening the skills of the textual analyst, and one that too many graudates of data-science or computer-science programs do without. The deeper a scholar has engaged with critical theory, the more subtle their interventions will be; indeed, this is one of the crucial places in the research process where students of computer or data science who have taken few humanities classes are at a distinct disadvantage. The data scientist who knows that gender is interesting as a category of analysis may have the instinct to count feminine and masculine pronouns, but the humanist who has deeply engaged problems of gender may think about how agency – cultural projections of the authority to act – is encoded at the level of grammar, an insight that may lead them to grow curious about the different verbs used with feminine and masculine pronouns.  This example, which we engage in the text (?), is merely one case among thousands that a creative scholar of culture and society might draw from in their analysis of language; what other data strategies might a scholar armed with critical theory pursue? Perhaps they will analyze gender and place in the landscape, or ethnicity and agency with respect to the state. They may examine the gender of political speakers and which subjects women in parliament were willing to introduce (as Blaxill and Blevins did), or they may create another textual investigation entirely on the basis of their readings of critical theory. These engagements are fertile; the examples now known in the digital humanities are the tip of the iceberg of what will become possible as scholars even more deeply engaged with theory begin to engage with tools of this kind.

Above all, focusing on insight means using critical thinking to ask what the counts of words really show when historical context and the words on the page are taken into account. A data analyst who does not take steps of this kind into account is liable to produce an analysis of text that is either obvious or simply wrong.  Consider the following example. During classroom experiments, some of our students concluded that an upward tick of the phrase "ignorant women" across the nineteenth century means that there were actually more ignorant women over time.  In fact, the phrase indicates the formation of what historians call a "discourse," that is, an increasingly accepted representation of reality.  A representation, however, is not the same as reality.  In the nineteenth century, speakers in parliament found that talking about the ignorance of women was useful as a tool to justify the much of the legislation they were trying to pass, especially in the era of the Contagious Disease Acts (1864-69). Parliamentary speakers -- again almost exclusively male -- blamed women for the spread of venereal disease through the British navy. The habit of blaming women had profoundly negative consequences for contemporary women, who lost important liberties during this period.  Because of political habits of blaming women, the laws were written in such a way that it was women who were arrested in the attempt to stem the spread of disease. The historically-informed interpretation of the changing pattern of words produces an answer that is much more interesting, meaningful, and surprising than the naive interpretation.

## Insight Comes from Understanding Background Context: The Value of History

Producing insight worthy of the attention of multiple fields is no simple game. It cannot be produced at the touch of a button or the application of a new algorithm to new data; it requires adjusting the algorithm, rethinking the questions, examining the data, and iterating through the work until something truly surprising has come to light. In general, this caliber of work can only be accomplished by teams that are equally serious about history as they are about algorithmic study.   Showing how to make use of code that confirms this knowledge will allow us to make the point that examples that show what scholars already know are important for validating the code as a human user. 

The point of this example is that an intelligent, reflective process of text mining requires more than the skills of counting words.  One must know a little about the history of a subject before one uses text mining to make new discoveries.  One must also have the power of reflection on where language operates as a game where assertions are made that might not reflect the truth of reality, and where the spread of those assertions, or their formation into a discourse composed of many apparent facts, can have real-world consequences in history.  Counting words and phrases gives us hints about how discourses changed, but it is not in itself sufficient to produce understanding.  

When analysts skip over the need to engage with what other historians have found, one frequent result is a data-driven analysis that makes no sense to expert practitioners.  Sadly, this mode of analysis has become more and more frequent with time -- an issue diagnosed in *The Dangerous Art of Text Mining* as one of the perils of a practice of data science as practiced without respect for the high standards of proof and argumentation typical of humanities training and history in particular.  

A data scientist who aims at genuine insight must begin by understanding the wealth of these debates about every area of expertise, from market behavior to popular culture, and the limits of what any dataset can provide -- no matter how skilfully the data has been organized.  Insight into data, that is, begins with understanding the bias of the dataset -- its origins, its omissions, and its slant.  The data scientist should proceed by carefully describing the limits to how a given dataset can bie interpreted.  For instance, in the dataset most intensively used in this book -- the Hansard parliamentary debates -- which excludes the voices of women, most of the working class, as well as colonized subjects of British empire. A data scientist wholly new to historical analysis will find a model in these exercises for what careful inquiry looks like. Techniques for moving between multiple sources -- secondary, primary, and data-driven analysis -- is one of the most important interventions that this book has to offer. 

Meaningful results from text mining require what one of us has elsewhere called "hybrid” work, where computer scientists and humanists work on the same team to produce new knowledge, collaborating over multiple iterations and investigations. We use the language of "hybridity" to suggest something that is alive and ongoing in a research practice -- that is, in contrast to "interdisciplinarity," which can be supported by reading a single book or a single coding task executed once, a "hybrid" practice is one where teams of researchers from different backgrounds continue to learn from each other over months or years, developing new strategies to approach each others' concerns. The principle of hybrid work -- of learning about where the data comes from and what its limits or biases are, of studying who has approached the dataset and its questions before, of returning to the text -- is a salutory practice for students of information science whether they expect to find themselves working on the analysis of hip hop lyrics, political speeches, social media, or corporate reports. A dialogue between the concerns of historians and the findings of data science is one of the components of insight from text mining.  

Engagement with historical secondary sources is one of the markers of work that takes historical analysis seriously. Throughout the book, the reader will find summaries of arguments about research from the field of history. We offer many shortcuts by way of summaries of the insights of other historians.Because we are writing for a general readership, will not assume a general knowledge of British History or the significance of the examples here; we will introduce British Prime Ministers and major events in a way that may surprise some readers.  Throughout these case studies, we will undertake basic explanations that presume no background knowledge, in a way that may feel simplistic to some readers with greater acquaintance with a given specialty. These summaries, if simplistic, are meant to speed the learning of readers who may come to the book from a background in information science or another field of the humanities, without any idea of why British historians talk about the year 1832 with heightened investment.   

There is much that a textbook on coding cannot hope to teach. Even while we will undertake preliminary explanations of both computer science and historical thinking, an introduction to coding such as this book cannot substitute for an introduction to the field of History nor  to the history of democracy, the history of the United States or Great Britain -- all subjects that intersect with the themes treated here.  In an ideal world, a student studying data science or computer science who encounters this book might simultaneously read this text alongside an introductory course on British history. With such a background, the student would be able to more intelligently absorb the significance of the examples undertaken here.

A data-driven practice of text mining that takes historical research seriously can provide higher standard of innovation in scholarship – by demanding findings that are literally surprising to the field– is a healthy practice, raising the bar for computational investigations of the human past. 

## A General Method for Quantitative Historical Research into Text

The orienting conceptual framework here called "historical analysis" has its origins in a mode of understanding the lifespans of individuals and collectives that goes back to the time of Herodotus. Historical analysis simply means reflection on change over time. The life in question may be the life of an individual or a collective, and the span may range from weeks or months to centuries or more -- depending on the coherence and quality of the evidence to be analyzed.  As such, historical analysis is a useful tool to understanding the life of any political, corporate, or cultural institution or group of practitioners. The theory behind the code can be useful to any work where critical thinking about social and cultural dimensions of data is necessary.

Historical change is generally understood in terms of events that have a path-determining quality on later episodes of life and thought; these events divide experience into eras or periods that share certain features which can be quantitatively analyzed. But historical analysis is also dense and varied; within any period, there are exceptions, holdovers, and anticipations of the new. Individuals may anticipate future events or defy the direction of history in their time.  Historical analysis may lead us towards questions of cause and effect, in which we try to unpack the relative impact of individuals on their respective futures. Or historical analysis may instead turn backwards and register the changing impact of memory as contemporary actors reflected on events in their respective past. Investigating the categories of temporal experience -- including event, period, agency, causality, and memory -- is among the tremendous undertakings of researchers in university departments of history.  The case studies in this book are intended as demonstrations that counting words can drive a thoughtful analysis of change over time driven by a respect for historians' categories. 

Applied to the understanding of law, industry, politics, or culture, historical analysis promises a richer portrait of individual and collective behavior than less nuanced methodologies. Text mining for historical analysis can be generalized to practically any flavor of inquiry based on text – whether intellectual history, histories of the state, or histories of identity and experience -- so long as an archive of text exists with relative cohesion and coverage for a continuous period of time.  We could have analyzed how hip hop lyrics or obituaries changed over time.

One aspect of the approach we preach is a respect for subject matter expertise and historical context in any choice made about data and its analysis. While we could, in theory, download a dataset of lyrics that document the history of the modern recording industry, we ourselves are no experts in that domain. The case studies we present are mainly from British History because one of us pursued a PhD in British History.  

In modeling our own expertise around British history, and showing how a familiarity with the parliamentary debates of Great Britain can drive an inquiry into the role of individuals in the abolition of slavery or the consequences of the Reform Act of 1832, we are attempting to model questions about major events, collective action, changing values, and individual interventions that appear at every period of history and every place in different guises. The student of hip hop or of Indian history will find questions just as deep and consequential in their own studies. Even the student of British history may quibble with the questions that we have chosen to undertake. Given the approaches to text mining presented here, they may just as easily pursue their own lines of inquiry.

No book teaches all subjects at once. This book offers an introduction to text mining for historical analysis via the programming language R.  This book only lightly treats the theory of using words as a means of navigating historical change over time -- a theme of great practical interest to many practicing data scientists.  Throughout the book, we will lightly summarize the background thinking that a reader is required to have to arrive at an adequate interpretation of why the language changed the way it did and what the analyst can learn from changing language, but a more intense engagement with the theory of ideas, concepts, and historical change must be sought elsewhere. As we will emphasize again and again, there must be a dance between text mining -- the counting of words -- and expertise about historical context. Our desire in such a general approach is to offer a textbook that will be useful to our colleagues and friends in San Francisco, Dallas, Atlanta, Bogota, Melbourne, Hong Kong, and Chennai, and to formulate an approach to text mining designed to elevate the methods of data scientists and historians alike. 

## A General Introduction to Computing

For many first-time coders, is hard to know where to get started with text. They may find themselves googling instructions, all the while asking themselves, should I begin with cleaning my data, or learning the basics of command-line instructions, or counting words, and should I commit to the computer language Python or R? So many choices have to be made for an individual program of study. 

Readers learning to code for the first time need a focused program of study that will equip them to perform exercises relevant to most scholars trying to make sense of change over time, not a series of disjoint lessons possibly irrelevant to their concern.  We have organized many of the main concerns that greet students of text into a simple series of lessons, choosing a language and software package that allow an analyst to become functionally competent in text mining for historical analysis without getting lost in questions of data type. 

At the time of writing, few resources offer to take a novice all the way to competency through a guided series of lessons tailored to scholars’ actual concerns. Over several years of teaching students of History and Computer Science to text mine for historical analysis, we came to the conclusion that it was vital that scholars wishing to learn these techniques have access to a deliberate, encompassing, soup-to-nuts series of lessons that apply the techniques of textual analysis to the concerns of History. We shared drafts of the manuscript and the accompanying code with our students, and these drafts have received the accolades of several cohorts of users for their transparency.  

A book-length publication solves the riddle that most beginners face when they wonder about when to clean their data, when to learn about their computer’s file systems, whether they should practice “scraping” new data from the cloud, the arcane question of “joins” and dataframes, and whether to aim for topic models or word embeddings. Designed to take the novice from meeting code for the first time through the deployment of the major algorithms that are used in digital history today, this book fills a gap left between the many individual lessons scattered around the internet today.  

In writing about code, we have taken pains to communicate basic concepts from computer science, defining terms such as "function," "memory," and "data type." Wherever possible, we have defined these terms when they first arise; a glossary of specialist is also provided in the appendix. Our methods of explanation therefore alternate between an introduction to the *grammar* of code -- intended to familiarize readers with how computer scientists think about the language with which they communicate -- and *metaphors* for code, which are intended to convey what particular actions of code accomplish through analogies that may be easier to grasp for those who have enver encountered the concepts before. 

It is intended that this book could serve as a textbook for a first-year course in computer science with the idea that a course based on Text Mining for Historical Analysis could be used as the intrdocutory course in a Computer Science department, and that those who complete the textbook would be able to move onto further work in Computer Science without disorientation. 

## The Data in this Book

In our chapter on data, we explore where data comes from and how it is prepared. Once data has been prepared in the way that we propose, the debates of legislatures in Hong Kong or the legal battles of peasants in Peru can be treated as we have treated the Parliamentary Debates – as historical documents whose changing patterns of referents can be examined over time. 

The experiments in this book all concern one dataset, commonly known as "Hansard," which contains the official record of the debates of Britain's House of Lords and House of Commons, 1803-1899. It is a highly interesting data set because the debates cover the transformations of the Industrial Revolution, the abolition of the trans-Atlantic slave trade by the British navy, the struggle for women's rights, and many other debates of general interest to readers of modern history. 

One advantage of exploring the British parliamentary debates, as we do for most of the volume, is that we have jointly worked for at least a decade on the datasets presented in this book. Our familiarity allows us to model an intelligent engagement with issues of data quality and historical inquiry -- something that would be more difficult to organize in a textbook that moved from dataset to dataset at random.  Deep familiarity with a dataset, its historical context, and the kinds of historical questions that might arise when looking at a dataaset is one of the prerequisites for serious analysis of change over time. 

As we will explore in the chapters that follow, Hansard is in many ways an imperfect data set. The Hansard data dates from a moment in history before the advent of modern technologies of transcription, including shorthand and electronic recording. Instead, it was compiled from press reports and notes, highly edited for what journalists assumed would be of interest to contemporary readers, and sometimes paraphrased to keep pace with the speaker.  Nonetheless, even in the early versions of the reports, a great deal of data about the social, economic, and political realities facing parliament remain preserved.  Though far from a verbatim transcript of the records of parliament, it is nevertheless an important historical source that has served generations of historians, many of them working with concerns social and cultural as well as political and intellectual. 

Part of what text mining can accomplish is an investigation into the record of bias left by an earlier age. The Hansard dataset is an artifact from an era where the vote was limited to a tiny portion of the aristocracy (before 1832) and the middle class (to 1867).  The record is also almost exclusively male, as women could not vote in Britain before 1918.  Britain of course controlled a global empire, and many of the speakers expressed disdainful attitudes towards their Roman Catholics, Indians, Africans, and Irish subjects; indeed, historians have reasoned that it was the British willingness to fabricate arguments about racial superiority and the necessity of violence that allowed the empire to exist.  The techniques in this volume allow the student of history to examine British attitudes about gender, race, and class for themselves.

## Using the Book With Your Own Data

What if you aren't a historian of nineteenth-century British parliament?  The tools of word count and analysis from this book can be applied to different data sets as soon as they are learned. New data sets are constantly coming into use, and scholars of practically any moment in history may find data to which the following lessons can be applied with relatively minor adjustments such as matching the names of columns where speaker, date, and speech data are held.  By the book's end, we will offer advice to those working on the Congressional debates of twentieth-century America.  More importantly, however, we submit that the exercises will still be in principle interesting, as they are formed around the problem of counting words. 

For first-time coders, we recommend working through the execises at least once using the data provided in the examples. One of the great delights of text mining is that a set of code can be applied to a new dataset with an adjustment of a half-dozen lines of code. In other words, once you have a dataset of text over time, the exercises in this book can help you produce meaningful insights about that dataset, no matter its content, with relatively little adjustment. 

What about when you're ready to model your own data? Where does one acquire the data, and how do you get it into the proper form? The answer depends on the state of available data resource,  the length of time or money that an analyst has to commit to the process in question, and whether one has previously attempted a data-mining project of any kind.

When talking to colleagues who intent to work on text mining for the first time, we  advise them to reach for data sets that are ready to go -- that is, data sets that have been prepared for study by other teams of researchers. As of the writing of this book, looking for ready-to-wear datasets typically means that one will work on one of the few datasets that are considered so broadly relevant to researchers across the humanities and social sciences that resources have previously been deployed to making the text accessible as data. Frequent targets include databases of novels and screenplays, databases of court rulings and opinions, or the debates of a legislature (perhaps not the U.S. or U.K., but that of another contemporary nation, a former colony or a European power). 

For the majority of historians, trained to identify archives that no other scholars have previously consulted, the idea of looking at novels, court rulings, newspaper articles, or political debates may feel only tangentially related to their field of study. But the joy of these general databases is that so vast are their themes that any given area of study may be refracted through the text in question. Contemplating whether these ready-to-go datasets are relevant to a particular question is an exercise that requires scholars to use their imagination about how their own topic instersects with a broader historical context that might include fiction, the courts, the newspapers, or the legislature. 

For example, if the larger project concerns the question of identity around a certain place and time, a ready-to-go dataset might provide a broader context of how that identity was debated in the national legislature over the course of decades -- context that can do a great deal to illuminate the potential significance of the microhistorical work in archives that forms the rest of the project. Thus a historian writing about a unique archive of trade union documents might benefit from a chapter that uses the Congressional debates to examine the longue duree of how Congress talked about trades unions, while the same historian will doubtlessly excavate ideas and personae from the archive who are invisible from the annals of Congress.  Archival microhistory and longue-duree work with digital archives can offer a complementary set of reflections on each other, with text mining offering an opportunity to reflect on grand changes over modernity or pinpointing moments of intense change, and archives offering the opportunity to peer more closely into the dynamics of that moment of change, glimpsing individuals and actions that are invisible from a greater remove. 

For contemporary records or certain kinds of archival material that has already been digitalized by libraries and archives, the process of assembling a dataset may be relatively straightforward. Exercises in the Programming Historian give a straightforward set of steps to scraping the records from an archival digitalization project into a database of a kind that can be examined with the processes in this book. 
  
Users who are engaging in a full-length dissertation or grant-funded project with these methods may approach the problem in a different way.  Funds are sometimes available to support the digitalization of complete archives, and text mining can support extracting information from those datasets once they are produced. In many cases, scholars will want to assemble a team of archivists and data specialists who can collaborate on identifying a set of archival documents to be digitalized and a strategy for creating a digital dataset. For this approach, problems of data quality are extremely imoprtant to addresss early on; our chapter on data gives some pointers about what constitutes a reasonable quality of textual data for text mining to become useful. 

The landscape of data is swiftly changing.  We already hear about graduate students who have had great success in digitalizing photographs from the archives of typescript or even handwritten manuscripts by using Large Language Models (LLMs) such as ChatGPT. Such practices  stand to dramatically lower the cost of digitalization. This horizon will doubtlessly change over the near horizon.  

While this textbook does not provide detailed instruction in readying a dataset with the help of LLM's, our chapter on data discusses the target of any digitalization process -- a clean-enough dataset with a column for text and a column for the date corresponding to when that text was produced. Once the analyst has such a dataset in hand, they may use their own dataset as the basis for any of the analyses discussed in the chapters. 
