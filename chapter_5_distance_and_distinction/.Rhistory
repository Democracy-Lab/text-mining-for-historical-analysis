# and keep the top 30 most distinctive words across the two decades
word_dist <- word_dist %>%
arrange(desc(abs(Partial_JSD))) %>%  # Sort by highest absolute contribution
slice(1:30)                          # Keep top 30 words
# create a mirrored bar chart showing which words contribute most to linguistic shift
# between 1840 and 1850
# bars are flipped to show direction, based on partial jsd
ggplot(word_dist, aes(x = reorder(word, Partial_JSD), # order words by divergence
y = Partial_JSD, # y-axis is signed partial jsd
fill = Direction)) + # fill bars by direction label
geom_bar(stat = "identity") + # draw bars with exact values
coord_flip() + # flip axes for horizontal layout
labs(title = "Partial JSD of Top Words: 1840 vs. 1850", # chart title
x = "word", # x-axis label
y = "partial jsd value", # y-axis label
fill = "change direction") + # legend title
theme_minimal() + # use a clean minimal theme
scale_fill_manual(values = c("1840 → 1850" = "red", # set color for 1840 shift
"1850 → 1840" = "blue")) + # set color for 1850 shift
theme(legend.position = "bottom") # move legend below plot
library(ggridges)
ggplot(word_dist, aes(x = Partial_JSD, y = Direction, fill = Direction)) +
geom_density_ridges(alpha = 0.7) +
scale_fill_manual(values = c("1840 → 1850" = "blue", "1850 → 1840" = "red")) +
labs(title = "Density of Partial JSD by Direction",
x = "Partial JSD",
y = "Direction of Change") +
theme_minimal()
library(wordcloud)
wordcloud(
words = word_dist$word,
freq = abs(word_dist$Partial_JSD),
colors = ifelse(word_dist$Direction == "1840 → 1850", "blue", "red"),
scale = c(4, 0.5),
random.order = FALSE)
library(text2vec)
knitr::opts_chunk$set(echo = TRUE)
list.files()
list.files(path = ".", full.names = TRUE)
file_path <- file.path("data", "raw", "speeches.csv")
file_path
library(here)
relative_file_path<- here("data", "raw", "speeches.csv")
relative_file_path
print(paste("Your current working directory is:", current_dir))
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("/home/stephbuongiorno/Desktop/Text Mining for Historical Analysis/appendix_b/file_paths_are_trees.jpg")
getwd()
file_path <- file.path("data", "raw", "speeches.csv")
file_path
library(here)
relative_file_path<- here("data", "raw", "speeches.csv")
relative_file_path
library(tidyverse)
library(tidytext)
library(fs)
# function to process and plot top words for a given decade
plot_decade <- function(decade) {
dataset_name <- paste0("hansard_", decade)
data(list = dataset_name, package = "hansardr", envir = environment())
hansard_data <- get(dataset_name)
p <- hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_head(n = 20) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(
title = paste("Top Words in Hansard Debates,", decade, "s"),
x = "Word",
y = "Count") +
theme_minimal()
outfile <- file.path(current_dir, paste0("hansard_", decade, "_topwords.png"))
ggsave(outfile, plot = p, width = 6, height = 4, dpi = 300)
message("Saved: ", outfile) }
decades <- c("1800", "1810", "1820")
current_dir <- getwd()
print(paste("Visualizations will be saved to:", current_dir))
for (decade in decades) {
print(paste0("Processing ", decade))
plot_decade(decade) }
list.files()
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("/home/stephbuongiorno/Desktop/Text Mining for Historical Analysis/appendix_b/file_paths_are_trees.jpg")
getwd()
file_path <- file.path("data", "raw", "speeches.csv")
file_path
library(here)
relative_file_path<- here("data", "raw", "speeches.csv")
relative_file_path
library(tidyverse)
library(tidytext)
library(fs)
# function to process and plot top words for a given decade
# it takes the decade of interest and the name of the directory
# where the visualization will be saved
plot_decade <- function(decade, output_dir) {
dataset_name <- paste0("hansard_", decade)
data(list = dataset_name, package = "hansardr", envir = environment())
hansard_data <- get(dataset_name)
p <- hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_head(n = 20) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(
title = paste("Top Words in Hansard Debates,", decade, "s"),
x = "Word",
y = "Count") +
theme_minimal()
outfile <- file.path(output_dir, paste0("hansard_", decade, "_topwords.png"))
ggsave(outfile, plot = p, width = 6, height = 4, dpi = 300)
message("Saved: ", outfile) }
# define the decades of interest
decades <- c("1800", "1810", "1820")
# find your current working directory
# your visualizations will be saved here
output_dir <- getwd()
print(paste("Visualizations will be saved to:", current_dir))
for (decade in decades) {
print(paste0("Processing ", decade))
plot_decade(decade, output_dir) }
list.files()
getwd()
# Define decades of interest
decades <- c("1800", "1810", "1820")
# Loop through each decade
for (decade in decades) {
# Create a new directory for the current decade
new_dir <- paste0("vis_", decade)
# Create the directory if it doesn't exist
dir_create(new_dir)
print(paste("Created folder:", new_dir))
# Print progress message
print(paste("Processing", decade))
# Set output directory for plot_decade() to save inside this folder
outdir <- new_dir
# Use your plotting function
plot_decade(decade) }
# Define decades of interest
decades <- c("1800", "1810", "1820")
# Loop through each decade
for (decade in decades) {
# Create a new directory for the current decade
new_dir <- paste0("vis_", decade)
# Create the directory if it doesn't exist
dir_create(new_dir)
print(paste("Created folder:", new_dir))
# Print progress message
print(paste("Processing", decade))
# Set output directory for plot_decade() to save inside this folder
# Use your plotting function
plot_decade(decade, new_dir) }
list.files()
knitr::opts_chunk$set(echo = TRUE)
# URLs for plain text versions
looking_glass_url <- "https://www.gutenberg.org/files/12/12-0.txt"
peter_url <- "https://www.gutenberg.org/cache/epub/14838/pg14838.txt"
# Download and save Alice's Adventures in Wonderland
looking_glass_text <- readLines(looking_glass_url, encoding = "UTF-8", warn = FALSE)
# Download and save The Tale of Peter Rabbit
peter_text <- readLines(peter_url, encoding = "UTF-8", warn = FALSE)
cat("Texts downloaded successfully.\n")
# Load required libraries
library(tidyverse)
library(tidytext)
# Convert to data frames
looking_glass_df <- tibble(writer = "Lewis Carroll", text = looking_glass_text)
peter_df <- tibble(writer = "Beatrix Potter", text = peter_text)
# Combine both texts
books <- bind_rows(looking_glass_df, peter_df)
# Tokenize: split into individual words
tokens <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$"))  # keep only alphabetic words
# Calculate word frequencies per author
word_counts <- tokens %>%
count(writer, word) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0)
# Focus on selected words of interest
target_words <- c("rabbit", "she", "little", "chortled", "shed", "the", "hoe", "slithy", "galumph")
selected <- word_counts %>%
select(writer, any_of(target_words))
# View the table
print(selected)
# Count word frequencies per author (long format)
word_counts2 <- tokens %>%
count(writer, word, sort = TRUE) %>%
rename(document = writer, term = word)
# Compute tf-idf values
tfidf_table <- word_counts2 %>%
bind_tf_idf(term = term, document = document, n = n)
# View top distinctive words for each author
filtered_tfidf <- tfidf_table %>%
filter(term %in% target_words) %>%
arrange(document, desc(tf_idf)) %>%
select(term, tf_idf)
# Create side-by-side tables with style
library(htmltools)
library(kableExtra)
##  Create styled tables as HTML strings
table_left <- kable(filtered_tfidf[1:6, ], format = "html",
caption = "<span style='font-size:18pt; font-weight:bold;'>Some Beatrix Potter Words</span>",
escape = FALSE) %>%
kable_styling(full_width = FALSE)
table_right <- kable(filtered_tfidf[7:12, ], format = "html",
caption = "<span style='font-size:18pt; font-weight:bold;'>Some Lewis Carroll Words</span>",
escape = FALSE) %>%
kable_styling(full_width = FALSE)
browsable(
tagList(
div(HTML(table_left), style = "float: left; width: 48%; margin-right: 2%;"),
div(HTML(table_right), style = "float: right; width: 48%; margin-left: 2%;"),
div(style = "clear: both;")  # clears the floats
)
)
# Create side-by-side tables with style
library(htmltools)
library(kableExtra)
# Ensure tf-idf values print in decimal, not scientific notation
filtered_tfidf <- filtered_tfidf %>%
mutate(tfidf = format(tfidf, scientific = FALSE, digits = 6))
# Create side-by-side tables with style
library(htmltools)
library(kableExtra)
# Ensure tf-idf values print in decimal, not scientific notation
filtered_tfidf <- filtered_tfidf %>%
mutate(tf_idf = format(tf_idf, scientific = FALSE, digits = 6))
# Create styled tables as HTML strings
table_left <- kable(filtered_tfidf[1:6, ], format = "html",
caption = "<span style='font-size:18pt; font-weight:bold;'>Some Beatrix Potter Words</span>",
escape = FALSE) %>%
kable_styling(full_width = FALSE)
table_right <- kable(filtered_tfidf[7:12, ], format = "html",
caption = "<span style='font-size:18pt; font-weight:bold;'>Some Lewis Carroll Words</span>",
escape = FALSE) %>%
kable_styling(full_width = FALSE)
browsable(
tagList(
div(HTML(table_left), style = "float: left; width: 48%; margin-right: 2%;"),
div(HTML(table_right), style = "float: right; width: 48%; margin-left: 2%;"),
div(style = "clear: both;")
)
)
knitr::opts_chunk$set(echo = TRUE)
# (REPLACES L103–L128) — tidyverse conventions, LaTeX/PDF-safe
library(dplyr)
library(knitr)
library(kableExtra)
library(purrr)
library(glue)
left_tab <- filtered_tfidf %>%
slice(1:6) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Some Beatrix Potter Words", escape = TRUE) %>%
kable_styling(latex_options = c("hold_position")) %>%
as.character()
knitr::opts_chunk$set(echo = TRUE)
# URLs for plain text versions
looking_glass_url <- "https://www.gutenberg.org/files/12/12-0.txt"
peter_url <- "https://www.gutenberg.org/cache/epub/14838/pg14838.txt"
# Download and save Alice's Adventures in Wonderland
looking_glass_text <- readLines(looking_glass_url, encoding = "UTF-8", warn = FALSE)
# Download and save The Tale of Peter Rabbit
peter_text <- readLines(peter_url, encoding = "UTF-8", warn = FALSE)
print("Texts downloaded successfully.")
# Load required libraries
library(tidyverse)
library(tidytext)
# Convert to data frames
looking_glass_df <- tibble(writer = "Lewis Carroll", text = looking_glass_text)
peter_df <- tibble(writer = "Beatrix Potter", text = peter_text)
# Combine both texts
books <- bind_rows(looking_glass_df, peter_df)
# Tokenize: split into individual words
tokens <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$"))  # keep only alphabetic words
# Calculate word frequencies per author
word_counts <- tokens %>%
count(writer, word) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0)
# Focus on selected words of interest
target_words <- c("rabbit", "she", "little", "chortled", "shed", "the", "hoe", "slithy", "galumph")
selected <- word_counts %>%
select(writer, any_of(target_words))
# View the table
print(selected)
# Count word frequencies per author (long format)
word_counts2 <- tokens %>%
count(writer, word, sort = TRUE) %>%
rename(document = writer, term = word)
# Compute tf-idf values
tfidf_table <- word_counts2 %>%
bind_tf_idf(term = term, document = document, n = n)
# View top distinctive words for each author
filtered_tfidf <- tfidf_table %>%
filter(term %in% target_words) %>%
arrange(document, desc(tf_idf)) %>%
select(term, tf_idf)
# (REPLACES L103–L128) — tidyverse conventions, LaTeX/PDF-safe
library(dplyr)
library(knitr)
library(kableExtra)
library(purrr)
library(glue)
left_tab <- filtered_tfidf %>%
slice(1:6) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Some Beatrix Potter Words", escape = TRUE) %>%
kable_styling(latex_options = c("hold_position")) %>%
as.character()
right_tab <- filtered_tfidf %>%
slice(7:12) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Some Lewis Carroll Words", escape = TRUE) %>%
kable_styling(latex_options = c("hold_position")) %>%
as.character()
glue(
"\\begin{{minipage}}[t]{{0.48\\textwidth}}
{left_tab}
\\end{{minipage}}\\hfill
\\begin{{minipage}}[t]{{0.48\\textwidth}}
{right_tab}
\\end{{minipage}}"
) %>%
knitr::asis_output()
# (REPLACES YOUR BLOCK) — tidyverse + gt, PDF-safe two-column layout
library(dplyr)
library(purrr)
library(gt)
library(glue)
library(knitr)
library(kableExtra)  # not strictly required, but often loaded elsewhere
# 1) Make a LaTeX-friendly gt table (no HTML/md in headers)
make_gt_table <- function(df) {
df %>%
select(word, tf_idf) %>%
gt() %>%
tab_header(
title = "Top TF-IDF Words",
subtitle = paste0("Decade: ", unique(df$decade))
) %>%
cols_label(
word = "Word",
tf_idf = "TF-IDF Score"
) %>%
fmt_number(columns = tf_idf, decimals = 6, use_seps = FALSE)
}
# 2) Build one gt table per decade
gt_tables <- top_hansard_tf_idf %>%
group_by(decade) %>%
group_split() %>%
map(make_gt_table)
knitr::opts_chunk$set(echo = TRUE)
# URLs for plain text versions
looking_glass_url <- "https://www.gutenberg.org/files/12/12-0.txt"
peter_url <- "https://www.gutenberg.org/cache/epub/14838/pg14838.txt"
# Download and save Alice's Adventures in Wonderland
looking_glass_text <- readLines(looking_glass_url, encoding = "UTF-8", warn = FALSE)
# Download and save The Tale of Peter Rabbit
peter_text <- readLines(peter_url, encoding = "UTF-8", warn = FALSE)
print("Texts downloaded successfully.")
# Load required libraries
library(tidyverse)
library(tidytext)
# Convert to data frames
looking_glass_df <- tibble(writer = "Lewis Carroll", text = looking_glass_text)
peter_df <- tibble(writer = "Beatrix Potter", text = peter_text)
# Combine both texts
books <- bind_rows(looking_glass_df, peter_df)
# Tokenize: split into individual words
tokens <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$"))  # keep only alphabetic words
# Calculate word frequencies per author
word_counts <- tokens %>%
count(writer, word) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0)
# Focus on selected words of interest
target_words <- c("rabbit", "she", "little", "chortled", "shed", "the", "hoe", "slithy", "galumph")
selected <- word_counts %>%
select(writer, any_of(target_words))
# View the table
print(selected)
# Count word frequencies per author (long format)
word_counts2 <- tokens %>%
count(writer, word, sort = TRUE) %>%
rename(document = writer, term = word)
# Compute tf-idf values
tfidf_table <- word_counts2 %>%
bind_tf_idf(term = term, document = document, n = n)
# View top distinctive words for each author
filtered_tfidf <- tfidf_table %>%
filter(term %in% target_words) %>%
arrange(document, desc(tf_idf)) %>%
select(term, tf_idf)
# (REPLACES L103–L128) — tidyverse conventions, LaTeX/PDF-safe
library(dplyr)
library(knitr)
library(kableExtra)
library(purrr)
library(glue)
left_tab <- filtered_tfidf %>%
slice(1:6) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Some Beatrix Potter Words", escape = TRUE) %>%
kable_styling(latex_options = c("hold_position")) %>%
as.character()
right_tab <- filtered_tfidf %>%
slice(7:12) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Some Lewis Carroll Words", escape = TRUE) %>%
kable_styling(latex_options = c("hold_position")) %>%
as.character()
glue(
"\\begin{{minipage}}[t]{{0.48\\textwidth}}
{left_tab}
\\end{{minipage}}\\hfill
\\begin{{minipage}}[t]{{0.48\\textwidth}}
{right_tab}
\\end{{minipage}}"
) %>%
knitr::asis_output()
distinctively_carroll <- tfidf_table %>%
filter(document == "Lewis Carroll") %>%
arrange(desc(tf_idf)) %>%
select(term, tf_idf)
kable(head(distinctively_carroll, 15),
format = "html",
caption = "<span style='font-size:14pt; font-weight:bold;'>Lewis Carroll's Most Distinctive Words (when Alice is Compared with Peter Rabbit)</span>",
escape = FALSE) %>%
kable_styling(full_width = TRUE)
install.packages("text2vec")  # Run this if not already installed
library(text2vec)             # Load the package
# define a function for jsd
jsd <- function(p, q) {
# Ensure p and q are probability distributions
p <- p / sum(p)
q <- q / sum(q)
m <- 0.5 * (p + q)
# Define a helper function for KL divergence
kl_div <- function(a, b) {
a <- ifelse(a == 0, 1, a)  # Avoid log(0)
b <- ifelse(b == 0, 1, b)
sum(a * log2(a / b))
}
0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)
}
# load Alice in Wonderland
alice_url <- "https://www.gutenberg.org/files/11/11-0.txt"
alice_text <- readLines(alice_url, encoding = "UTF-8", warn = FALSE)
# Combine into one data frame
books <- bind_rows(
tibble(booktitle = "Alice", text = alice_text),
tibble(booktitle = "Peter", text = peter_text),
tibble(booktitle = "LookingGlass", text = looking_glass_text)
)
# Tokenise and count
word_freqs <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$")) %>%
count(booktitle, word) %>%
group_by(booktitle) %>%
mutate(freq = n / sum(n)) %>%
ungroup()
# Create a frequency matrix (book x word)
freq_matrix <- word_freqs %>%
select(booktitle, word, freq) %>%
pivot_wider(names_from = word, values_from = freq, values_fill = 0) %>%
column_to_rownames("booktitle") %>%
as.matrix()
# Compute JSD for each pair of books
book_names <- rownames(freq_matrix)
n_books <- nrow(freq_matrix)
jsd_matrix <- matrix(0, n_books, n_books)
rownames(jsd_matrix) <- book_names
colnames(jsd_matrix) <- book_names
for (i in 1:n_books) {
for (j in 1:n_books) {
jsd_matrix[i, j] <- jsd(freq_matrix[i, ], freq_matrix[j, ])
}
}
# View results
kable(jsd_matrix)
# Load our needed R packages
library(hansardr)
library(tidyverse)
library(lubridate)
library(tidytext)
library(gt)
# Load tidytext's built-in list of stop words for filtering out common words
data("stop_words")
# Load the Hansard debates from the 1830s and 1860s
data("hansard_1830")
data("hansard_1860")
# Add a 'decade' column to hansard_1830 to tag all rows with the value 1830
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
# Add a 'decade' column to hansard_1860 to tag all rows with the value 1860
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
