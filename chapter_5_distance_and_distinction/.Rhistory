# Update packages to match the new R version
update.packages(ask = FALSE, checkBuilt = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(text2vec)
library(here)
library(fs)
library(hansardr)
devtools::install("~/Repos/hansardr")
library(hansardr)
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(text2vec)
library(here)
library(fs)
library(hansardr)
devtools::install("~/Repos/hansardr")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(text2vec)
library(here)
library(fs)
library(hansardr)
library(lubridate)
library(tidyverse)
library(hansardr)
library(tidytext)
# Load Hansard data for the 1840s and 1850s
data("hansard_1840")
data("hansard_1850")
# preprocess the 1840s hansard data:
# tokenize the text column into words, remove stop words,
# and filter out tokens that contain digits
hansard_1840 <- hansard_1840 %>% # create a new dataset
unnest_tokens(word, text) %>% # convert text into one word per row
anti_join(stop_words) %>% # remove common stop words
filter(!str_detect(word, "[:digit:]"))  # remove words with digits
# preprocess the 1850s hansard data using the same steps:
# tokenize, remove stop words, and exclude numeric tokens
hansard_1850 <- hansard_1850 %>% # create a new dataset
unnest_tokens(word, text) %>% # tokenize text
anti_join(stop_words) %>% # remove stop words
filter(!str_detect(word, "[:digit:]"))  # exclude tokens with digits
# Function to calculate one-sided KL divergence component of JSD
partial_jsd_1840_to_1850 <- function(p, q) {
m <- 0.5 * (p + q) # Midpoint distribution
return(p * log2(p / m))} # 1830 deviation from midpoint
partial_jsd_1850_to_1840 <- function(p, q) {
m <- 0.5 * (p + q) # Midpoint distribution
return(q * log2(q / m))} # 1860 deviation from midpoint
# define a function for jsd
jsd <- function(p, q) {
# Ensure p and q are probability distributions
p <- p / sum(p)
q <- q / sum(q)
m <- 0.5 * (p + q)
# Define a helper function for KL divergence
kl_div <- function(a, b) {
a <- ifelse(a == 0, 1, a)  # Avoid log(0)
b <- ifelse(b == 0, 1, b)
sum(a * log2(a / b))
}
0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)
}
# Count word frequencies for both decades
freq_1840 <- hansard_1840 %>%
count(word, name = "count_1840")
freq_1850 <- hansard_1850 %>%
count(word, name = "count_1850")
freq_1840
# define how many top words to extract from each decade's corpus.
# this number controls the vocabulary size for comparison.
top_n <- 50  # number of top words to extract
# extract the top n most frequent words from the 1840s word frequency table.
# keep only the word column and return it as a character vector.
top_words_1840 <- freq_1840 %>%
slice_max(order_by = count_1840, n = top_n) %>%  # select top n by count
select(word) %>%  # keep only the word column
deframe()  # convert to a character vector
# extract the top n most frequent words from the 1850s word frequency table.
# repeat the same steps to prepare for cross-decade comparison.
top_words_1850 <- freq_1850 %>%
slice_max(order_by = count_1850, n = top_n) %>%  # select top n by count
select(word) %>%  # keep only the word column
deframe()  # convert to a character vector
# Combine top words from both decades and keep only unique words
all_top_words <- unique(c(top_words_1840, top_words_1850))
# merge the top words from both decades with their frequency tables,
# ensuring that missing words are filled with zero counts to enable comparison
word_dist <- data.frame(word = all_top_words) %>%  # create dataset with union of top words
left_join(freq_1840, by = "word") %>%  # join 1840s word counts
left_join(freq_1850, by = "word") %>%  # join 1850s word counts
replace_na(list(count_1840 = 0, count_1850 = 0))  # replace missing counts with 0
# normalize raw word counts to probabilities by dividing each word's count
# by the total count from its respective decade. this prepares data
# for comparison using divergence or distance metrics.
word_dist <- word_dist %>%
mutate(p_1840 = count_1840 / sum(count_1840),  # normalize 1840 counts
p_1850 = count_1850 / sum(count_1850))  # normalize 1850 counts
# The logarithm of zero (log(0)) is undefined in mathematics—it approaches negative infinity.
# In practice, trying to compute log(0) in R will result in -Inf or NaN, which can break downstream calculations.
# To avoid this, we replace zero probabilities with a very small constant (1e-10),
# which approximates zero but keeps the log function well-defined.
word_dist$p_1840[word_dist$p_1840 == 0] <- 1e-10
word_dist$p_1850[word_dist$p_1850 == 0] <- 1e-10
head(word_dist)
# Compute Partial Jensen-Shannon Divergence (JSD) for each word in both directions:
# - From the 1840 distribution to the 1850 distribution
# - And from 1850 to 1840
# This captures how much each word contributes to the overall divergence between decades,
# depending on the direction of comparison.
word_dist <- word_dist %>%
rowwise() %>%
mutate(Partial_JSD_1840 = partial_jsd_1840_to_1850(p_1840, p_1850), # Contribution from 1840
Partial_JSD_1850 = partial_jsd_1850_to_1840(p_1840, p_1850)) %>% # Contribution from 1850
ungroup()
# For interpretation, assign each word to a single dominant direction:
# - If its contribution is greater when comparing 1840 → 1850, it is labeled "1840 → 1850"
# - Otherwise, it's labeled "1850 → 1840"
# Also, assign a signed Partial JSD value:
# - Negative if the word is more distinctive of 1840 (helps indicate direction visually)
# - Positive if more distinctive of 1850
word_dist <- word_dist %>%
mutate(Direction = ifelse(Partial_JSD_1840 > Partial_JSD_1850,  # decide dominant direction
"1840 → 1850",
"1850 → 1840"),
Partial_JSD = ifelse(Direction == "1840 → 1850",
-Partial_JSD_1840,  # negate to show shift from 1840
Partial_JSD_1850))  # keep positive for 1850-leaning
# Rank words by the magnitude of their contribution to divergence (i.e., absolute Partial JSD)
# and keep the top 30 most distinctive words across the two decades
word_dist <- word_dist %>%
arrange(desc(abs(Partial_JSD))) %>%  # Sort by highest absolute contribution
slice(1:30)                          # Keep top 30 words
# create a mirrored bar chart showing which words contribute most to linguistic shift
# between 1840 and 1850
# bars are flipped to show direction, based on partial jsd
ggplot(word_dist, aes(x = reorder(word, Partial_JSD), # order words by divergence
y = Partial_JSD, # y-axis is signed partial jsd
fill = Direction)) + # fill bars by direction label
geom_bar(stat = "identity") + # draw bars with exact values
coord_flip() + # flip axes for horizontal layout
labs(title = "Partial JSD of Top Words: 1840 vs. 1850", # chart title
x = "word", # x-axis label
y = "partial jsd value", # y-axis label
fill = "change direction") + # legend title
theme_minimal() + # use a clean minimal theme
scale_fill_manual(values = c("1840 → 1850" = "red", # set color for 1840 shift
"1850 → 1840" = "blue")) + # set color for 1850 shift
theme(legend.position = "bottom") # move legend below plot
library(ggridges)
ggplot(word_dist, aes(x = Partial_JSD, y = Direction, fill = Direction)) +
geom_density_ridges(alpha = 0.7) +
scale_fill_manual(values = c("1840 → 1850" = "blue", "1850 → 1840" = "red")) +
labs(title = "Density of Partial JSD by Direction",
x = "Partial JSD",
y = "Direction of Change") +
theme_minimal()
library(wordcloud)
wordcloud(
words = word_dist$word,
freq = abs(word_dist$Partial_JSD),
colors = ifelse(word_dist$Direction == "1840 → 1850", "blue", "red"),
scale = c(4, 0.5),
random.order = FALSE)
library(text2vec)
knitr::opts_chunk$set(echo = TRUE)
list.files()
list.files(path = ".", full.names = TRUE)
file_path <- file.path("data", "raw", "speeches.csv")
file_path
library(here)
relative_file_path<- here("data", "raw", "speeches.csv")
relative_file_path
print(paste("Your current working directory is:", current_dir))
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("/home/stephbuongiorno/Desktop/Text Mining for Historical Analysis/appendix_b/file_paths_are_trees.jpg")
getwd()
file_path <- file.path("data", "raw", "speeches.csv")
file_path
library(here)
relative_file_path<- here("data", "raw", "speeches.csv")
relative_file_path
library(tidyverse)
library(tidytext)
library(fs)
# function to process and plot top words for a given decade
plot_decade <- function(decade) {
dataset_name <- paste0("hansard_", decade)
data(list = dataset_name, package = "hansardr", envir = environment())
hansard_data <- get(dataset_name)
p <- hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_head(n = 20) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(
title = paste("Top Words in Hansard Debates,", decade, "s"),
x = "Word",
y = "Count") +
theme_minimal()
outfile <- file.path(current_dir, paste0("hansard_", decade, "_topwords.png"))
ggsave(outfile, plot = p, width = 6, height = 4, dpi = 300)
message("Saved: ", outfile) }
decades <- c("1800", "1810", "1820")
current_dir <- getwd()
print(paste("Visualizations will be saved to:", current_dir))
for (decade in decades) {
print(paste0("Processing ", decade))
plot_decade(decade) }
list.files()
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("/home/stephbuongiorno/Desktop/Text Mining for Historical Analysis/appendix_b/file_paths_are_trees.jpg")
getwd()
file_path <- file.path("data", "raw", "speeches.csv")
file_path
library(here)
relative_file_path<- here("data", "raw", "speeches.csv")
relative_file_path
library(tidyverse)
library(tidytext)
library(fs)
# function to process and plot top words for a given decade
# it takes the decade of interest and the name of the directory
# where the visualization will be saved
plot_decade <- function(decade, output_dir) {
dataset_name <- paste0("hansard_", decade)
data(list = dataset_name, package = "hansardr", envir = environment())
hansard_data <- get(dataset_name)
p <- hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_head(n = 20) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(
title = paste("Top Words in Hansard Debates,", decade, "s"),
x = "Word",
y = "Count") +
theme_minimal()
outfile <- file.path(output_dir, paste0("hansard_", decade, "_topwords.png"))
ggsave(outfile, plot = p, width = 6, height = 4, dpi = 300)
message("Saved: ", outfile) }
# define the decades of interest
decades <- c("1800", "1810", "1820")
# find your current working directory
# your visualizations will be saved here
output_dir <- getwd()
print(paste("Visualizations will be saved to:", current_dir))
for (decade in decades) {
print(paste0("Processing ", decade))
plot_decade(decade, output_dir) }
list.files()
getwd()
# Define decades of interest
decades <- c("1800", "1810", "1820")
# Loop through each decade
for (decade in decades) {
# Create a new directory for the current decade
new_dir <- paste0("vis_", decade)
# Create the directory if it doesn't exist
dir_create(new_dir)
print(paste("Created folder:", new_dir))
# Print progress message
print(paste("Processing", decade))
# Set output directory for plot_decade() to save inside this folder
outdir <- new_dir
# Use your plotting function
plot_decade(decade) }
# Define decades of interest
decades <- c("1800", "1810", "1820")
# Loop through each decade
for (decade in decades) {
# Create a new directory for the current decade
new_dir <- paste0("vis_", decade)
# Create the directory if it doesn't exist
dir_create(new_dir)
print(paste("Created folder:", new_dir))
# Print progress message
print(paste("Processing", decade))
# Set output directory for plot_decade() to save inside this folder
# Use your plotting function
plot_decade(decade, new_dir) }
list.files()
