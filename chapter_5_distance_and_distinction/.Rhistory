knitr::opts_chunk$set(echo = TRUE)
# URLs for plain text versions
looking_glass_url <- "https://www.gutenberg.org/files/12/12-0.txt"
peter_url <- "https://www.gutenberg.org/cache/epub/14838/pg14838.txt"
# Extract "Through the Looking-Glass" and save it to RStudio's global environment
looking_glass_text <- readLines(looking_glass_url, encoding = "UTF-8", warn = FALSE)
# Extract "The Tale of Peter Rabbit" and save it to RStudio's global environment
peter_text <- readLines(peter_url, encoding = "UTF-8", warn = FALSE)
# Load required libraries
library("tidyverse")
library("tidytext")
# Convert to data frames
looking_glass_df <- tibble(writer = "Lewis Carroll", text = looking_glass_text)
peter_df <- tibble(writer = "Beatrix Potter", text = peter_text)
# Combine both texts
books <- bind_rows(looking_glass_df, peter_df)
# Tokenize text by spliting it into individual words
tokens <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$"))  # keep only alphabetic words
# Calculate word frequencies per author
word_counts <- tokens %>%
count(writer, word) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0)
# Focus on selected words of interest
target_words <- c("rabbit", "she", "little", "chortled", "shed", "the", "hoe", "slithy", "galumph")
selected <- word_counts %>%
select(writer, any_of(target_words))
# View the table
print(selected)
# Count word frequencies per author (long format)
word_counts2 <- tokens %>%
count(writer, word, sort = TRUE) %>%
rename(document = writer, term = word)
# Compute tf-idf values
tfidf_table <- word_counts2 %>%
bind_tf_idf(term = term, document = document, n = n)
# View top distinctive words for each author
filtered_tfidf <- tfidf_table %>%
filter(term %in% target_words) %>%
arrange(document, desc(tf_idf)) %>%
select(term, tf_idf)
library(knitr)
make_tab <- function(df) {
df %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE, caption = NULL,
col.names = c("Word", "TF-IDF")) %>%
as.character() }
left_tab  <- make_tab(filtered_tfidf[1:6, ])
right_tab <- make_tab(filtered_tfidf[7:12, ])
two_pane_table <- paste0(
"\\noindent
\\begin{minipage}[t]{0.48\\textwidth}
\\raggedright\\textbf{Some Beatrix Potter Words}\\\\[3pt]",
left_tab, "
\\end{minipage}\\hfill
\\begin{minipage}[t]{0.48\\textwidth}
\\raggedright\\textbf{Some Lewis Carroll Words}\\\\[3pt]",
right_tab, "
\\end{minipage}")
asis_output(two_pane_table)
library("kableExtra")
distinctively_carroll <- tfidf_table %>%
filter(document == "Lewis Carroll") %>%
arrange(desc(tf_idf)) %>%
select(term, tf_idf)
kable(head(distinctively_carroll, 15),
format = "latex",
booktabs = TRUE,
caption = "\\textbf{\\large Lewis Carroll's Most Distinctive Words (when \\textit{Alice} is Compared with \\textit{Peter Rabbit})}",
col.names = c("Word", "TF-IDF"),   # <-- THIS IS THE FIX
escape = FALSE) %>%
kable_styling(full_width = TRUE)
library("text2vec")             # Load the package
# define a function for jsd
jsd <- function(p, q) {
# Ensure p and q are probability distributions
p <- p / sum(p)
q <- q / sum(q)
m <- 0.5 * (p + q)
# Define a helper function for KL divergence
kl_div <- function(a, b) {
a <- ifelse(a == 0, 1, a)  # Avoid log(0)
b <- ifelse(b == 0, 1, b)
sum(a * log2(a / b)) }
0.5 * kl_div(p, m) + 0.5 * kl_div(q, m) }
# load Alice in Wonderland
alice_url <- "https://www.gutenberg.org/files/11/11-0.txt"
alice_text <- readLines(alice_url, encoding = "UTF-8", warn = FALSE)
# Combine into one data frame
books <- bind_rows(
tibble(booktitle = "Alice", text = alice_text),
tibble(booktitle = "Peter", text = peter_text),
tibble(booktitle = "LookingGlass", text = looking_glass_text))
# Tokenise and count
word_freqs <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$")) %>%
count(booktitle, word) %>%
group_by(booktitle) %>%
mutate(freq = n / sum(n)) %>%
ungroup()
# Create a frequency matrix (book x word)
freq_matrix <- word_freqs %>%
select(booktitle, word, freq) %>%
pivot_wider(names_from = word, values_from = freq, values_fill = 0) %>%
column_to_rownames("booktitle") %>%
as.matrix()
# Compute JSD for each pair of books
book_names <- rownames(freq_matrix)
n_books <- nrow(freq_matrix)
jsd_matrix <- matrix(0, n_books, n_books)
rownames(jsd_matrix) <- book_names
colnames(jsd_matrix) <- book_names
for (i in 1:n_books) {
for (j in 1:n_books) {
jsd_matrix[i, j] <- jsd(freq_matrix[i, ], freq_matrix[j, ]) } }
# View results
kable(jsd_matrix)
# Load our needed R packages
library("hansardr")
library("tidyverse")
library("lubridate")
library("tidytext")
library("gt")
# Load tidytext's built-in list of stop words for filtering out common words
data("stop_words")
# Load the Hansard debates from the 1830s and 1860s
data("hansard_1830")
data("hansard_1860")
# Add a 'decade' column to hansard_1830 to tag all rows with the value 1830
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
# Add a 'decade' column to hansard_1860 to tag all rows with the value 1860
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
# Combine the two Hansard datasets by stacking their rows into one data frame
# 1830 will be stacked above 1860
hansard_data <- bind_rows(hansard_1830, hansard_1860)
# Inspect the data
hansard_data[, 2:3] %>%
sample_n(5) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Sample from Hansard Data") %>%
kable_styling(latex_options = "hold_position")
# Code does not show, just output
tokenized_hansard_data <- suppressMessages(
hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[[:digit:]]")))
# tokenize the 'text' column into individual words,
# remove stop words, and filter out tokens that contain digits
tokenized_hansard_data <- hansard_data %>%
unnest_tokens(word, text) %>% # break text into one word per row
anti_join(stop_words) %>% # remove stop words like 'the' or 'and'
filter(!str_detect(word, "[:digit:]")) # remove words with digits
# count how often each word appears per decade,
# sort by frequency, and remove words that appear only once
words_per_decade <- tokenized_hansard_data %>%
count(decade, word, sort = TRUE, name = "word_count_per_decade") %>%  # count word frequency by decade
filter(word_count_per_decade > 1)  # keep words that appear more than once
# Inspect the data
words_per_decade %>%
rename(hansard_word = word, hansard_decade = decade) %>%
sample_n(8) %>%
arrange(desc(word_count_per_decade)) %>%
pivot_wider(names_from = hansard_word, values_from = word_count_per_decade,
values_fill = 0)
# calculate tf-idf (term frequency–inverse document frequency) for each word by decade
# using word frequency and total count
hansard_tf_idf <- words_per_decade %>%
bind_tf_idf(word, decade, word_count_per_decade)  # compute tf-idf \
hansard_tf_idf %>%
sample_n(5) %>%
arrange(desc(tf_idf))
# keep top 15 tf-idf words per decade
top_hansard_tf_idf <- hansard_tf_idf %>%
group_by(decade) %>% # group by decade
slice_max(tf_idf, n = 15) %>% # keep top 15 words
arrange(decade, desc(tf_idf)) %>%
ungroup()
library("dplyr")
library("knitr")
library("kableExtra")
make_kable <- function(df, dec_label) {
df %>%
select(word, tf_idf) %>%
arrange(desc(tf_idf)) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kbl(format = "latex",
booktabs  = TRUE,
col.names = c("Word", "TF--IDF Score"),
align = c("l", "r"),
caption = paste0("Top TF--IDF Words — ", dec_label),
escape = TRUE) %>%
kable_styling(full_width = FALSE) }
target_decades <- if (is.numeric(top_hansard_tf_idf$decade)) c(1830, 1860) else c("1830s", "1860s")
top_hansard_tf_idf %>%
filter(decade %in% target_decades) %>%
group_by(decade) %>%
group_walk(~ print(make_kable(.x, unique(.x$decade))))
# Load Hansard data for the 1840s and 1850s
data("hansard_1840")
data("hansard_1850")
# Add a "decade" column to the 1840s dataset
hansard_1840 <- hansard_1840 %>%
mutate(decade = 1840)
# Add a "decade" column to the 1850s dataset
hansard_1850 <- hansard_1850 %>%
mutate(decade = 1850)
# Append the 1840s and 1850s data to the existing hansard_data
hansard_data_1830_to_1870 <- hansard_data %>%
bind_rows(hansard_1840) %>% # Add 1840s data
bind_rows(hansard_1850) # Add 1850s data
# define the list of decades to analyze. each value will be used to
# filter the main dataset, enabling decade-by-decade processing.
decades <- c(1830, 1840, 1850, 1860)
# create an empty data frame to hold word frequency counts from each decade.
# this object will be built up by appending results from within the loop.
hansard_words_1830_to_1870 <- data.frame()
# iterate through each specified decade to perform tokenization,
# filtering, and word frequency counting. results are combined into a single table.
for(d in decades) {
# filter the full hansard dataset for just the current decade (d)
new_decade <- hansard_data_1830_to_1870 %>%
filter(decade == d)  # keep only rows from this decade
# tokenize the text column into individual words, remove stop words,
# and exclude any tokens that contain numeric digits (e.g., years)
tokenized_new_decade <- new_decade %>%
unnest_tokens(word, text) %>%  # convert text to one word per row
anti_join(stop_words) %>%      # remove stop words
filter(!str_detect(word, "[:digit:]"))  # remove words with digits
# count the frequency of each remaining word in the current decade
# then filter to retain only words appearing more than 20 times
new_decade_count <- tokenized_new_decade %>%
count(decade, word, sort = TRUE) %>%  # count word frequencies
filter(n > 20)  # keep words with frequency > 20
# append the word counts from this decade to the overall results table
hansard_words_1830_to_1870 <- bind_rows(hansard_words_1830_to_1870, new_decade_count)}  # combine rows
# Inspect the Data
hansard_words_1830_to_1870 %>% group_by(decade) %>%
sample_n(2) %>%
ungroup() %>%
arrange(desc(decade)) %>%
head()
# compute tf-idf values and keep top 15 words by tf-idf for each decade
most_dist_hansard_words <- hansard_words_1830_to_1870 %>% # start with merged word data
bind_tf_idf(word, decade, n) %>% # compute tf-idf for each word by decade
group_by(decade) %>% # group by decade
slice_max(tf_idf, n = 15) # keep top 15 tf-idf words per group
library("dplyr")
library("knitr")
library("kableExtra")
make_kable <- function(df, dec_label) {
df %>%
select(word, tf_idf) %>%
arrange(desc(tf_idf)) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kbl(
format    = "latex",
booktabs  = TRUE,
col.names = c("Word", "TF--IDF Score"),
align     = c("l", "r"),
caption   = paste0("Top TF--IDF Words — ", dec_label),
escape    = TRUE
) %>%
kable_styling(full_width = FALSE)
}
# Optional: pick specific decades; otherwise render all present
# target_decades <- c("1830s","1860s")  # or c(1830,1860) if numeric
# df_in <- most_dist_hansard_words %>% filter(decade %in% target_decades)
df_in <- most_dist_hansard_words
# If you want a specific order, set a factor:
# df_in <- df_in %>%
#   mutate(decade = factor(decade, levels = sort(unique(decade))))
# Print one table per decade (no list() artifact)
df_in %>%
group_by(decade) %>%
group_walk(~ print(make_kable(.x, unique(.x$decade))))
# Remove both datasets from the global environment
rm(hansard_1830, hansard_1840)
# List all objects currently in the global environment
ls()
# List all objects currently in the global environment
all_variables <- ls()
# Print the names of all objects (optional, for confirmation)
print(all_variables)
# Remove all objects from the global environment
rm(list = all_variables)
# Run garbage collection to reclaim memory
gc()
library("tidyverse")
library("tidytext")
library("hansardr")
library("tidyverse")
library("hansardr")
library("tidytext")
# Load Hansard data for the 1840s and 1850s
data("hansard_1840")
data("hansard_1850")
# preprocess the 1840s hansard data:
# tokenize the text column into words, remove stop words,
# and filter out tokens that contain digits
hansard_1840 <- hansard_1840 %>% # create a new dataset
unnest_tokens(word, text) %>% # convert text into one word per row
anti_join(stop_words) %>% # remove common stop words
filter(!str_detect(word, "[:digit:]"))  # remove words with digits
# preprocess the 1850s hansard data using the same steps:
# tokenize, remove stop words, and exclude numeric tokens
hansard_1850 <- hansard_1850 %>% # create a new dataset
unnest_tokens(word, text) %>% # tokenize text
anti_join(stop_words) %>% # remove stop words
filter(!str_detect(word, "[:digit:]"))  # exclude tokens with digits
# Function to calculate one-sided KL divergence component of JSD
partial_jsd_1840_to_1850 <- function(p, q) {
m <- 0.5 * (p + q) # Midpoint distribution
return(p * log2(p / m))} # 1830 deviation from midpoint
partial_jsd_1850_to_1840 <- function(p, q) {
m <- 0.5 * (p + q) # Midpoint distribution
return(q * log2(q / m))} # 1860 deviation from midpoint
# define a function for jsd
jsd <- function(p, q) {
# Ensure p and q are probability distributions
p <- p / sum(p)
q <- q / sum(q)
m <- 0.5 * (p + q)
# Define a helper function for KL divergence
kl_div <- function(a, b) {
a <- ifelse(a == 0, 1, a)  # Avoid log(0)
b <- ifelse(b == 0, 1, b)
sum(a * log2(a / b))
}
0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)
}
# Count word frequencies for both decades
freq_1840 <- hansard_1840 %>%
count(word, name = "count_1840")
freq_1850 <- hansard_1850 %>%
count(word, name = "count_1850")
freq_1840
# define how many top words to extract from each decade's corpus.
# this number controls the vocabulary size for comparison.
top_n <- 50  # number of top words to extract
# extract the top n most frequent words from the 1840s word frequency table.
# keep only the word column and return it as a character vector.
top_words_1840 <- freq_1840 %>%
slice_max(order_by = count_1840, n = top_n) %>%  # select top n by count
select(word) %>%  # keep only the word column
deframe()  # convert to a character vector
# extract the top n most frequent words from the 1850s word frequency table.
# repeat the same steps to prepare for cross-decade comparison.
top_words_1850 <- freq_1850 %>%
slice_max(order_by = count_1850, n = top_n) %>%  # select top n by count
select(word) %>%  # keep only the word column
deframe()  # convert to a character vector
# Combine top words from both decades and keep only unique words
all_top_words <- unique(c(top_words_1840, top_words_1850))
# merge the top words from both decades with their frequency tables,
# ensuring that missing words are filled with zero counts to enable comparison
word_dist <- data.frame(word = all_top_words) %>%  # create dataset with union of top words
left_join(freq_1840, by = "word") %>%  # join 1840s word counts
left_join(freq_1850, by = "word") %>%  # join 1850s word counts
replace_na(list(count_1840 = 0, count_1850 = 0))  # replace missing counts with 0
# normalize raw word counts to probabilities by dividing each word's count
# by the total count from its respective decade. this prepares data
# for comparison using divergence or distance metrics.
word_dist <- word_dist %>%
mutate(p_1840 = count_1840 / sum(count_1840),  # normalize 1840 counts
p_1850 = count_1850 / sum(count_1850))  # normalize 1850 counts
# The logarithm of zero (log(0)) is undefined in mathematics—it approaches negative infinity.
# In practice, trying to compute log(0) in R will result in -Inf or NaN, which can break downstream calculations.
# To avoid this, we replace zero probabilities with a very small constant (1e-10),
# which approximates zero but keeps the log function well-defined.
word_dist$p_1840[word_dist$p_1840 == 0] <- 1e-10
word_dist$p_1850[word_dist$p_1850 == 0] <- 1e-10
head(word_dist)
# Compute Partial Jensen-Shannon Divergence (JSD) for each word in both directions:
# - From the 1840 distribution to the 1850 distribution
# - And from 1850 to 1840
# This captures how much each word contributes to the overall divergence between decades,
# depending on the direction of comparison.
word_dist <- word_dist %>%
rowwise() %>%
mutate(Partial_JSD_1840 = partial_jsd_1840_to_1850(p_1840, p_1850), # Contribution from 1840
Partial_JSD_1850 = partial_jsd_1850_to_1840(p_1840, p_1850)) %>% # Contribution from 1850
ungroup()
# For interpretation, assign each word to a single dominant direction:
# - If its contribution is greater when comparing 1840 to 1850, it is labeled "1840 to 1850"
# - Otherwise, it's labeled "1850 to 1840"
# Also, assign a signed Partial JSD value:
# - Negative if the word is more distinctive of 1840 (helps indicate direction visually)
# - Positive if more distinctive of 1850
word_dist <- word_dist %>%
mutate(Direction = ifelse(Partial_JSD_1840 > Partial_JSD_1850,  # decide dominant direction
"1840 to 1850",
"1850 to 1840"),
Partial_JSD = ifelse(Direction == "1840 to 1850",
-Partial_JSD_1840,  # negate to show shift from 1840
Partial_JSD_1850))  # keep positive for 1850-leaning
# Rank words by the magnitude of their contribution to divergence (i.e., absolute Partial JSD)
# and keep the top 30 most distinctive words across the two decades
word_dist <- word_dist %>%
arrange(desc(abs(Partial_JSD))) %>%  # Sort by highest absolute contribution
slice(1:30)                          # Keep top 30 words
# create a mirrored bar chart showing which words contribute most to linguistic shift
# between 1840 and 1850
# bars are flipped to show direction, based on partial jsd
ggplot(word_dist, aes(x = reorder(word, Partial_JSD), # order words by divergence
y = Partial_JSD, # y-axis is signed partial jsd
fill = Direction)) + # fill bars by direction label
geom_bar(stat = "identity") + # draw bars with exact values
coord_flip() + # flip axes for horizontal layout
labs(title = "Partial JSD of Top Words: 1840 vs. 1850", # chart title
x = "word", # x-axis label
y = "partial jsd value", # y-axis label
fill = "change direction") + # legend title
theme_minimal() + # use a clean minimal theme
scale_fill_manual(values = c("1840 to 1850" = "red", # set color for 1840 shift
"1850 to 1840" = "blue")) + # set color for 1850 shift
theme(legend.position = "bottom") # move legend below plot
library("ggridges")
ggplot(word_dist, aes(x = Partial_JSD, y = Direction, fill = Direction)) +
geom_density_ridges(alpha = 0.7) +
scale_fill_manual(values = c("1840 to 1850" = "blue", "1850 to 1840" = "red")) +
labs(title = "Density of Partial JSD by Direction",
x = "Partial JSD",
y = "Direction of Change") +
theme_minimal()
library("wordcloud")
wordcloud(
words = word_dist$word,
freq = abs(word_dist$Partial_JSD),
colors = ifelse(word_dist$Direction == "1840 to 1850", "blue", "red"),
scale = c(4, 0.5),
random.order = FALSE)
ggplot(word_dist, aes(x = Partial_JSD,
y = reorder(word, Partial_JSD),
color = Direction)) +
geom_point(size = 3) +
geom_text(aes(label = word),
hjust = ifelse(word_dist$Partial_JSD > 0, -0.2, 1.2)) +
scale_color_manual(values = c("1840 to 1850" = "blue", "1850 to 1840" = "red")) +
labs(title = "Partial JSD of Words (Top 30)",
x = "Partial JSD",
y = "Word") +
theme_minimal()
