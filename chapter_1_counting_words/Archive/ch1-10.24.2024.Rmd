---
title: 'Chapter 1: Counting Words in the British Parliamentary Reports'
output:
  pdf_document:
    latex_engine: lualatex
geometry: "left=1in,right=1in,top=2in,bottom=1in"

---

```{r setup, include=FALSE}
# Chunk options
knitr::opts_chunk$set(
 fig.width = 6,
 fig.asp = 0.8,
 out.width = "80%"
)
```

_This is a peer-reviewed book chapter from Text Mining for Historical Analysis, under contract with Cambridge University Press. This is not it's final form. It is still undergoing formatting and edits for publication in a print and digital book._ 

The introduction to text mining in this book begins with tracking words' context.  As the linguist John Rupert Firth said, "You shall know a word by the company it keeps."  Tracking subtle variations in the context with which different words are used helps historians and other analysts to understand the cultural biases that structure our collective thinking.  They may help us to understand how a culture understands men and women differently, how it talks about people of different nationalities, the difference between a concept like "the foreign" and its near synonyms like "outsider" that nevertheless are used in important and different ways, or how these concepts are changing over time.  In the course of this book, we will offer a taste of the subtle and refined approaches that scholars of texts and data scientists have used to unpack the context of different words.  

In this chapter we introduce three basic techniques for processing text so as to foreground change over time through investigating the frequencies of a keyword. In this chapter, we will: a) access data for the British parliamentary debates of the House of Lords and House of Commons, 1850-59 using the **hansardr** library, b) filter a relevant decade for keywords of interest, and c) count the words that appear in the same context as our keywords in order to study the keywords' meaning in more detail.  

Importantly, the techniques presented in this chapter have both promise and limits.  As our companion volume, _The Dangerous Art of Text Mining_, makes clear, simple word counts are almost never sufficient on their own to support an in-depth analysis of culture.  Nevertheless, knowing how to count words and compare the context with which different concepts are used is a first step to using text as data.  

## Loading the Hansard Data

The first step in working with text as data is to load the data we wish to analyze. Analysts, programmers, researchers, and the like routinely import software packages developed by others to expand the range of tools available to them.  

The authors developed a data package to go with this book, **hansardr**, to give readers tools to access the Hansard 19th-Century British Parliamentary Debates with Improved Speaker Names within the R environment. This is a cleaned, analysis-ready corpus of the 19th-century British Parliamentary Debates (1803-1899), also known as "Hansard" in reference to Thomas Curzon Hansard, the publisher.  It identifies debates whose records are missing from UK Parliamentâ€™s corpus, and it also offers a field for disambiguated speakers. We believe these improvements will enable researchers to analyze the Hansard debates, including speaker discourse, in a way that has not been accessible before. Once installed, the data can be loaded with the `data()` function.

In R, a function is a block of code that performs a specific task. It can take inputs, called "arguments", that serve as instructions for how the function should behave, process data, or return an output. Functions are like the verbs of the R language in the sense that they say what should be done to the data. 

To use a function from another package, one must first install the package with `install.packages()`.  After a software package has been installed the first time, we can load the software package into computer memory using `library().`  It is conventional to load the software packages that you will use at the beginning of each new session. When writing code, loading just the tools you need for a particular task ensures things run smoothly by preventing different functions from interfering with each other. For the sake of instructions, we will regularly load packages throughout a chapter so the reader can more easily see which functions tie back to different packages. 

**hansardr** can be installed from the Comprehensive R Archive Network (or "CRAN" for short). Use `install.packages()` to install a package from CRAN.

```{r, eval=FALSE}
install.packages("hansardr")
```

**hansardr** can also be installed direclty from our GitHub repository:

```{r, eval=FALSE}
require("devtools")
install_github("stephbuon/hansardr")
```

As we explained in the introduction, packages only have to be installed once, but they need to be reloaded for each new R session using `library()`.

```{r, message=FALSE}
library(hansardr)
```

**hansardr** is now ready for use.

### Navigating the **hansardr** data

To make working with the Hansard corpus quicker and easier--and to prevent our personal computers from being overloaded with too much data--we broke the century long corpus into decade sections. In the field of data analysis, it's common to call these sections "subsets."  The first kind of dataset we will meet is a collection of the speeches of parliament, 1850-59, broken into sentences. Later on, we will meet more datasets that contain "metadata," or information about the speaker, year and date of the speech, and other important contextual information for historical analysis.  

In this chapter, we will start working with textual datasets (as opposed to working with dates) because they are easy to process for counting words. Both text and metadata are divided into subsets named by decade, for instance, *hansard_1850*. *hansard_1850* contains just the speech text as well as a unique sentence ID that can be used to connect the speech text with its corresponding metadata. 

We can use `data()` to load one of these decade-based subsets of the Hansard corpus. 

```{r}
data("hansard_1850")
```

A new variable named `hansard_1850` is now able to be seen in our Global Environment panel.  If you are using RStudio, you can explore the data by clicking on its name in the Global Environment pane (the top right quadrant of RStudio).  Clicking on the name of the dataset will open a new tab containing a view of the data, which you can search by keyword as you would in a spreadsheet. You can also double-click on any row in the data frame to select a row, then copy that data and paste it into a Word Document or other tool. 

The Hansard subsets are purposefully small enough to open and explore on a personal computer. When working with larger datasets, it may not be possible to open the entire dataset in another tab. Instead, in the Global Environment Panel you can also click on the blue ">" symbol to the left of each data frame to display the "fields" by which the dataset is structured, in this case, the names of the columns in the dataset, as well as a preview of the content of each of these columns.   

In addition to exploring the data in RStudio, there are a few functions that provide the means to look at sections of the dataset in greater detail. Throughout this book, we will routinely "inspect" the data by using functions like `head()` to see how the data is transformed at different steps. Getting into the habit of using functions like `head()` makes it easier to notice and interrogate problems as they arise, or ensure that the data transformations meet your expectations. At the console, you can also use `head()` to ask R to display the first six rows of the data. In some versions of RStudio you will see an arrow that allows you to move between two columns.  

As stated, the first column in this dataset is `sentence_id`, which gives a unique identifier for each sentence, including information about which series of parliamentary proceedings we're looking at, which volume and page, and which sentence number on each page.  The second column is `text`. Each row is a distinct sentence of text. These speeches were broken into sentences during the dataset creation stage, where the authors wrote a script to detect punctuation and line breaks.  

```{r}
head(hansard_1850)
```

## Tidy Text Mining and Loading Libraries

Most of the functions used in the first chapters of this book come from a software package called **tidyverse** which is a family of smaller packages developed by Chief Scientist at Posit (formerly known as RStudio), Hadley Wickham.  

The tidyverse was created to order data in "tidy" tables where it is structured as a data frame made up of columns and rows, and every observation has one row.  In other words, the tidyverse is built to work with data like the "text" column in `hansard_1850`.

We have found that a tidy approach to text exploration and analysis allows us to write code quickly and effectively, giving analysts ample control over the data, and allowing them to inspect the output of every form of analysis. This library thus enables analysts to focus on questions about history without being weighed down by the nuances of different structural and syntactic coding problems. 

In this chapter we will also use the **tidytext** library, developed by David Robinson and Julia Silge, to work with textual data.  The tidytext library contains a variety of tools useful for processing text and treating text as data.  Most lessons in this book will therefore begin by loading the two libraries tidyverse and tidytext.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
```

## Using Functions to Process Data

The primary purpose of functions is to create reusable code, allowing you to avoid repeating the same data processing steps.

From this point on we are going to use more complicated functions from different R packages to wrangle our data. Before we take this next step, however, we first want to equip you with a basic understanding of a function's underlying components in R by creating our own simple function: 

```{r}
# Function to add two numbers
add_numbers <- function(a, b) { 
  result <- a + b 
  return(result)
}
```

The above code can be broken down into these core parts: 

1. **Function Name**: The name given to the function. In this example, the function is named `add_numbers`.
2. **Parameters**: The variables listed in the function definition that will accept arguments, for instance, `function(a, b)`.
3. **Body**: The code that is executed when the function is called, written here as `result <- a + b`. 
4. **Return Value**: The output produced by the function, for example, `return(result)`. 

Under the hood, functions each contain this "hidden" body. 

We can now use our `add_numbers()` function by passing arguments to the specified parameters. In this example, the "a" parameter is passed the number 2, and the "b" parameter is passed the number 3. The two numbers are added in the body, and then the results are returned. You can now reuse this function any time I want to add two numbers without rewriting the body. 

```{r}
print(add_numbers(2, 3))
```
To count words in Hansard, we will use the `unnest_tokens()` function from tidytext. While there are many ways to count words over time, one way to do this--that is also very human readable--is to put our data into a format where each word of the text is on a separate row. These individual words are called "tokens." The process of splitting chunks of text into individual units is called "tokenization." A common term used to instead describe multi-word phrases is "n-gram" (such as using the term "bigrams" to describe two word units). 

In this one-token-per-row format we can easily "wrangle" (or further process) our data. This approach to data wrangling is common in the humanities and social sciences and handles text using the "Bags of Words" (BoW) approach where text data is treated as a collection (or "bag") of individual words without regard to grammar, order, or context. The focus is instead on word frequency or occurrence.

```{r}
tokenized_hansard <- hansard_1850 %>%
  unnest_tokens(word, text)
```

The above code also has new symbols, called "operators." Throughout the course of this book we will introduce multiple different kinds of operators. 

The assignment operator, `<-`, sometimes translated as "gets," points to the output of a function and is followed by the name of an original dataset.  The `%>%` operator, called a "pipe," and translated as "next," is used to chain together successive transformations of a dataset. This will make more sense when we cover more examples later in the book.

If this is your first time using a programming language for analysis, it can be useful to practice reading each line of code as a sentence of instructions that tells the computer what output to make, what input to use, and what processes to go through in a certain order. We sometimes suggest that first time programmers translate code into a series of English sentences, using their knowledge of the `<-` and `%>%` operators, the names of variables, and their understanding of distinct commands.  The code above be read as follows: "Create a new variable called `tokenized_hansard`.  Start with the dataset `hansard_1850`.  Next, unnest the tokens in the `text` column of `hansard_1850`, unpacking those strings of text into individual words."

You may also notice the arguments passed to `unnest_tokens()`, "word" and "text." Functions like `unnest_tokens()` take a set number of arguments, typically in a precise order. The first argument is the type of unnesting that will be employed. The second argument is the name of the column from the original `hansard_1850` data that we will work on, in this case, "text." 

As you inspect the arguments of each function, consider the fact that those arguments exist because they allow the analyst to change the arguments to get different results.  With the `unnest_tokens()` function, the user could hypothetically choose to "unnest" the sentences into tokens of different length, for instance, sentences, n-grams, or other units of text.   In later chapters we will explore some of these features of the command "unnest."   Alternatively, if the `unnest_tokens()` command were being applied to a different dataset where the column had a different name than "text," we could change the column name so that the command would work. In later examples, you will see the same commands applied to different datasets with different column names.  For now, we will mainly use `unnest_tokens()` with the arguments "word" and "text."

Next, let's inspect the results of our unnesting using `head()`, which returns just the first rows of a data frame.

```{r, echo=FALSE}
head(tokenized_hansard)
```

`unnest_tokens()` did a lot of heavy lifting for us! 

As part of our data transformation, the output data has a column named "word" for each individual token of the corpus. Notice that the words in `tokenized_hansard` look just like the words in the first sentence of the first speech in `hansard_1850`. Those speeches have been split into words, but the words are still in the same order. Notice that the command `unnest_tokens()` has also transformed all the words to lower-case and removed punctuation marks such as commas and periods.  This transformation will make the words easier to count.  

As we inspect the results of unnesting, you should notice that the input was "tidy" in format -- that is, it had two columns, one for "sentence_id" and one for "text," the sentences and sentence fragments of the original parliamentary speeches. After working with `unnest_tokens()`, the resulting output, `tokenized_hansard`, also is "tidy" in format.  That is, it has two columns, one for `sentence_id` and one for `word`.  In this tidy format, where each word is on its each row, the computer can easily count the words in the dataframe.  

## Counting Words

Now we are going to bring together what we have learned by chaining together multiple functions that will be used to process the Hansard data. 

The tidyverse `count()` function groups like tokens with like tokens, then counts them. Below, we are passing `count()` one argument, the name of the column to work on--in this case, "word."

```{r}
tokenized_hansard_counts <- tokenized_hansard %>%
  count(word) %>%
  arrange(desc(n))
```

With the command `arrange(desc(n))`, we tell the computer to arrange the results from greatest to least. We are doing this step so that the results are easier to read, not because this step is necessary for counting words. 

```{r, echo=FALSE}
head(tokenized_hansard_counts)
```

We now have a frequency count of every word recorded in the speeches of Hansard, 1850-59. The initial results are unimpressive.  In our dataset, as with most textual datasets, the most common words are also the least interesting.  For this reason, analysts compile lists of the most common words, called "stop words." Analysts often remove stop words from their results before analysis.  

The tidytext package comes with a prepared list of stop words.  We can remove them from our results by loading the "stop_words" data set and then passing the dataset as an argument to the `anti_join()` function. 

```{r, echo=FALSE}
data("stop_words")

stopworded_hansard_counts <- tokenized_hansard_counts %>%
  anti_join(stop_words)
```

```{r}
head(stopworded_hansard_counts)
```

These results seem sensible for parliament. Some of them demonstrate the way that members of parliament refer to each other, for example, as "my noble lord" or "the hon[ourable] member [in question]."  Speakers in parliament also routinely refer to the business of parliament, especially referencing the "house [of commons or lords]," the "bill" or "question" under debate, the state of the "country," and how much "time" is available for debate.  

General words, of course, give us little insight into the business of parliament.  Later sections will give us richer tools for examining how the content of speeches changed over time.  For now, we have successfully achieved the first step of analysis, the processing of text into words, and the counting of words. 

## Searching for Keywords

The Tidyverse provides several commands that we will use throughout this book to track individual words. The `filter()` function allows us to find an exact match in the data.  Suppose that we want to inspect the tenth sentence of the parliamentary speeches, series 3, volume 108, which has the id number "S3V0108P0_10."  Alternately, `filter()` can be used to subset the `stopworded_hansard_counts` data frame, returning only the rows where the word column contains the value "india". Here we pass `filter()` two arguments: first, the name of the column we will search for ("word"), and second the name to match ("india"). Here we are using the `==` operator to indicate we want to return an exact match, as opposed to words that might contain the word "india" such as "indian". 

```{r}
stopworded_hansard_counts %>% 
  filter(word == "india")
```

```{r}
stopworded_hansard_counts %>% 
  filter(word == "france")
```

```{r}
stopworded_hansard_counts %>% 
  filter(word == "jamaica")
```

Filtering for every word we wish to analyze, one-by-one, can be tedious and time consuming. Filter can also be used with other R operators, such as `%in%`, that save us time and energy by giving us the opportunity to cycle through multiple words of interest instead of just one. 

To do this, we can first create our own custom list of words we wish to analyze.

```{r}
identifiers_for_woman = c("girl", "woman", "girls", "women", "wife", "wives", "widow",
                      "widows", "sister", "sisters", "famale", "females", "grandmother", 
                      "grandmothers", "aunt", "aunts")
```

Now we can use the `%in%` operator to check if a word from our list, `identifiers_for_woman`, is in the "word" column. 

```{r}
stopworded_hansard_counts %>% 
  filter(word %in% identifiers_for_woman)
```

`filter()` is useful for more than just text data. It also works on quantitative data with comparison operators like the greater than, `>`, or less than, `<`, signs. This is handy if we want to filter for words that are said more or less than a specified number of times, as one example. For the sake of readability, we are only going to show the first 20 words that follow our filter. 

```{r}
stopworded_hansard_counts %>% 
  filter(n < 474) %>%
  slice_max(order_by = n, n = 20) 
```

We can also use multiple operators in conjunction with one-another. In the following code we do this by supplying the `&` operator to combine multiple conditions. The following example filters rows where n is greater than 474 and less than 476.

```{r}
stopworded_hansard_counts %>% 
  filter(n > 474 & n < 476) %>%
  slice_max(order_by = n, n = 20)
```

In just this quick glimpse of our data, meaningful words are beginning to appear. The word "Jamaica," for example, appears along with the word "depositors." These words gesture towards the political, economic, and social issues of the time, specifically the declining state of the economy. Members of Parliament discussed the slave revolt in Jamaica, including its scale and significance. Since the 17th century, Jamaica had its own parliament but was placed under martial law during the revolt. This demonstrates that Jamaica was significant enough to command parliamentary attention, elevating the revolt to a major historical event that could not be ignored by Parliament.

## Tailoring our Word Count Analysis 

Detecting and filtering for topics of interest can be particularly meaningful for using data to tell history. 

Our next function, `str_detect()`, treats data as patterns and detects their presence or absence. It returns `TRUE` if the pattern is detected, or `FALSE` if it is not. When a data type represents only two possible values, `TRUE` or `FALSE`, it is typically referred to as a "boolean" value.

In the following example, we iterate through each word in the `identifiers_for_woman` list to check whether the value contains "girl."

```{r}
identifiers_for_woman %>% 
  str_detect("girl")
```
The results from `str_detect()` aren't very telling for analysis. Instead, for a textual analysis we often use both functions, `filter()` and `str_detect()`, together so that we return a more human-readable result. 

The following example searches `hansard_1850` for the keyword "coal." Notice that `str_detect()` is nested inside the command `filter()`.  Together, these commands tell the computer to filter for only those rows of the column "text" that contain the keyword "coal."

```{r}
hansard_coal <- hansard_1850 %>%
  filter(str_detect(text, "\\bcoal\\b"))
```

Something else interesting is happening in our code. The word "coal" has a funny looking set of characters, `\\b`, on either side. Together with the word "coal," these special characters are make up part of a "regular expression" often abbreviated as "regex." Regular expressions are sequences of characters that define search patterns, typically used for string matching, processing, and extraction. 

One reason the `str_detect()` function is so special is because, unlike `filter()` by itself, the `str_detect()` function enables us to match with regular expressions, not just words. `\\b` indicates a word's boundaries. It matches the position between a word character (e.g., letters, numbers) and a non-word character (e.g., space, punctuation, or the start/end of a string). `\\b` at the end ensures that "coal" is a standalone word, meaning it will match "coal" but not "coalesce" or "charcoal."

The result of our search is variable, `hansard_coal`, that contains only sentences from the 1850 debates that contain the word "coal."

```{r}
head(hansard_coal)
```

## Finding and Comparing Keywords' Context  

Typically, we want to do more than simply search for a singular keyword. We may also want to search for collocates, or words that are "co-located" next to one-another. These words form the context fo a given keyword. 

We already have everything we need to make this happen: we can search for a keyword, we can break up text into words, and we can count the words.  Let's use our tools to search for the context of several keywords and compare them in visual form.  

```{r}
coal_context <- hansard_coal %>% 
  unnest_tokens(word, text) %>% # tokenize text
  anti_join(stop_words) %>% # remove stop words
  filter(!str_detect(word, "[:digit:]")) %>% # return tokens that do not contain digits, like dates 
  count(word) %>% # count the tokens
  arrange(desc(n)) # arrange the tokens descending order for easy reading 
```

Instead of running these functions independently, we have chosen to chain them together with the pipe, `%>%`, operator from tidyverse. Using the pipe allows us to use many functions sequentially without storing intermediary variables. 

Notice the line of code that reads `filter(!str_detect(word, "[:digit:]"))`. `[:digit:]` is a regular expression in R that encapsulates all digits, this way we do not have to specify them one-by-one. Here, we add `!` to filter out any entries that comprise digits. The negation operator, `!`, tells R to do the opposite of what the function usually does. Using it in the `filter()` function and preceding `str_detect()`, we ask to return text, not digits like the dates recorded in Hansard. 

```{r}
head(coal_context)
```

Above are the words that appear most frequently with the word "coal." When sharing our results, however, we may want to visualize our data instead of providing a table. Visualizations can make patterns, trends, and insights easier to understand, especially when dealing with large datasets or complex relationships. Visualizations can highlight key points in our data, and allowing for quicker interpretation compared to reading through rows of numbers in a table. 

Let's visualize just the top 20 words that co-occur with the word "coal." 

```{r}
top_coal <- coal_context  %>%
  top_n(20)
```

To visualize our data, we will use **ggplot**. **ggplot** is an R package used for creating high-quality, customizable visualizations. It follows a "grammar of graphics" approach, allowing users to build plots layer by layer, combining data, aesthetics (e.g., axes, colors), and geometric objects (e.g., points, lines) to create informative visualizations. We will use ggplot frequently throughout this book. 

```{r}
ggplot(data = top_coal) + # define the dataset we wish to visualize 
  geom_col(aes(x = reorder(word, n),  
               y = n),
           fill = "steel blue") + # set the color we wish our visualization to be
  coord_flip() +
  labs(title = "Figure 1.1: Top Words Occuring in Sentences Mentioning Coal", # add labels to describe our data
       subtitle = "From the 1850 Hansard debates",
       x = "Word",
       y = "Count") 
```

The resulting bar chart shows us the number of times "coal" was mentioned, as well as the top words that appear with coal. Note that because we are analyzing collocates, these counts do not reflect the total number of times they were mentioned between 1850 to 1859. Instead, the counts represent the total number of times these words appear in the same context as "coal." The second most frequent word is "steam" followed by "duties," "bill," and "mines." Words like "duty," "iron," "tax," and "supply" indicate key economic and industrial topics related to coal during the debates, with "iron" possibly tied to the role of coal in iron production. "Harbour," "ships," and "vessels" suggest discussions related to transportation and trade of coal.

Taken together, the visualization emphasizes the central role of coal in the British economy and legislation in the 1850s, as well as its links to other industrial and governmental matters. The visualization also reflects concerns around infrastructure, taxation, and policy decisions regarding coal. 

Our analysis will gain nuance the more we compare near concepts.  While coal was important to powering Britain's industrial revolution, perhaps the most controversial economic debates of the 1850s revolved around the issue of the Corn Laws, the system of taxation of wheat and other grains, which were originally introduced to protect British farmers.  Working-class demands for cheap bread led to the repeal of the Corn Laws in 1846.  Knowledge of these facts might lead us to ask: were coal and corn debated using the same language or different words?

```{r, message=FALSE}
corn_context <- hansard_1850 %>% 
  filter(str_detect(text, "\\bcorn\\b")) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, "[:digit:]")) %>%
  count(word) %>%
  arrange(desc(n))
```

```{r}
head(corn_context)
```

```{r}
top_corn <- corn_context  %>%
  top_n(20)
```

```{r}
ggplot(data = top_corn) +
  geom_col(aes(x = reorder(word, n), 
               y = n),
           fill = "steel blue") +
  coord_flip() +
  labs(title = "Top Words Occuring in Sentences Mentioning Corn",
       subtitle = "From the 1850 Hansard debates",
       x = "Word",
       y = "Count") 
```

Comparison between the words used in the context of coal and corn gives us a foothold for beginning to think about how taxation was discussed in different contexts.  For instance, the context of both keywords reference the role of time, duties, taxes, and the government, but debates over corn are more linked to the question of Britain's dependence "foreign" powers, whereas debates over coal are more crucially related to the condition of "England" itself. We do not pretend that noticing this difference constitutes a major historical insight; further reading and research would be necessary for the analyst to decide whether comparing these two commodities as keywords is worthwhile, and which collocates offer insight.

Nevertheless, at this point you have learned the preliminary process with which most of the analysis in this book will start: loading data, inspecting data, breaking data into words, and counting those words.  Later chapters will give us more tools -- importantly the tools of counting data over time and understanding historical change -- which can produce a richer form of analysis.  

## Finding a Word's Context using "Keywords in Context" (KWIC)

Another way to explore a word's context is by using a tool called "Keywords in Context" or "KWIC" for short. In this section we use `KWIC()` from the **quanteda** library and look at the sentence-level context for the word "corn" for February 1850. 

To look at just the month of February, we need to load our decade subset that contains information about dates, called `debate_metadata_1850`, and join it with the data set containing the debates, `hansard_1850`.

```{r}
data("debate_metadata_1850")
```

```{r}
head(debate_metadata_1850)
```

We will cover joins in greater detail in chapter 2. For now it's just important to know that we are using our unique ID, `sentence_id`, to join the two data frames together. 

```{r, message=FALSE}
hansard_1850_with_metadata <- left_join(hansard_1850, debate_metadata_1850)
```

```{r}
head(hansard_1850_with_metadata)
```

In the following code we filter for speech dates that are on or after (greater than or equal to `>=`) February 1, 1850, and on or before (less than or equal to `<=`) February 28, 1850.

```{r}
top_month <- hansard_1850_with_metadata %>% 
  filter(speechdate >= "1850-02-01",
         speechdate <= "1850-02-28") 
```

KWIC returns a concordance-style text output with the words that come directly before and after the keyword in a sentence. Quanteda's version of `KWIC()` uses a different approach from tidyverse. It processes data quicker if we first transform the data from a table into a "quanteda corpus" using `corpus()`. Using this approach, we can tell Quanteda that we wish to process the `text` column. 

When using `KWIC()` we can specify the keyword, the maximum number of words to display before and after the keyword, and we can also tell `KWIC()` to do a case insensitive match so that we match with uppercase or lowercase instances of our keyword.

```{r}
library(quanteda)

# transform the data into a quanteda corpus
my_corpus <- corpus(top_month, text_field = "text")

# look for the 5 words appearing before and after "corn"
corn_kwic <- kwic(tokens(my_corpus), 
                  "corn",
                  window = 5,
                  case_insensitive = TRUE)  
```

```{r}
head(corn_kwic)
```

## Critical Thinking With Collocates

When is collocate analysis the right tool?  Researchers of conceptual history frequently use collocates to understand the subtle differences between synonyms.  For instance, historian Ruben Ros (2021) has investigated the way that Dutch newspapers in the nineteenth century began to use terms such as "foreign," "overseas," and "strange" in the process of constructing a narrative that increasingly linked the dangers of foreign influence and suspicion of ethnic minorities.  

With collocate analysis alone, we cannot trace discourses over time -- a crucial step in understanding the construction of political and cultural positions.  But we can begin to tease apart the meanings of closely-related ideas at various points in the past.  Let's recreate Ros's case study in miniature here, asking the question: how did British members of parliament in the 1850s talk about foreigners and colonial subjects?

In the following code we create a function for filtering the hansard data for a keyword and visualizing the top 20 words. Here we are using our function to produce a series of related graphs in succession without writing out the same instructions multiple times.  

```{r, message=FALSE}
generate_word_plot <- function(word, hansard_data) {
    
    filtered_hansard_1850 <- hansard_data %>%
      filter(str_detect(text, paste0("\\b", word, "\\b"))) %>%
      unnest_tokens(word, text) %>%
      anti_join(stop_words) %>%
      filter(!str_detect(word, "[:digit:]")) %>%
      count(word) %>%
      top_n(20)
    
    plot <- ggplot(filtered_hansard_1850) +
      geom_col(aes(x = reorder(word, n), 
                   y = n),
               fill = "steel blue") +
      coord_flip() +
      labs(title = paste("Top Words Occurring in Sentences Mentioning", toupper(word)),
           subtitle = "From the 1850 Hansard debates",
           x = "Word",
           y = "Count") 
    
    print(plot) }

```


```{r, message=FALSE}
generate_word_plot("foreign", hansard_1850)
```

```{r, message=FALSE}
generate_word_plot("colonial", hansard_1850)
```

```{r, message=FALSE}
generate_word_plot("savage", hansard_1850)
```

```{r, message=FALSE}
generate_word_plot("native", hansard_1850)
```

```{r, message=FALSE}
generate_word_plot("barbarian", hansard_1850)
```

Producing many graphs in succession can be useful for the process of preparing material for careful comparison, which is one of the major skills that humanists and social scientists use when considering the subtle structures of meaning and difference that define cultural and political conversations.  A careful comparison between the charts above offers the beginnings of a nuanced analysis of overlapping terms used by British members of parliament to refer to people and forces from beyond Britain.  

In the results, we see several closely related terms which compose a field of interrelated meanings.  Together, these terms sketch out the diversity of contrasting ways that British members of parliament spoke about the world outside of Britain. Some of the attributes ascribed to the foreign were neutral in value -- for instance, the "foreign" was mainly spoken about in terms of laws, duties, trade, and protection, a world in which different nations "competed," but generally not a space of moral inferiority and superiority.  By contrast, terms such as "savage" and "barbarian" telegraphed intellectual and ethical judgments (for instance, "brutal") onto cultures outside of Britain, typically associating inferiority with racial identity.  With these terms, a set of binaries is set up by the speakers of Parliament, dividing the world into "barbarian" cultures and "christian" "civilisations," a distinction that may have been analyzed in terms of "rights," "treaties," "war," "rents," "passions," "liberty," and "energies.   The distinctions between the "foreign" and the "barbarian" are invoked in relationship to the same abstractions we saw typifying the "foreign," including  "law," the "country," and "treaties."  We also see the "barbarian" being invoked in discussions of many commodities, including "woolens," "forage," and "rent."  Indeed, the frequent invocation of law, rights, and foreignness alongside discussions of the barbarian and the savage suggests that the judgments ascribed to race actually bled into conversations about law, tariffs, trade, and protectionism, perhaps as justification for invoking British superiority. 

Some readers may assume that we are performing contemporary prejudices about racism, but the point of this exercise is that it is grounded in a detailed and careful examination of the words counted by computational methods. The paragraph above is not concocted out of thin air; it is actually an objective description of the prejudices on display in a quantitative reading of how British members of parliament spoke about the world beyond their nation.  We emphasize that this analysis of the related language of the "foreign" and the "barbarian" is not concocted on the basis of our readings of contemporary theories about racism, but is actually derived from a careful reading of language, as well as immersion in writing about the history of empire.    

In general, descriptions of word counts and arguments made on the basis of word counts tend to be more persuasive when they take on every word in a list.  Critical thinking about the meaning of proximate keywords is enhanced when the analyst slows down, taking the trouble to examine the distinctiveness of each individual keyword and its context.  In a further analysis, the scholar might ask questions that require examining the results in more detail.  They might ask: What biases does each term convey?  Where do the terms overlap?  What new information is encapsulated by some words but not others?  A thorough examination would trace the keyword-context pairs back to their original context in sentences and speeches on the page, showing how speakers used these words to formulate actual arguments with consequences for the development of nations and their economies.  

Examining collocates provides the basis for critical thinking about the changing and overlapping meaning of shared concepts in the human past.  Skillful analysts can use approaches of this kind to examine the meaning of language about categories beyond those of gender, race, nationality, and commodities, asking questions about the intellectual categories that governed how members of parliament understood governance itself, economics, truth, science, virtue, economics, and reason itself.  In every case, the approach would be the same: to examine not merely one or two keywords, but many related keywords, drawing attention to the subtle overlap and differences between the terms in usage, and how those overlapping concepts together work to produce a field of meaning.   

## Critical Thinking About Data and its Limits

Another fruitful avenue for critical thinking is the question of which problems are suited to textual analysis and which are not. Skillful analysts are wary of asking the wrong question with the wrong tool.  Among the techniques that we have learned in this chapter is the ability to load and inspect data.  Inspecting the data allows the analyst to notice issues caused by computational processing of text that may cause issues down the line unless the analyst is aware of them.

Looking at the first lines of `hansard_1850`, you might ask yourself these questions: are all of the lines in the "text" field actually sentences?  Are all the words in the same style with respect to capitalization? We can inspect this further. 

In R, the expression `hansard_1850[1:10, 2]` is selecting a specific element from `hansard_1850`. `hansard_1850` refers to a data frame or matrix, and `[1, 2]` refers to the row and column indices we wish to look at. The structure of the syntax is this: [row, column].  The syntax "1:10" in the "row" field tells r to display rows 1 to 10.  We put "2" in the "column" field to tell R to display

```{r, eval=FALSE}
hansard_1850[1:10, 2]
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60),echo=FALSE}
knitr::kable(setNames(hansard_1850[1:10, 2], NULL), format = "markdown")
```

Perhaps you have noticed that some of the rows in the "text" field appear not to be complete sentences.  We can now investigate further.

```{r, eval=FALSE}
hansard_1850[1, 2]
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60),echo=FALSE}
knitr::kable(setNames(hansard_1850[1, 2], NULL), format = "markdown")
```

Notice that your view of the data is cut off in this view.  To view the whole text, double-click on the cell, copy and paste the text into a word processor.  You should see this content:

", which had been prorogued successively from the 1st of August to the 9th of October, from thence to the 20th of November, thence to the 16th of January, and from thence to the 31st January, met this day for despatch [sic] of business."

The first row appears to be a description of the opening of parliament.  Technically, it is a commentary by the printer, not a line of debate. The convention of discussing parliament's timetables was in place in 1850, but we don't know if the printer always printed this line through the entire century.  We should be aware of this convention and curious about it if we find ourselves counting months or discussions of words such as "business."

Let's keep inspecting the data, this time turning to the third row in our dataset.

```{r, eval=FALSE}
hansard_1850[3, 2]
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60),echo=FALSE}
knitr::kable(setNames(hansard_1850[3, 2], NULL), format = "markdown")
```

Here, we see another sentence fragment:

"called attention to a great omission of their duty on the part of Ministers, with respect to the privileges of their Lordships, which might and ought to have been avoided."

The sentence fragment has resulted from attempting to process data while working on another convention of printing: the fact that the publisher of Hansard for much of the century gave the name of the speaker followed by a description of the airing of the speech.  On the page, we would be given the name of a speaker or their title, for example, "The Lord Chancellor" or "Mr. Gladstone."  The passage of text would therefore read, [the speaker] "called attention to a great omission..." -- in other words, it would not be a sentence fragment, so much as a journalistic description of a speaker.   The actual speech probably began with a statement something like this: "It is a great omission of their duty on the part of Ministers[...]."  

In other words, we are looking at an oddity that resulted from a mismatch between the nineteenth-century printed record and how the computer processed the data.  We compiled the data and separate all of the speaker names into one "field" of data, which we stored in a separate dataset, to be accessed in Chapter 2.  We have sorted all of the text into the "text" field, which is held in `hansard_1850`. The computer didn't process the journalistic description of a speech rather than a normal speech. 

Looking at the data presents an opportunity for critical thinking. Analysts of text as data routinely need to inspect their data and make sure that the data's quality and density are sufficient to support claims that they might make.

Already, we can see that the data has certain features that might interfere with certain kinds of queries, and we should already be thinking about what these limits might be. Yet the convention of journalistic reporting on speeches can cause trouble for the analyst if the analysts aren't aware of what has changed.  The conventions of describing speeches changed over the course of the century; at the beginning of the century, journalistic descriptions were more common. Later in the century, the conventions changed, and many more speeches were reported near verbatim without journalistic commentary.  We should be aware of conventions of this kind if we count words which may be related to journalistic observations such as "attention," "spoke," "declared," and so on. Otherwise we may be tempted to interpret changes to the publisher's conventions of printing speeches as evidence for changing ideas about politics.

Whether or not data quality will interfere with our ability to make inferences from the data depends on what we are asking.  If we are counting mentions of the word "duty" or the rest of the words in the content of the speech, then our counts of the words will be correct. If we contrive to study the history of journalistic observation by tracking sentence fragments in the dataset, this data will support such a reading.  But we should be wary making inferences about any kind of inquiry where there is a risk of overlap between substantive discussions and journalistic observation.  

## Exercises

Expand your grasp of the code and your powers of critical reflection by trying the following exercises:

1) Alter the code above to compare the context for the keywords "girl" and "woman."  Do you expect the context to be similar for both?  Does anything surprise you about the results?

2) Use your powers of inspecting data to move from the keyword context back to the individual sentences.  Cut and paste twenty sentences from the context of each keyword into a word processor.  What do you learn from these sentences that you did not learn from counting keywords?  Does your analysis become more perusasive when quantitative graphs are accompanied with readings of particular speeches where the words are examined in their original context?  

3) We have examined 'coal' and 'corn' in the 1850s.  Now, search for the words 'coal' and 'corn' in dataset `hansard_1820`.  Were they being used in the same way?  What changed?

4) Above, we created the list of words called `female_list`.  Use the operator `%in%` with the command `filter()` to create a list of the words in the context of discussions of women.  Create another list called `male_list` and do the same for discussions of men.  Create visualizations for the top terms used in discussions of men and women.

5) Reflect on your work so far -- in which you have compared words for gender, the concept of the foreign, and the context of discussions of different commodities.  You have made comparisons over time and comparisons of lists.  Which of the comparisons do you find the most productive of insight about how certain words were being used in political debate?  Are you persuaded by large numbers?  Are you persuaded by subtle variations in the context of different words?  Write a paragraph explaining for yourself wherein constitutes the most persuasive usage of comparisons between keyword in context.  

6) For the visualizations of the terms associated with different terms for the "foreign," tell us: What biases does each term convey?  Where do the terms overlap?  What new information is encapsulated by some words but not others?  What do we learn from this exercise about the prejudices of Britain's members of parliament in the 1850s?

## Practicing with ChatGPT

Working through the process of writing your own code step by step can help you better understand the underlying concepts behind working with text as data. Understanding the premise behind syntax, functions, and how they tie back to data processing enables us to actualize our analytic goals. With this knowledge, we can also effectively leverage AI technologies without being blind to mechanism. Here, we iterate on our analyses by practicing with ChatGPT. 

More than just writing code, ChatGPT can help with brainstorming and improving our analysis. In the above examples, we removed generic stop words (e.g. "the" or "and") from our data and this drastically improved our analysis. Let's now prompt ChatGPT to generate more stop words, tailored to 19th-century Hansard, that we can critically inspect before removing from our data.

> **ChatGPT Prompt:**
> 
> "Act like an R Programmer and generate a list of stopwords to be removed for an analysis of the 19th-century Hansard corpus."
>
> **ChatGPT's Response:**

```{r}
custom_stopwords <- c("lord", "lords", "hon", "mr", "sir", "gentleman", "gentlemen",
                      "speaker", "right", "honourable", "house", "parliament", 
                      "minister", "bill", "member", "question", "motion", "committee", 
                      "government", "debate", "order", "division", "clause")
```

Before removing the stop words, we can think critically about whether the results align with our own analytic goals. We may, for example, want to keep the word "government" if we wanted to analyze this word's collocates. 

We can use ChatGPT for more than just generating possible stop words. We can also use it to help us decide which topics we may want to analyze. 

> **ChatGPT Prompt:**
> 
> "I want to text mine and analyze heated topics from within the 1850 Hansard Parliamentary debates. Suggest several keywords that might be interesting." 
>
> **ChatGPT's Response:**
> 
> When analyzing the 1850 Hansard Parliamentary debates for heated topics, you'll want to focus on issues that were contentious or widely discussed at the time. Here are some keyword suggestions:
> 
> 1. **"Corn Laws"** â€“ Although the Corn Laws were repealed in 1846, the economic and social impacts continued to be a hot topic of debate.
> 
> 2. **"Irish Famine"** â€“ The Great Irish Famine (1845â€“1852) was a significant issue, particularly regarding the British government's response.
> 
> 3. **"Chartism"** â€“ Though the Chartist movement had declined by 1850, discussions about labor rights, suffrage, and political reform were still prevalent.
> 
> 4. **"Poor Law"** â€“ The administration and reform of the Poor Laws were highly debated, particularly in relation to poverty and public welfare.
> 
> 5. **"Colonial Governance"** â€“ Issues around the governance and management of British colonies, particularly in places like India and Ireland.
> 
> 6. **"Free Trade"** â€“ Discussions around free trade policies, especially in the aftermath of the Corn Laws repeal.
> 
> 7. **"Slavery"** â€“ Although the British Empire abolished slavery in 1833, discussions about slavery and the slave trade (particularly in relation to other nations) continued.
> 
> 8. **"Catholic Emancipation"** â€“ Ongoing debates about the rights of Catholics in Britain and Ireland.
> 
> 9. **"Industrialization"** â€“ Issues related to the rapid industrialization of Britain, including worker rights, factory conditions, and urbanization.
> 
> 10. **"Public Health"** â€“ Particularly in the context of the cholera outbreaks and the general health of the urban poor.
> 
> These keywords should help you identify debates that were particularly heated or significant during that time. You can further refine your analysis by focusing on specific phrases or related terms within these broader topics.

This response provides us with an overview of other critical, interrelated topics. Treating these topics as keywords can lead to additional, meaningful analyses. In the following examples, we use our code, introduced earlier in the chapter, for identifying the context for the word "Irish." 

```{r}
irish_context <- hansard_1850 %>% 
  filter(str_detect(text, "\\bIrish\\b")) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, "[:digit:]")) %>%
  count(word) %>%
  arrange(desc(n))
```

```{r}
top_irish <- irish_context  %>%
  top_n(20)
```

```{r, message=FALSE}
ggplot(data = top_irish) +
  geom_col(aes(x = reorder(word, n), 
               y = n),
           fill = "steel blue") +
  coord_flip() +
  labs(title = "Top Words Occuring in Sentences Mentioning Irish",
       subtitle = "From the 1850 Hansard debates",
       x = "Word",
       y = "Count") 
```

Here we have begun to show you how we can leverage our knowledge of historic theory, code, and AI to perform historical analyses. In later chapters, we will build upon this knowledge to introduce more text mining and analytic techniques, as well as to interrogate the usage of AI for data analysis. One question we will ask is whether ChatGPT is capable of replicating our analyses, or performing a historic reading. ChatGPT can be a powerful tool for exploratory analysis, and we will also explore the conditions under which ChatGPT can return nonfactual or overly general responses. 









