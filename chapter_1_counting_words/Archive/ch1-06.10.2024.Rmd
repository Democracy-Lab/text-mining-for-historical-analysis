---
title: 'Chapter 1: Counting Words in the British Parliamentary Reports'
output: pdf_document
geometry: "left=1in,right=1in,top=2in,bottom=1in"
---

```{r setup, include=FALSE}
# Chunk options
knitr::opts_chunk$set(
 fig.width = 6,
 fig.asp = 0.8,
 out.width = "80%"
)
```

The introduction to text mining in this book begins with tracking words' context.  As the linguist John Rupert Firth said, "You shall know a word by the company it keeps."  Tracking subtle variations in the context with which different words are used helps historians and other analysts to understand the cultural biases that structure our collective thinking.  They may help us to understand how a culture understands men and women differently, how it talks about people of different nationalities, the difference between a concept like "the foreign" and its near synonyms like "outsider" that nevertheless are used important and different ways, or how these concepts are changing over time.  In the course of this textbook, we will offer a taste of the subtle and refined approaches that scholars of texts and data scientists have used to unpack the context of different words.  

In this chapter we introduce three basic techniques for processing text so as to foreground change over time through investigating the changing relative frequencies of a keyword. In this chapter, we will: a) access data for the British parliamentary debates of the House of Lords and House of Commons, 1806-1911 from the *hansardr* library, b) filter that decade for keywords of interest, and c) count the words that appear in the same context as our keywords in order to study the keywords' meaning in more detail.  

Importantly, the techniques presented in this chapter have both promise and limits.  As our companion volume, __The Dangerous Art of Text Mining__, makes clear, simple word counts are almost never sufficient on their own to support an in-depth analysis of culture.  Nevertheless, knowing how to count words and compare the context with which different concepts are used is a first step to using text as data.  

## Loading the Hansard Data

The first step in working with text as data is to load the data we wish to analyze. Users of code routinely import software packages developed by other coders to expand the range of tools available to them.  

The authors of this book developed a data package to go with this book: hansardr. 

*hansardr* is a package for easily accessing our version of The Hansard 19th-Century British Parliamentary Debates with Improved Speaker Names within the R environment. This is a cleaned, analysis-ready corpus of the 19th-century British Parliamentary Debates (1803-1899), also known as "Hansard" in reference to Thomas Curzon Hansard, the publisher.  It identifies debates whose records are missing from UK Parliamentâ€™s corpus, and it also offers a field for disambiguated speakers. We believe these improvements will enable researchers to analyze the Hansard debates, including speaker discourse, in a way that has not been accessible before. Once installed, the data can be loaded with the familiar `data()` function.

To use a command from a software package, one must first install the package, something we learned to do in the introduction with the command `install.packages()`.  After a software package has been installed the first time, we can load the software package into memory using the command `library().`  A student of code typically loads the software packages to be used at the beginning of each new coding session. Loading only relevant packages for each session helps to keep the computer running quickly, which allows us to process large-scale data quickly and efficiently. For teaching purposes we will regularly load packages throughout a chapter so the reader can see which lines of code evoke different packages. 

**hansardr** can be installed from the Comprehensive R Archive Network (or "CRAN" for short). Use `install.packages()` to install a package from CRAN.

```
install.packages("hansardr")
```

If you have **devtools** installed, **hansardr** can also be installed from our GitHub repository:

```{r}
require(devtools)
install_github("stephbuon/hansardr")
```

As we explained in the introduction, packages only have to be installed once, but they need to be reloaded for each new R session using `library()`. `library()` tells R to load software programs that give us additional functionality. 

```{r, message=FALSE}
library(hansardr)
```

hansardr and dhmeasures are now ready for use.

### Navigating the hansardr data

To make working with the Hansard corpus quicker and easier--and to prevent our computers from being overloaded with too much data--we broke the century long corpus into decade sections. In the field of data analysis, it's common to call these sections "subsets."  The first kind of dataset we will meet is a collection of text: this data covers the recorded words spoken in parliament, 18063-1899, broken into sentences. Later on, we will meet extra datasets that contain "metadata," or information about the speaker, year and date of the speech, and other important contextual information which is useful to historical analysis.  

In this chapter, we will begin working with the text-based datasets, because they are easy to manipulate to count words. Counting words is the first step in most text mining for analysis of content.  

Both text and metadata are divided into subsets named by decade, for instance, *hansard_1850.*  We can use the command `data()` to load one of these decade-based subsets of the Hansard corpus. 

```{r}
data("hansard_1850")
```

A new variable named `hansard_1850` is now viewable in our Global Environment panel.  If you are using RStudio, you can explore the data by clicking on its name in the Global Environment pane (the top right quadrant of RStudio).  Clicking on the name of the dataset will open a new tab containing a view of the data, which you can search by keyword as you would in a spreadsheet. You can also double-click on any row in the data frame to select a row, then copy that data and paste it into a Word Document or other tool.  Please try copy and pasting some data from `hansard_1850` now. 

In the Global Environment Panel, you can also click on the blue ">" symbol to the left of each data frame to display the "fields" by which the dataset is structured, in this case, the names of the columns in the dataset, as well as a preview of the content of each of these columns.  Please investigate the data now.  

Throughout this book, we will routinely "inspect" the data by using commands like `head()` to see how the data is transformed between commands; this is a good habit to get into as you code.  A few commands will give you the tools to look at any part of the dataset in detail. 

Most of the time, when we load the data, we will want to quickly make sure that the data looks like what we expect; otherwise, errors may be produced.  At the command line, you can also use the command `head()` to ask R to display the first six rows of the data. In some versions of R Studio you will see an arrow that allows you to move between two columns.  The first column is `sentence_id`, which gives a unique identifier for each sentence, including information about which series of parliamentary proceedings we're looking at, which volume and page, and which sentence number on each page).  The second column is `text`, and each row is a distinct sentence of text, as identified by computational tools that look for punctuation and line breaks.  


```{r}
head(hansard_1850)
```

## Tidy Text Mining and Loading Software Libraries

Most of the commands used in the first chapters of this book come from a software package called *tidyverse* which is actually a family of smaller software packages developed by Chief Scientist at Posit (formerly known as RStudio), Hadley Wickham.  

The tidyverse commands were created to order data in "tidy" tables where data is structured as a data frame made up of columns and rows, and every observation has one row.  In other words, the tidyverse is built to work with data like the "text" column in *hansard_1850*.

We have found that a tidy approach to text exploration and analysis allows us to teach code quickly and effectively, giving analysts ample control over the data at all times, and allowing them to inspect the output of every form of analysis.  This powerful library thus allows analysts to focus on analytical questions without getting weighed down by the nuances of different structural and stylistic coding problems. 

We will also use the *tidytext* library, developed by David Robinson and Julia Silge to work with textual data.  The tidytext library contains a variety of commands useful for processing text and treating text as data.  Most lessons in this book will therefore begin by loading the two libraries tidyverse and tidytext.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
```

## Preparing Data for Wordcount: The `unnest_tokens()` Function

In order to count words over time, we first want to put our data into a format where each word of the text is on a separate row.  Only in this one-observation-per-row format can a computer easily count each word.  

As we have seen, the data in *hansard_1850* is structured into sentences or sentence fragments, with one sentence per row in the column "text." To count the words spoken in parliament in the 1850s, we must begin by treating the speeches as bags of words that can be counted and analyzed.

The *tidytext* package provides several functions that are useful for processing text.  Learning to use R typically proceeds by mastering one function at a time. "Functions," or commands, are the verbs of the computer world: they tell the computer what to do to the data, one step at a time.  

Our first function, `unnest_tokens()`, tells the computer to break up strings of text -- like the sentences in hansard_1850 -- into individual tokens.  "Tokens," in this case, refer to any unit of text that is meaningful for analysis, typically a word, a multi-word phrase (also called an n-gram), or a sentence. The process of splitting chunks of text into individual units is called "tokenization." 

In the line of code below, we apply the function `unnest_tokens()` to the "text" column of *hansard_1850*. We assign the output to a variable named *tokenized_hansard*. Instead of a column named "text" the output has a column named "word" containing each individual word of the corpus. 

Throughout the course of this book we will introduce multiple different kinds of symbols, called "operators." Certain operators connect the input, function, and output.  The assignment operator, `<-,` sometimes translated as "gets," points to the output of a command and is followed by the name of an original dataset.  The symbol `%>%` called the "pipe," and translated "next," is used to string together successive transformations of a dataset. This will make more sense when we cover more examples later in the book.

```{r}
tokenized_hansard <- hansard_1850 %>%
  unnest_tokens(word, text)
```

If this is your first time using a programming language for analysis, it is useful to practice trying to read each line of code as a sentence of instructions which tells the computer what output to make, what input to use, and what processes to go through in a certain order.  

We sometimes suggest that first time programmers translate code into a series of English sentences, using their knowledge of the `<-` and `%>%` operators, the names of variables, and their understanding of distinct commands.  The line of code below could be read as follows: "Create a new variable called *tokenized_hansard*.  Start with the dataset *hansard_1850*.  Next, unnest the tokens in the *text* column of *hansard_1850*, unpacking those strings of text into individual words."

In the line of code above, you may also notice the use of the argument "word" in the line of code, `unnest_tokens(word, text).` "Arguments" refer to the instructions given to a command in code.  Commands like `unnest_tokens()` take a set number of arguments, typically in a precise order.  The arguments gives the command precise instructions about what to do.  The function `unnest_tokens()` takes two arguments: the first is the kind of token to identify, whether a word or a sentence.  The second argument is the name of the column to work on, in this case, "text."  

We have instructed `unnest_tokens()` to "unnest" the sentences in hansard_1850 into individual words -- hence the "word" argument in the line of code `unnest_tokens(word, text)`.  

As you inspect the arguments of each command, consider the fact that those arguments exist because they allow the coder to change the arguments to get different results.  With the `unnest_tokens()` command, the user could hypothetically choose to "unnest" our sentences into tokens of different length, for instance, sentences, n-grams (that is, multi-word phrases), or other units of text.   In later chapters we will explore some of these features of the command "unnest."   Alternatively, if the `unnest_tokens()` command were being applied to a different dataset where the column had a different name than "text," we could change the column name so that the command would work. In later examples, you will see the same commands applied to different datasets with different column names.  For now, we will mainly use `unnest_tokens()` with the arguments "word" and "text."

Next, let's inspect the results of our unnesting using `head()`, which returns just the first five rows of a data frame.

```{r, echo=FALSE}
head(tokenized_hansard)
```

Notice that the words in `tokenized_hansard` look just like the words in the first sentence of the first speech in hansard_1850. Those speeches have been split into words, but the words are still in the same order.  

Notice that the command `unnest_tokens()` has done a little extra work for us: it has transformed all the words to lower-case and removed punctuation marks such as commas and periods.  This transformation will make the words easier to count.  

As we inspect the results of unnesting, you should notice that the input was "tidy" in format -- that is, it had two columns, one for "sentence_id" and one for "text," the sentences and sentence fragments of the original parliamentary speeches. 

After working with the command unnest_tokens(), the resulting output, `tokenized_hansard`, also is "tidy" in format.  That is, it has two columns, one for `sentence_id` and one for `word`.  In this tidy format, where each word is on its each row, the computer can easily count the words in the dataframe.  

The tidyverse command "count" groups like entries with like entries, counting every word.  The command "count" takes one argument, the name of the column to work on -- in this case, "word."

```{r}
tokenized_hansard_counts <- tokenized_hansard %>%
  count(word) %>%
  arrange(desc(n))
```

With the command `arrange(desc(n))`, we tell the computer to arrange the results from greatest to least.

```{r, echo=FALSE}
head(tokenized_hansard_counts)
```

We now have a frequency count of every word recorded in the speeches of Hansard, 1850-59. The initial results are unimpressive.  In our dataset, as with most datasets of text, the most common words are also the least interesting.  For this reason, linguists compile lists of the most common words, which they call "stop words." Analysts often remove stop words from their results before analysis.  

The tidytext package comes with a prepared list of stop words.  We can remove them from our results.

```{r, echo=FALSE}
data("stop_words")

stopworded_hansard_counts <- tokenized_hansard_counts %>%
  anti_join(stop_words)
```

```{r}
head(stopworded_hansard_counts)
```

These results seem sensible for parliament. Some of them demonstrate the way that members of parliament refer to each other, for example, as "my noble lord" or "the hon[ourable] member [in question]."  Speakers in parliament also routinely refer to the business of parliament, especially referencing the "house [of commons or lords]," the "bill" or "question" under debate, the state of the "country," and how much "time" is available for debate.  

General words, of course, give us little insight into the business of parliament.  Later chapters will give us richer tools for examining how the content of speeches changed over time.  For now, we have successfully achieved the first step of analysis, the processing of text into words, and the counting of words. 

## Searching for Keywords

The Tidyverse provides several commands that we will use throughout this book to track individual words.  Two commands, `filter()` and `str_detect()`, are used together to match and search data.  

The command "filter" allows us to find an exact match in the data.  Suppose that we want to inspect the tenth sentence of the parliamentary speeches, series 3, volume 108, which has the id number "S3V0108P0_10."  

We use the command `filter()`, which takes two arguments: first, the name of the column we will search for ("word"), and second the name of the strong to match ("india"). Another symbol, the double equals sign (`==`), is used to tell the computer that we want an exact match.

```{r}
stopworded_hansard_counts %>% 
  filter(word == "india")
```

```{r}
stopworded_hansard_counts %>% 
  filter(word == "france")
```

```{r}
stopworded_hansard_counts %>% 
  filter(word == "jamaica")
```

Filter can also be used with standard R operators, such as `%in%`. In this example, we create a list of words using the command "c" or "concatenate."  Then, we search for the list of words:

```{r}
female_list = c("girl", "woman", "girls", "women", "wife", "wives", "widow", "widows", "sister", "sisters", "famale", "females", "grandmother", "grandmothers", "aunt", "aunts")

stopworded_hansard_counts %>% 
  filter(word %in% female_list)
```

The command "filter" also works on quantitative information with comparative and boolean operations. 

```{r}
stopworded_hansard_counts %>% 
  filter(n > 474 & n < 476)
```

Our next command, `str_detect()`, detects the presence or absence of a pattern and returns the boolean values `TRUE` or `FALSE`.  

The following example asks the computer to move through each word in the list "female_list" and to check whether the value contains the string "girl."

```{r}
female_list %>% 
  str_detect("girl")
```

In practice, textual analysis often requires that we use both commands, `filter()` and `str_detect()`, together.

The following example searches hansard_1850 for the keyword "coal." Notice that `str_detect()` is nested inside the command `filter()`.  Together, these commands tell the computer to filter for only those rows of the column 'text' that contain the keyword "coal."

```{r}
hansard_coal <- hansard_1850 %>%
  filter(str_detect(text, "\\bcoal\\b"))
```

The result of our search is variable, `hansard_coal`, that contains only sentences from the 1850 debates that contain the word "coal."

```{r}
head(hansard_coal)
```

Inspecting the line of code above, you may notice that we placed special characters -- `\\b` -- on both sides of the keyword. Together with the word "coal," these special characters are make up part of a "regular expression" or "regex." The `\\b` indicates word boundaries. In other words, we told `str_detect()` that we only wish to return text that contain an exact match with the word "coal." If we didn't use word boundaries when telling the computer to look for "coal," the computer would also search for every word containing "coal," including "coalition." 

## Finding and Comparing Keywords' Context  

Typically, we want to do more than simply search for a keyword.   We may also want to search for collocates, the words that form the context of a given keyword.  We already have everything we need to make this happen: we can search for a keyword, we can break up text into words, and we can count the words.  Let's use our tools to search for the context of several keywords and compare them in visual form.  

```{r}
coal_context <- hansard_coal %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, "[:digit:]")) %>%
  count(word) %>%
  arrange(desc(n))
```

As you may have noticed, you can run these many of functions independently or chained together with the 'pipe' from the maggirtr package: `%>%`. Using the pipe allows us to use many functions sequentially without storing intermediary variables. 

Notice the line of code that reads `filter(!str_detect(word, "[:digit:]"))`.  Here, we are filtering out any entries that contain digits as part of the word.

```{r}
head(coal_context)
```

```{r}
top_coal <- coal_context  %>%
  top_n(20)
```

```{r}
ggplot(data = top_coal) +
  geom_col(aes(x = reorder(word, n), 
               y = n),
           fill = "steel blue") +
  coord_flip() +
  labs(title = "Figure 1.1: Top Words Occuring in Sentences Mentioning Coal",
       subtitle = "From the 1850 Hansard debates",
       x = "Word",
       y = "Count") 
```

Our analysis will gain nuance the more we compare near concepts.  While coal was important to powering Britain's industrial revolution, perhaps the most controversial economic debates of the 1850s revolved around the issue of the Corn Laws, the system of taxation of wheat and other grains, which were originally introduced to protect British farmers.  Working-class demands for cheap bread led to the repeal of the Corn Laws in 1846.  Knowledge of these facts might lead us to ask: were coal and corn debated using the same language or different words?

```{r}
corn_context <- hansard_1850 %>% 
  filter(str_detect(text, "\\bcorn\\b")) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, "[:digit:]")) %>%
  count(word) %>%
  arrange(desc(n))
```

```{r}
head(corn_context)
```

```{r}
top_corn <- corn_context  %>%
  top_n(20)
```

```{r}
ggplot(data = top_corn) +
  geom_col(aes(x = reorder(word, n), 
               y = n),
           fill = "steel blue") +
  coord_flip() +
  labs(title = "Top Words Occuring in Sentences Mentioning Corn",
       subtitle = "From the 1850 Hansard debates",
       x = "Word",
       y = "Count") 
```

Comparison between the words used in the context of coal and corn gives us a foothold for beginning to think about how taxation was discussed in different contexts.  For instance, the context of both keywords reference the role of time, duties, taxes, and the government, but debates over corn are more linked to the question of Britain's dependence "foreign" powers, whereas debates over coal are more crucially related to the condition of "England" itself. We do not pretend that noticing this difference constitutes a major historical insight; further reading and research would be necessary for the analyst to decide whether comparing these two commodites as keywords is worthwhile, and which collocates offer insight.

Nevertheless, the student will have learned by this point the preliminary process with which most of the analysis in this book will start: loading data, inspecting data, breaking data into words, and counting those words.  Later chapters will give us more tools -- importantly the tools of counting data over time and understandign historical change -- which can produce a richer form of analysis.  

## Finding a Word's Context using KWIC

Another way to explore a word's context is by using a tool termed "Keywords in Context" or "KWIC" for short. In this section we use `KWIC()` from the quanteda library and look at the sentence-level context for the word "corn" for February 1850. 

To look at just the month of February, we need to load our decade subset that contains information about dates, called `debate_metadata_1850`, and join it with the data set containing the debates, `hansard_1850`. We will cover joins in greater detail in chapter 2. 

```{r}
data("debate_metadata_1850")
```

```{r}
head(debate_metadata_1850)
```

```{r}
hansard_1850_with_metadata <- left_join(hansard_1850, debate_metadata_1850)
```

```{r}
head(hansard_1850_with_metadata)
```

In the folllowing code we filter for speech dates that are on or after (greater than or equal to `>=`) February 1, 1850, and on or before (less than or equal to `<=`) February 28, 1850.

```{r}
top_month <- hansard_1850_with_metadata %>% 
  filter(speechdate >= "1850-02-01",
         speechdate <= "1850-02-28") 
```

KWIC returns a concordance-style text output with the words that come directly before and after the keyword in a sentence. Quanteda's version of `KWIC()` works quickest if we first transform the data from a dataframe to a "corpus" object using `corpus()`. We tell Quanteda that the column we want to operate on is called "text."

When using `KWIC()` we can specify the keyword, the maximum number of words to display before and after the keyword, and we can also tell `KWIC()` to do a case insensitive match so that we match with uppercase or lowercase instances of our keyword.

```{r}
library(quanteda)

my_corpus <- corpus(top_month, text_field = "text")

corn_kwic <- kwic(tokens(my_corpus), 
                  "corn",
                  window = 5,
                  case_insensitive = TRUE)  
```

```{r}
head(corn_kwic)
```

## Critical Thinking With Collocates

When is collocate analysis the right tool?  Researchers of conceptual history frequently use collocates to understand the subtle differences between synonyms.  For instance, historian Ruben Ros has investigated the way that Dutch newspapers in the nineteenth century began to use terms such as "foreign," "overseas," and "strange" in the process of constructing a narrative that increasingly linked the dangers of foreign influence and suspicion of ethnic minorities.  

With collocate analysis alone, we cannot trace discourses over time -- a crucial step in understanding the construction of political and cultural positions.  But we can begin to tease apart the meanings of closely-related ideas at various points in the past.  Let's recreate Ros's case study in miniature here, asking the question: how did British members of parliament in the 1850s talk about foreigners and colonial subjects?

The following code uses a "for loop" to cycle through a list of related words.  We will look at for loops in greater detail later in the book, but the basic concept should be easy to grasp.  For each of the words in the list "wordlist," the for loop performs the same set of operations: it searches hansard_1850 for each keyword, then it counts the words in context, and it graphs the top results.  The for loop allows us to produce a series of related graphs in succession without writing out the same instructions multiple times.  

```{r, results='asis'}
wordlist <- c("foreign", "overseas", "colonial", "savage", "native", "barbarian")

for (word in wordlist) {
  
  filtered_hansard_1850 <- hansard_1850 %>% 
    filter(str_detect(word, paste0("\\b", word, "\\b"))) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    filter(!str_detect(word, "[:digit:]")) %>%
    count(word) %>%
    top_n(20)

  ggplot(filtered_hansard_1850) +
    geom_col(aes(x = reorder(word, n), 
                 y = n),
             fill = "steel blue") +
  coord_flip() +
  labs(title = "Top Words Occuring in Sentences Mentioning ", toupper(word),
       subtitle = "From the 1850 Hansard debates",
       x = "Word",
       y = "Count") }
```

Producing many graphs in succession can be useful for the process of preparing material for careful comparison, which is one of the major skills that humanists use when considering the subtle structures of meaning and difference that define cultural and political conversations.  A careful comparison between the charts above offers the beginnings of a nuanced analysis of overlapping terms used by British members of parliament to refer to people and forces from beyond Britain.  

In the results, we see several closely related terms which compose a field of interrelated meanings.  Together, these terms sketch out the diversity of contrasting ways that British members of parliament spoke about the world outside of Britain. Some of the attributes ascribed to the foreign were neutral in value -- for instance, the "foreign" was mainly spoken about in terms of laws, duties, trade, and protection, a world in which different nations "competed," but generally not a space of moral inferiority and superiority.  By contrast, terms such as "savage" and "barbarian" telegraphed intellectual and ethical judgments (for instance, "brutal") onto cultures outside of Britain, typically associating inferiority with racial identity.  With these terms, a set of binaries is set up, dividing the world into "barbarian" cultures and "christian" "civilisations," a distinction that may have been analyzed in terms of "rights," "treaties," "war," "rents," "passions," "liberty," and "energies.   The distinctions between the "foreign" and the "barbarian" are invoked in relationship to the same abstractions we saw typifying the "foreign," including  "law," the "country," and "treaties."  We also see the "barbarian" being invoked in discussions of many commodities, including "woolens," "forage," and "rent."  Indeed, the frequent invocation of law, rights, and foreignness alongside discussions of the barbarian and the savage suggests that the judgments ascribed to race actually bled into conversations about law, tariffs, trade, and protectionism, perhaps as justification for invoking British superiority. 

Some readers may assume that we are performing contemporary prejudices about racism, but the point of this exercise is that it is grounded in a detailed and careful examination of the words counted by computational methods. The paragraph above is not concocted out of thin air; it is actually an objective description of the prejudices on display in a quantitative reading of how British members of parliament spoke about the world beyond their nation.  We emphasize that this analysis of the related language of the "foreign" and the "barbarian" is not concocted on the basis of our readings of contemporary theories about racism, but is actually derived from a careful reading of language, as well as immersion in writing about the history of empire.    

In general, descriptions of word counts and arguments made on the basis of word counts tend to be more persuasive when they take on every word in a list.  Critical thinking about the meaning of proximate keywords is enhanced when the analyst slows down, taking the trouble to examine the distinctiveness of each individual keyword and its context.  In a further analysis, the scholar might ask questions that require examining the results in more detail.  They might ask: What biases does each term convey?  Where do the terms overlap?  What new information is encapsulated by some words but not others?  A thorough examination would trace the keyword-context pairs back to their original context in sentences and speeches on the page, showing how speakers used these words to formulate actual arguments with consequences for the development of nations and their economies.  

Examining collocates provides the basis for critical thinking about the changing and overlapping meaning of shared concepts in the human past.  Skillful analysts can use approaches of this kind to examine the meaning of language about categories beyond those of gender, race, nationality, and commodities, asking questions about the intellectual categories that governed how members of parliament understood governance itself, economics, truth, science, virtue, economics, and reason itself.  In every case, the approach would be the same: to examine not merely one or two keywords, but many related keywords, drawing attention to the subtle overlap and differences between the terms in usage, and how those overlapping concepts together work to produce a field of meaning.   

## Critical Thinking About Data and its Limits

Another fruitful avenue for critical thinking is the question of which problems are suited to textual analysis and which are not. Skillful analysts are wary of asking the wrong question with the wrong tool.  Among the techniques that we have learned in this chapter is the ability to load and inspect data.  Inspecting the data allows the analyst to notice issues caused by computational processing of text that may cause issues down the line unless the analyst is aware of them.

Looking at the first lines of `hansard_1850`, you might ask yourself these questions: are all of the lines in the "text" field actually sentences?  Are all the words in the same style with respect to capitalization?  

Here is another command which gives us a little more control over what we see.  The square brackets tell R that we want to look at a certain column and row or group of columns and rows.  The structure of the syntax is this: [row, column].  The syntax "1:10" in the "row" field tells r to display rows 1 to 10.  We put "2" in the "column" field to tell R to display

```{r}
hansard_1850[1:10, 2]
```

Perhaps you have noticed that some of the rows in the "text" field appear not to be complete sentences.  We can use square brackets to investigate further.

```{r}
hansard_1850[1, 2]
```

Notice that your view of the data is cut off in this view.  To view the whole text, double-click on the cell, copy and paste the text into a word processor.  You should see this content:

", which had been prorogued successively from the 1st of August to the 9th of October, from thence to the 20th of November, thence to the 16th of January, and from thence to the 31st January, met this day for despatch [sic] of business."

The first row appears to be a description of the opening of parliament.  Technically, it is a commentary by the printer, not a line of debate. The convention of discussing parliament's timetables was in place in 1850, but we don't know if the printer always printed this line through the entire century.  We should be aware of this convention and curious about it if we find ourselves counting months or discussions of words such as "business."

Let's keep inspecting the data, this time turning to the third row in our dataset.

```{r}
hansard_1850[3,2]
```

Here, we see another sentence fragment:

"called attention to a great omission of their duty on the part of Ministers, with respect to the privileges of their Lordships, which might and ought to have been avoided."

The sentence fragment has resulted from the computer attempting to generate data while working on  another convention of printing: the fact that the publisher of Hansard for much of the century gave the name of the speaker followed by a description of the airing of the speech.  On the page, we would be given the name of a speaker or their title, for example, "The Lord Chancellor" or "Mr. Gladstone."  The passage of text would therefore read, [the speaker] "called attention to a great omission..." -- in other words, it would not be a sentence fragment, so much as a journalistic description of a speaker.   The actual speech probably began with a statement something like this: "It is a great omission of their duty on the part of Ministers[...]."  

In other words, we are looking at an error that resulted from a mismatch between the nineteenth-century printed record and what we asked the computer to do with the text of that record.  In compiling the data, we asked the computer to separate all of the speaker names into one "field" of data, which we stored in a separate dataset, to be accessed in Chapter 2.  We have sorted all of the text into the "text" field, which is held in hansard_1850.   The computer doesn't know how to process the journalistic description of a speech rather than a normal speech. 

Looking at the data presents an opportunity for critical thinking. Analysts of text as data routinely need to inspect their data and make sure that the data's quality and density are sufficient to support claims that they might make.

Already, we can see that the data has certain features that might interfere with certain kinds of queries, and we should already be thinking about what these limits might be. Yet the convention of journalistic reporting on speeches can cause trouble for the analyst if the analysts aren't aware of what has changed.  The conventions of describing speeches changed over the course of the century; at the beginning of the century, journalistic descriptions were more common. Later in the century, the conventions changed, and many more speeches were reported near verbatim without journalistic commentary.  We should be aware of conventions of this kind if we count words which may be related to journalistic observations such as "attention," "spoke," "declared," and so on. Otherwise we may be tempted to interpret changes to the publisher's conventions of printing speeches as evidence for changing ideas about politics.

Whether or not data quality will interfere with our ability to make inferences from the data depends on what we are asking.  If we are counting mentions of the word "duty" or the rest of the words in the content of the speech, then our counts of the words will be correct. If we contrive to study the history of journalistic observation by tracking sentence fragments in the dataset, this data will support such a reading.  But we should be wary making inferences about any kind of inquiry where there is a risk of overlap between substantive discussions and journalistic observation.  

## Exercises

Expand your grasp of the code and your powers of critical reflection by trying the following exercises:

1) Alter the code above to compare the context for the keywords "girl" and "woman."  Do you expect the context to be similar for both?  Does anything surprise you about the results?

2) Use your powers of inspecting data to move from the keyword context back to the individual sentences.  Cut and paste twenty sentences from the context of each keyword into a word processor.  What do you learn from these sentences that you did not learn from counting keywords?  Does your analysis become more perusasive when quantitative graphs are accompanied with readings of particular speeches where the words are examined in their original context?  

3) We have examined 'coal' and 'corn' in the 1850s.  Now, search for the words 'coal' and 'corn' in dataset `hansard_1820`.  Were they being used in the same way?  What changed?

4) Above, we created the list of words called `female_list`.  Use the operator `%in%` with the command `filter()` to create a list of the words in the context of discussions of women.  Create another list called `male_list` and do the same for discussions of men.  Create visualizations for the top terms used in discussions of men and women.

5) Reflect on your work so far -- in which you have compared words for gender, the concept of the foreign, and the context of discussions of different commodities.  You have made comparisons over time and comparisons of lists.  Which of the comparisons do you find the most productive of insight about how certain words were being used in political debate?  Are you persuaded by large numbers?  Are you persuaded by subtle variations in the context of different words?  Write a paragraph explaining for yourself wherein constitutes the most persuasive usage of comparisons between keyword in context.  

6) For the visualizations of the terms associated with different terms for the "foreign," tell us: What biases does each term convey?  Where do the terms overlap?  What new information is encapsulated by some words but not others?  What do we learn from this exercise about the prejudices of Britain's members of parliament in the 1850s?

