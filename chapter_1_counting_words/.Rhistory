align = c("l", "r"),
booktabs = TRUE) %>%
kable_styling(full_width = FALSE)
tbl_2020
library(ggrepel)
# take the top N words returned by most_similar_coal
words <- most_similar_coal$word
# include the anchor term too
words <- c("coal", words)
# subset embedding matrix to just those words
emb_small <- word_embeddings_2000[words, ]
# reduce high dimensions → 2D PCA
pca <- prcomp(emb_small, center = TRUE, scale. = TRUE)
coords <- as_tibble(pca$x[,1:2]) %>%
mutate(word = words)
# plot
ggplot(coords, aes(PC1, PC2, label = word)) +
geom_point(size = 3) +
geom_text_repel() +
theme_minimal() +
ggtitle("Words close to 'coal' in embedding space (2D PCA)")
vector_subtraction <- function(word1, word2, embeddings, top_n = 10) {
# keep 1×d matrices so sim2() gets matrices on both sides
a <- embeddings[word1, , drop = FALSE]
b <- embeddings[word2, , drop = FALSE]
target <- a - b
sims <- sim2(embeddings, target, method = "cosine", norm = "l2")[, 1]
tibble(word = names(sims), similarity = unname(sims)) %>%
filter(!word %in% c(word1, word2)) %>%
arrange(desc(similarity)) %>%
slice_head(n = top_n) }
vector_subtraction("renewable", "fossil", word_embeddings_2000)
vector_subtraction("democrat", "republican", word_embeddings_2010)
vector_addition <- function(word1, word2, embeddings, top_n = 10) {
a <- embeddings[word1, , drop = FALSE]
b <- embeddings[word2, , drop = FALSE]
target <- a + b
sims <- sim2(embeddings, target, method = "cosine", norm = "l2")[,1]
tibble(word = names(sims), similarity = unname(sims)) %>%
filter(!word %in% c(word1, word2)) %>%
arrange(desc(similarity)) %>%
slice_head(n = top_n) }
vector_addition("climate", "earth", congress_daily_climate_change_2000)
vector_addition("climate", "earth", congress_daily_climate_change_2000)
vector_addition("republican", "earth", congress_daily_climate_change_2000)
vector_addition("republican", "democrate", congress_daily_climate_change_2000)
vector_addition("republican", "democrat", congress_daily_climate_change_2000)
vector_addition("man", "woman", congress_daily_climate_change_2000)
congress_daily_climate_change_enter <- congress_daily_climate_change_2000_2020 %>%
mutate(date = ymd(date),
year = year(date),
period_5 = paste0(floor(year/5)*5, "-", floor(year/5)*5 + 4))
congress_daily_climate_change_enter <- congress_daily_climate_change_enter %>%
mutate(period_5 = ifelse(year == 2020, "2020", period_5))
# 0) Make 5-year periods (2000–2004, 2005–2009, 2010–2014, 2015–2019, 2020)
congress_5 <- congress_daily_climate_change_2000_2020 %>%
mutate(
date = ymd(date),
year = year(date),
period_5 = paste0(floor(year/5)*5, "-", floor(year/5)*5 + 4),
period_5 = ifelse(year == 2020, "2020", period_5)
)
# 1) Split text by 5-year period -> named list of character vectors
texts_by_period <- congress_5 %>%
filter(!is.na(content), str_squish(content) != "") %>%
split(.$period_5) %>%
map(~ .x$content)
# 2) Build embeddings per 5-year period
# Assumes you already have a function:
#   find_word_embeddings(text_vec) -> matrix with rownames = words
embeddings_by_period <- texts_by_period %>%
imap(~ {
if (length(.x) == 0) return(NULL)
find_word_embeddings(.x)
}) %>%
discard(is.null)
# 3) Similarity trajectory over 5-year periods
similarity_trajectory <- function(word_a, word_b, embs = embeddings_by_period) {
# order the x-axis by the numeric start year of each label
order_levels <- names(embs) %>%
as_tibble() %>%
rename(lbl = value) %>%
mutate(start = as.integer(str_replace(lbl, "-.*$", ""))) %>%
arrange(start) %>%
pull(lbl)
tibble(period = factor(names(embs), levels = order_levels)) %>%
mutate(
similarity = map_dbl(as.character(period), \(p) {
W <- embs[[p]]
if (is.null(W))                          return(NA_real_)
if (!all(c(word_a, word_b) %in% rownames(W))) return(NA_real_)
as.numeric(sim2(
W[word_a, , drop = FALSE],
W[word_b, , drop = FALSE],
method = "cosine", norm = "l2"
))
})
)
}
# 4) Example
df_sim <- similarity_trajectory("coal", "emissions")
ggplot(df_sim, aes(period, similarity, group = 1)) +
geom_line(linewidth = 1) +
geom_point(size = 3) +
theme_minimal() +
labs(
title = "Cosine similarity across 5-year periods",
subtitle = "coal  ~  emissions",
x = "5-year period", y = "Cosine similarity (−1 to 1)"
)
vector_subtraction("renewable", "fossil", word_embeddings_2010)
knitr::opts_chunk$set(echo = TRUE)
# Use the hansardr library to load the debate text
library("hansardr")
data("hansard_1860")
# This code does not show, just the table
library("knitr")
library("kableExtra")
library(tidyverse)
df_head <- hansard_1860 %>%
slice_head(n = 5)
kbl(df_head, format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Hansard Speeches 1860") %>%
kable_styling(
latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
data("debate_metadata_1860")
# This code does not show, just the table
df_head <- debate_metadata_1860 %>%
slice_head(n = 5)
kbl(df_head, format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Hansard Metadata 1860.") %>%
kable_styling(
latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# Load Hansard speaker metadata for 1860
data("speaker_metadata_1860")
# This code does not show, just the table
df_head <- speaker_metadata_1860 %>%
slice_head(n = 5)
kbl(df_head, format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Hansard Metadata 1860.") %>%
kable_styling(
latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# Load Hansard file metadata for 1860
data("file_metadata_1860")
# This code does not show, just the table
df_head <- file_metadata_1860 %>%
slice_head(n = 5)
kbl(df_head, format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Source File Metadata 1860.") %>%
kable_styling(
latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# Load the tidyverse
library("tidyverse")
# Join the Hansard debate text with the debate metadata into a single data frame
# left_join() will automatically detect that both datasets share the sentence_id
hansard_1860 <- left_join(hansard_1860, debate_metadata_1860)
# Show the first few rows of the 1860 Hansard debate text
hansard_1860 %>%
head() %>%
mutate(text = str_trunc(text, 120)) %>%
kable("latex", booktabs = TRUE) %>%
kable_styling(full_width = FALSE, position = "left") %>%
column_spec(2, width = "4cm")
# load packages for tokenization and date handling
library("tidytext")
library("lubridate")
# create a new data frame "hansard_1867" from "hansard_1860" that contains
# data for 1867
hansard_1867 <- hansard_1860 %>% # create a new data frame
mutate(year = year(speechdate)) %>% # extract year from speechdate
filter(year == 1867, # keep only year 1867
speechdate <= ymd("1867-08-15")) # and speechdate before or on august 15
# Generate bigrams (two-word sequences) from the "text" column of the hansard_1867 data
bigrams_1867 <- hansard_1867 %>% # create a new dataset
unnest_tokens(bigram, text, token = "ngrams", n = 2) # unnest bigrams
# This code does not show, just the table
df_head <- bigrams_1867 %>%
slice_head(n = 5)
kbl(df_head, format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Hansard Bigrams with Metadata.") %>%
kable_styling(
latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# remove bigrams that contain any stop words
# keep only bigrams made up of two lowercase alphabetic words
clean_bigrams_1867 <- bigrams_1867 %>%
filter(!str_detect(bigram,
paste0("\\b(", paste(stop_words$word, collapse = "|"), ")\\b"))) %>%
filter(str_detect(bigram, "^[a-z]+ [a-z]+$"))
# This code does not show, just the table
df_head <- clean_bigrams_1867 %>%
slice_head(n = 5)
kbl(df_head, format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Clean Hansard Bigrams with Metadata.") %>%
kable_styling(latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# count the frequency of each bigram in the cleaned dataset
# keep the top 100 most frequent bigrams and sort them in descending order
top_bigrams_1867 <- clean_bigrams_1867 %>%
count(bigram) %>% # count frequency of each bigram
top_n(100) %>% # select top 100 bigrams
arrange(desc(n)) # sort bigrams by frequency, highest first
# This code does not show, just the table
df_head <- top_bigrams_1867 %>%
slice_head(n = 5)
kbl(df_head, format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Top Hansard Bigrams for 1867.") %>%
kable_styling(latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# Filter the 1867 Hansard speeches to keep only those that mention "public meetings"
my_reading <- hansard_1867 %>% # create a new dataset
filter(str_detect(text, "public meetings")) # detect the phrase
my_reading %>%
head() %>%
mutate(text = str_wrap(text, 80)) %>%
select(text) %>%
kable("latex", booktabs = TRUE, longtable = TRUE, escape = FALSE) %>%
column_spec(1, width = "14cm")
# create 5-grams (sequences of 5 consecutive words) from the "text" column
# store the results in a new column called "ngram"
# unnest_tokens() is used with token = "ngrams" and n = 5 to generate 5-word sequences
fivegrams_1867 <- hansard_1867 %>% # create a new dataset
unnest_tokens(ngram, text, token = "ngrams", n = 5) # generate 5-grams from text
# filter for 5-grams that contain the word "people"
# count the frequency of those 5-grams and keep the top 50
people_n_grams <- fivegrams_1867 %>% # create a new dataset
filter(str_detect(ngram, "people")) %>% # keep 5-grams containing "people"
count(ngram) %>% # count frequency of each 5-gram
top_n(50) # keep top 50 by count
head(people_n_grams)
# Filter the 1867 Hansard speeches to include only rows where the exact phrase
# "minds of the people" appears in the "text" column
minds <- hansard_1867 %>% # create a new dataset
filter(str_detect(text, "minds of the people")) # detect the phrase
minds %>%
head() %>%
mutate(text = str_wrap(text, 80)) %>%
select(text) %>%
kable("latex", booktabs = TRUE, longtable = TRUE, escape = FALSE) %>%
column_spec(1, width = "14cm")
# Load the Hansard speech data and the corresponding debate metadata for the 1860s
data("hansard_1860")
data("debate_metadata_1860")
# merge the hansard speech data with its metadata using a left join
# this retains all rows from hansard_1860 and adds matching metadata columns
hansard_1860 <- left_join(hansard_1860, debate_metadata_1860)
# create a new dataset for just the year 1867
hansard_1867 <- hansard_1860 %>% # create a new data frame
mutate(year = year(speechdate)) %>% # keep only speeches from the year 1867,
filter(year == 1867, #keep only speeches made on or before August 15th
speechdate <= as.Date("1867-08-15"))
# Create bigrams (2-word sequences) from the "text" column of the 1867 Hansard data
# - "bigram" is the name of the new column to store the 2-word phrases
# - "text" is the input column containing the speech text
# - token = "ngrams" with n = 2 extracts bigrams
bigrams_1867 <- hansard_1867 %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
# remove bigrams that contain any stop word (e.g., "the", "of", "and")
# and keep only bigrams made up of two lowercase alphabetic words
clean_bigrams_1867 <- bigrams_1867 %>%
filter(!str_detect(bigram, paste0("\\b(",
paste(stop_words$word, collapse = "|"), ")\\b"))) %>%
filter(str_detect(bigram, "^[a-z]+ [a-z]+$"))
head(clean_bigrams_1867)
# define a controlled vocabulary of event-related keywords to search for in bigrams
controlled_vocab = c("riot", "meeting", "famine","revolt", "exhibition", "massacre",
"strike", "war")
# filter the cleaned bigrams to keep only those that contain one of the vocabulary terms
# build a regex pattern from the vocabulary and match bigrams containing those words
# select only the bigram and the associated speech date for further analysis
events_1867 <- clean_bigrams_1867 %>%
filter(str_detect(bigram,
paste0("\\b(", paste(controlled_vocab, collapse = "|"), ")\\b"))) %>%
select(bigram, speechdate)
head(events_1867)
# define a controlled vocabulary of specific multi-word historical events (as bigrams)
controlled_vocab_events = c("yorkshire meeting", "clontarf meeting", "recent meeting",
"public meeting", "paris exhibition", "orissa famine",
"irish famine", "crimean war", "civil war", "affghan war",
"kaffir war", "bristol riot")
# filter the events to keep only those that exactly match a defined historical event
# group the matched events and count their occurrences
# keep only the top 10 most frequently mentioned events
top_events_1867 <- events_1867 %>%
filter(bigram %in% controlled_vocab_events) %>% # match exact event bigrams
group_by(bigram) %>% # group by event phrase
summarize(total = n()) %>% # count occurrences
top_n(10) # keep top 10 events
head(top_events_1867)
# join the top event bigrams with the full events dataset to recover their speech dates
# extract the month, count mentions by date, and return a regular data frame
top_events_1867_w_speech_metadata <- top_events_1867 %>%
left_join(events_1867, by = "bigram") %>%  # join to recover speech dates
mutate(month = month(speechdate)) %>% # extract month from speechdate
count(bigram, speechdate) %>% # count mentions by date
ungroup() # remove grouping
head(top_events_1867_w_speech_metadata)
# Load the viridis color palette library for colorblind-friendly colors
library("viridis")
# Create a bubble plot showing how often specific events were mentioned over time
ggplot(top_events_1867_w_speech_metadata,
aes(x = speechdate, # x-axis: date of the speech
y = bigram, # y-axis: event bigram (e.g., "irish famine")
size = n, # bubble size represents number of mentions
color = n)) + # bubble color also reflects number of mentions
# Use a viridis color scale with a log transformation for good contrast in frequency
# treat "n" as continuous, not categorical, for a smooth gradient of colors
scale_color_viridis(breaks = round, # apply rounding to color scale breaks
trans = "log", # log-transform to spread small/large counts
option = "A", # use Viridis option A color scheme
discrete = FALSE, # treat "n" as continuous
direction = 1) + # default color direction (ascending)
# format and customize the appearance of a timeline plot showing event mentions
# x-axis shows months, point size reflects frequency, and plot is styled for readability
# includes rotated labels, extended margins, and simplified legend
scale_x_date(date_breaks = "1 month", date_labels = "%B") + # x axis one label per month
scale_size_continuous(range = c(1, 10)) + # scale point sizes
geom_point(alpha = .5, shape = 15) + # semi-transparent square points
coord_cartesian(clip = "off") + # don't clip outer elements
# appearance tweaks: legend, axis, margin, and label formatting
theme(legend.position = "bottom", # legend below plot
axis.text.x = element_text(angle = 45), # tilt x labels
plot.margin = unit(c(1, 20, 1, 1), "lines")) + # widen right margin
guides(shape = "none") + # hide shape legend
# axis labels and titles
labs(x = "when parliament spoke", # x label
y = "events mentioned in the past by number of mentions", # y label
subtitle = "In the Months Leading Up to August 1867") + # subtitle
ggtitle("Events Mentioned by Name") # main title
# load the scholar-created "events" dataset from hansardr
data("events")
# process the dataset to get a cleaned list of historical events
# each row represents a unique event with its assigned historical date
# ensures lowercase names for matching and excludes events after 1840
eventslist <- events %>% # create a new dataset
distinct(event_name, scholar_assigned_date) %>% # remove duplicates
filter(!scholar_assigned_date > 1840) %>% # exclude post-1840 events
mutate(event_name = tolower(event_name)) %>% # lowercase event names
select(event_name, scholar_assigned_date) # keep only name and date
head(eventslist)
# Load the Hansard speech text and associated metadata for the year 1830
data("hansard_1830")
data("debate_metadata_1830")
# Merge the speech data with its metadata using a left join
# All rows from hansard_1830 are retained, with metadata added where available
hansard_1830 <- hansard_1830 %>% # create a new dataset
left_join(debate_metadata_1830) # join debate text and metadata
# Tokenize the speech text into bigrams
# - "ngram" is the name of the new column to store the bigrams
# - "text" is the input column containing the speech text
# - token = "ngrams" with n = 2 instructs the function to extract bigrams
bigrams_1830 <- hansard_1830 %>% # create a new dataset
unnest_tokens(ngram, text, token = "ngrams", n = 2) # tokenize text column
# Identify bigrams that match known historical events from "eventslist"
# - Performs an inner join where the bigram matches the cleaned event name
# - Keeps only rows with a match, linking bigram use to historical context
events_1830 <- bigrams_1830 %>% # create a new dataset
inner_join(eventslist, by = c("ngram" = "event_name")) # join the bigrams and events
events_1830 %>%
head() %>%
mutate(ngram = str_trunc(ngram, 120)) %>%
kable("latex", booktabs = TRUE) %>%
kable_styling(full_width = FALSE,
position = "left",
latex_options = c("hold_position", "scale_down"),
font_size = 7) %>%
column_spec(3, width = "4cm")
# count how many times each event bigram is mentioned on each speech date
# extract year from date, group by relevant fields, and remove grouping after summarizing
counted_events <- events_1830 %>% # create a new dataset
mutate(year = year(speechdate)) %>% # extract year from speech date
group_by(ngram, year, speechdate, scholar_assigned_date) %>% # group data
summarize(n = n()) %>% # count mentions
ungroup() # remove grouping
head(counted_events)
# summarize total mentions of each event across all speech dates
# grouped by event name and assigned historical date
# then sort chronologically for historical analysis
cleaned_events <- counted_events %>%
group_by(ngram, scholar_assigned_date) %>% # group by event and historical date
summarize(total = sum(n)) %>% # sum total mentions
ungroup() %>% # remove grouping
arrange(scholar_assigned_date) # sort by historical date
head(cleaned_events)
# create an index to divide the historical date range into 30 equal chronological bins
# group by each bin and select the most frequently mentioned event within each
# then join back with the full event data to recover speech-level detail
indexed_events <- cleaned_events %>%
mutate(index = scholar_assigned_date %/%
((max(scholar_assigned_date) -
min(scholar_assigned_date)) / 30)) %>% # assign bin index
group_by(index) %>% # group by time bin
filter(total == max(total)) %>% # keep most mentioned event in each bin
ungroup() %>% # remove grouping
left_join(counted_events, by = c("ngram", "scholar_assigned_date")) # join data
head(indexed_events)
# Define a boundary value by adding 8 days to the latest speech date in the dataset
# Make the visualization easier to read by adding padding to the visualization
left_range <- max(events_1830$speechdate) + 8
# create a bubble plot showing when historical events were mentioned in parliament
# x-axis shows the date of mention; y-axis shows the historical date of the event
# bubble size and color represent frequency of mentions
ggplot(data = indexed_events, # provide the name of the dataset
aes(x = speechdate, # assign the x-axis
y = scholar_assigned_date, # assign the y-axis
size = n, # assign the size of the points
label = paste0("            ", ngram, " (", scholar_assigned_date, ")"),
color = n)) +  # assign the color
# use viridis color scale with log transform for better contrast
# set bubble size aesthetics for visual clarity
# use viridis option A (purple to yellow-green gradient)
# and use a forward gradient (low values = dark, high = bright)
scale_color_viridis(breaks = round, # round legend breaks
trans = "log", # log scale
option = "A", # viridis option
discrete = FALSE, # continuous scale
direction = 1) + # forward gradient
# set up point appearance, plot boundaries, and x-axis formatting
# this controls how bubbles are sized, shaped, placed, and how time is shown
scale_size_continuous(range = c(1, 10)) + # bubble size range
geom_point(alpha = .5, shape = 15) + # square bubbles
coord_cartesian(clip = "off") + # no clipping of labels
scale_x_date(date_breaks = "1 year", date_labels = "%Y") + # yearly ticks on x-axis
# add one label per event, placed at the right edge and aligned with its historical date
geom_text(data = indexed_events %>% # assign data
group_by(ngram) %>% # group data
sample_n(1), # pick one speech instance per event
show.legend = FALSE, # no text in legend
aes(color = 1, # fixed color for all labels
x = left_range, # position label at far right
y = scholar_assigned_date, # align vertically with event date
hjust = 0, # left-align label text
size = 8)) + # label size
# style plot appearance for readability and layout
theme(legend.position = "bottom", # move legend below plot
plot.margin = unit(c(1, 250, 50, 1), unit = "pt"), # add right margin for labels
axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 1), # rotate x-axis labels
axis.title.x = element_text(vjust = -0.5)) + # adjust x-axis title spacing
guides(shape = "none") + # remove shape legend
# add axis labels and title
labs(x = "when parliament spoke", # assign the x-axis
y = "events mentioned in the past") + # assign the y-axis
ggtitle("Events Mentioned by Name in Parliament") # create a title
# add a new column extracting the year from the speech date
hansard_1830 <- hansard_1830 %>%
mutate(year = year(speechdate))
# filter speeches from the year 1832 that mention the phrase "fifteenth century"
# - str_detect() searches for the exact phrase in the text
# - select() keeps only the "text" column of the matching speeches
c15_mentions <- hansard_1830 %>% # create a new dataset
filter(year > 1831 & year < 1833, # filter for the date range
str_detect(text, "fifteenth century")) %>% # detect the phrase
select(text) # select just the text column
c15_mentions %>%
head() %>%
mutate(text = str_wrap(text, 80)) %>%
select(text) %>%
kable("latex", booktabs = TRUE, longtable = TRUE, escape = FALSE) %>%
column_spec(1, width = "14cm")
# Filter speeches from the year 1832 that mention the phrase "fourteenth century"
# - Only rows with "fourteenth century" in the text are retained
# - Only the "text" column is selected
c14_mentions <- hansard_1830 %>% # create a new dataset
filter(year > 1831 & year < 1833, # filter for the date range
str_detect(text, "fourteenth century")) %>% # detect the phrase
select(text) # select just the text column
# display the filtered speech texts that mention "fourteenth century"
c14_mentions %>%
head() %>%
mutate(text = str_wrap(text, 80)) %>%
select(text) %>%
kable("latex", booktabs = TRUE, longtable = TRUE, escape = FALSE) %>%
column_spec(1, width = "14cm")
