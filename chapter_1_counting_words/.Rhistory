# Combine into one data frame
books <- bind_rows(
tibble(booktitle = "Alice", text = alice_text),
tibble(booktitle = "Peter", text = peter_text),
tibble(booktitle = "LookingGlass", text = looking_glass_text))
# Tokenise and count
word_freqs <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$")) %>%
count(booktitle, word) %>%
group_by(booktitle) %>%
mutate(freq = n / sum(n)) %>%
ungroup()
# Create a frequency matrix (book x word)
freq_matrix <- word_freqs %>%
select(booktitle, word, freq) %>%
pivot_wider(names_from = word, values_from = freq, values_fill = 0) %>%
column_to_rownames("booktitle") %>%
as.matrix()
# Compute JSD for each pair of books
book_names <- rownames(freq_matrix)
n_books <- nrow(freq_matrix)
jsd_matrix <- matrix(0, n_books, n_books)
rownames(jsd_matrix) <- book_names
colnames(jsd_matrix) <- book_names
for (i in 1:n_books) {
for (j in 1:n_books) {
jsd_matrix[i, j] <- jsd(freq_matrix[i, ], freq_matrix[j, ]) } }
# View results
kable(jsd_matrix)
# Load our needed R packages
library("hansardr")
library("tidyverse")
library("lubridate")
library("tidytext")
library("gt")
# Load tidytext's built-in list of stop words for filtering out common words
data("stop_words")
# Load the Hansard debates from the 1830s and 1860s
data("hansard_1830")
data("hansard_1860")
# Add a 'decade' column to hansard_1830 to tag all rows with the value 1830
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
# Add a 'decade' column to hansard_1860 to tag all rows with the value 1860
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
# Combine the two Hansard datasets by stacking their rows into one data frame
# 1830 will be stacked above 1860
hansard_data <- bind_rows(hansard_1830, hansard_1860)
# Inspect the data
hansard_data[, 2:3] %>%
sample_n(5) %>%
kable(format = "latex",
booktabs = TRUE,
caption = "Sample from Hansard Data") %>%
column_spec(1, width = "14cm")
# Code does not show, just output
tokenized_hansard_data <- suppressMessages(
hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[[:digit:]]")))
# tokenize the 'text' column into individual words,
# remove stop words, and filter out tokens that contain digits
tokenized_hansard_data <- hansard_data %>%
unnest_tokens(word, text) %>% # break text into one word per row
anti_join(stop_words) %>% # remove stop words like 'the' or 'and'
filter(!str_detect(word, "[:digit:]")) # remove words with digits
# count how often each word appears per decade,
# sort by frequency, and remove words that appear only once
words_per_decade <- tokenized_hansard_data %>%
count(decade, word, sort = TRUE, name = "word_count_per_decade") %>%  # count word frequency by decade
filter(word_count_per_decade > 1)  # keep words that appear more than once
# Inspect the data
words_per_decade %>%
rename(hansard_word = word, hansard_decade = decade) %>%
sample_n(8) %>%
arrange(desc(word_count_per_decade)) %>%
pivot_wider(names_from = hansard_word, values_from = word_count_per_decade,
values_fill = 0)
# calculate tf-idf (term frequency–inverse document frequency)
# for each word by decade using word frequency and total count
hansard_tf_idf <- words_per_decade %>%
bind_tf_idf(word, decade, word_count_per_decade)  # compute tf-idf
hansard_tf_idf %>%
sample_n(5) %>%
arrange(desc(tf_idf))
# keep top 15 tf-idf words per decade
top_hansard_tf_idf <- hansard_tf_idf %>%
group_by(decade) %>% # group by decade
slice_max(tf_idf, n = 15) %>% # keep top 15 words
arrange(decade, desc(tf_idf)) %>%
ungroup()
library("knitr")
library("kableExtra")
make_kable <- function(df, dec_label) {
df %>%
select(word, tf_idf) %>%
arrange(desc(tf_idf)) %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kbl(format = "latex",
booktabs  = TRUE,
col.names = c("Word", "TF--IDF Score"),
align = c("l", "r"),
caption = paste0("Top TF--IDF Words — ", dec_label),
escape = TRUE) %>%
kable_styling(full_width = FALSE) }
target_decades <- if_else(is.numeric(top_hansard_tf_idf$decade),
list(c(1830, 1860)),
list(c("1830s", "1860s"))) %>%
unlist()
top_hansard_tf_idf %>%
filter(decade %in% target_decades) %>%
group_by(decade) %>%
group_walk(~ print(make_kable(.x, unique(.x$decade))))
# chunk options
knitr::opts_chunk$set(
fig.width = 6,
fig.asp = 0.8,
out.width = "80%"
)
R.version.string
print("Hello World")
# load the hansardr package
library("hansardr")
# code does not show, just the table
library("knitr")
library("kableExtra")
library("tidyverse")
df <- data.frame(Label = c("hansard_YYYY", "debate_metadata_YYYY", "speaker_metadata_YYYY", "file_metadata_YYYY"),
Description = c("Hansard debate text",
"Metadata such as speech date and title.",
"Metadata on speakers.",
"Metadata for source file, column, and more."),
Key = rep("sentence_id", 4))
kable(df, caption = "Overview of the hansardr corpus, showing the primary text files and their associated metadata. Each data set is linked through a common sentence ID.")
# load the Hansard debate text for 1850
data("hansard_1850")
df_head <- hansard_1850 %>%
slice_head(n = 5)
kable(df_head,
format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Excerpt from the 1850 Hansard corpus showing
sentence-level text with associated sentence ID values.") %>%
kable_styling(latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# load the tidyverse and tidytext packages
library("tidyverse")
library("tidytext")
# define a custom function to add two numbers
add_numbers <- function(a, b) {
result <- a + b
return(result) }
# use the new function we created to add 2 and 3
print(add_numbers(2, 3))
# tokenize the 'text' column of the hansard_1850 dataset into individual words
# - 'unnest_tokens()' from the 'tidytext' package splits text into lowercase words
# - the new column will be called 'word'
# - each word becomes its own row
tokenized_hansard <- hansard_1850 %>% # create a new data frame
unnest_tokens(word, text) # tokenize the 'text' column
# show the first few rows of our tokenized data
head(tokenized_hansard)
# show the first fifteen rows of our tokenized data
head(tokenized_hansard, n=15)
View(hansard_1850)
View(hansard_1850)
# mutate() modifies variables in a dataframe, or creates new ones
# str_to_lower() transforms text to lowercase
# str_remove_all() removes all matches of a pattern (here, punctuation) from text
hansard_1850_clean <- hansard_1850 %>%
mutate(text = str_to_lower(text), # lowercase
text = str_remove_all(text, "[[:punct:]]")) # strip punctuation
print(hansard_1850_clean)
# define a function to generate a bar plot of top words that co-occur with a given keyword
# filters the dataset for rows containing the keyword as a whole word
# tokenizes the text, removes stop words and numeric tokens
# keeps the top 20 most frequent words and plots them as a horizontal bar chart
generate_word_plot <- function(keyword, hansard_data) {
filtered_hansard_1850 <- hansard_data %>%
filter(str_detect(text, regex(paste0("\\b", keyword, "\\b"),
ignore_case = TRUE)))# filter rows
unnest_tokens(word, text) %>% # tokenize text
anti_join(stop_words) %>% # remove stop words
filter(!str_detect(word, "[:digit:]")) %>% # remove numeric tokens
count(word) %>% # count word frequency
top_n(20) # keep top 20 words
plot <- ggplot(filtered_hansard_1850) + # create a new dataset for the plot
geom_col(aes(x = reorder(word, n), y = n), # bar chart ordered by count
fill = "steel blue") + # set bar color
coord_flip() + # horizontal bars
labs(title = paste("Top Words Occurring in Sentences Mentioning",
toupper(keyword)), # plot title
subtitle = "From the 1850 Hansard debates", # plot subtitle
x = "Word", # x-axis label
y = "Count") # y-axis label
print(plot) } # display the plot
# use our function to create a clean bar plot of the top 20 words in sentences
# mentioning the word "foreign"
generate_word_plot("foreign", hansard_1850)
# chunk options
knitr::opts_chunk$set(
fig.width = 6,
fig.asp = 0.8,
out.width = "80%"
)
R.version.string
print("Hello World")
# load the hansardr package
library("hansardr")
# code does not show, just the table
library("knitr")
library("kableExtra")
library("tidyverse")
df <- data.frame(Label = c("hansard_YYYY", "debate_metadata_YYYY", "speaker_metadata_YYYY", "file_metadata_YYYY"),
Description = c("Hansard debate text",
"Metadata such as speech date and title.",
"Metadata on speakers.",
"Metadata for source file, column, and more."),
Key = rep("sentence_id", 4))
kable(df, caption = "Overview of the hansardr corpus, showing the primary text files and their associated metadata. Each data set is linked through a common sentence ID.")
# load the Hansard debate text for 1850
data("hansard_1850")
df_head <- hansard_1850 %>%
slice_head(n = 5)
kable(df_head,
format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Excerpt from the 1850 Hansard corpus showing
sentence-level text with associated sentence ID values.") %>%
kable_styling(latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
# load the tidyverse and tidytext packages
library("tidyverse")
library("tidytext")
# tokenize the 'text' column of the hansard_1850 dataset into individual words
# - 'unnest_tokens()' from the 'tidytext' package splits text into lowercase words
# - the new column will be called 'word'
# - each word becomes its own row
tokenized_hansard <- hansard_1850 %>% # create a new data frame
unnest_tokens(word, text) # tokenize the 'text' column
# show the first few rows of our tokenized data
head(tokenized_hansard)
# show the first fifteen rows of our tokenized data
head(tokenized_hansard, n=15)
hansard_1850 %>%
head() %>%
mutate(text = str_wrap(text, 80)) %>% # wrap long text for readable table output
select(sentence_id, text) %>%
kable(format = "latex",
booktabs = TRUE,
longtable = TRUE,
escape = FALSE) %>%
column_spec(2, width = "14cm")
# mutate() modifies variables in a dataframe, or creates new ones
# str_to_lower() transforms text to lowercase
# str_remove_all() removes all matches of a pattern (here, punctuation) from text
hansard_1850_clean <- hansard_1850 %>%
mutate(text = str_to_lower(text), # lowercase
text = str_remove_all(text, "[[:punct:]]")) # strip punctuation
hansard_1850_clean %>%
head() %>%
mutate(text = str_wrap(text, 80)) %>% # wrap long text for readable table output
select(sentence_id, text) %>%
kable(format = "latex",
booktabs = TRUE,
longtable = TRUE,
escape = FALSE) %>%
column_spec(2, width = "14cm")
# count the frequency of each word in the tokenized dataset
# this creates a data frame with one row per unique word and a column 'n' for the count
tokenized_hansard_counts <- tokenized_hansard %>% # Create a new data frame
count(word) %>% # count how many times each word appears
arrange(desc(n)) # sort the results in descending order of frequency
head(tokenized_hansard_counts)
# load tidytext's dataset of common stop words
data("stop_words")
# remove common stop words from the tokenized word counts
stopworded_hansard_counts <- tokenized_hansard_counts %>% # create a new data frame
anti_join(stop_words) # exclude words that appear in the stop_words dataset
head(stopworded_hansard_counts)
# filter the word column for "india"
stopworded_hansard_counts %>%
filter(word == "india")
# filter the word column for "france"
stopworded_hansard_counts %>%
filter(word == "france")
# filter the word column for "jamaica"
stopworded_hansard_counts %>%
filter(word == "jamaica")
# create a list of words commonly used to identify or refer to women
# the purpose is to help filter or analyze texts for references to women
identifiers_for_woman <- c("girl", "woman", "girls", "women", "wife", "wives", "widow",
"widows", "sister", "sisters", "female", "females",
"grandmother","grandmothers", "aunt", "aunts")
# filter the word counts to keep only those words that identify or refer to women
# this checks if each word is in the 'identifiers_for_woman' list
stopworded_hansard_counts %>%
filter(word %in% identifiers_for_woman)
# filter the word counts to include only words that appear fewer than 474 times
stopworded_hansard_counts %>%
filter(n < 474) %>% # filter for words stated less than 474 times
slice_max(order_by = n, n = 20) # select the 20 most frequent words
stopworded_hansard_counts %>%
filter(n > 474 & n < 476) %>% # keep words that occur exactly 475 times
slice_max(order_by = n, n = 20) # select 20 words (all will have n = 475)
# check which words in 'identifiers_for_woman' contain "girl"
identifiers_for_woman %>%
str_detect("girl")
# filter speeches that mention the exact word "coal"
# \\b ensures "coal" is matched as a whole word, not as part of another word
# (e.g. "coalesce")
hansard_coal <- hansard_1850 %>% # create a new data frame
filter(str_detect(text, "\\bcoal\\b")) # filter the text
library(knitr)
library(kableExtra)
hansard_coal %>%
head() %>%
mutate(text = str_wrap(text, 80)) %>%
select(text) %>%
kable(format = "latex",
booktabs = TRUE,
longtable = TRUE,
escape = FALSE) %>%
column_spec(1, width = "14cm")
# tokenize the 'text' column into individual lowercase words
# remove common stop words (e.g., 'the', 'and') that carry little meaning
# filter out any tokens that contain digits (e.g., years, numbers)
# count the frequency of each remaining word
# arrange the word counts in descending order for easier interpretation
coal_context <- hansard_coal %>% # create a new data frame
unnest_tokens(word, text) %>% # tokenize text
anti_join(stop_words) %>% # remove stop words
filter(!str_detect(word, "[:digit:]")) %>% # remove tokens with digits
count(word) %>% # count tokens
arrange(desc(n)) # sort by frequency
head(coal_context)
# select the top 20 most frequent words from the 'coal_context' dataset
# this will be used to limit the plot to the most relevant terms
top_coal <- coal_context %>%
top_n(20)  # top 20 words
# create a horizontal bar chart of word frequencies
# this visualizes the most common terms in coal-related sentences
ggplot(data = top_coal) +
geom_col(aes(x = reorder(word, n), y = n), # bar chart with reordered x-axis
fill = "steel blue") + # set bar color
coord_flip() + # flip for horizontal layout
labs(title = "Top Words from Sentences Mentioning Coal", # add title
subtitle = "From the 1850 Hansard debates", # add subtitle
x = "Word", # x-axis label
y = "Count") # y-axis label
# filter the dataset to include only rows where the word "corn" appears as a whole word
# tokenize the filtered text into individual lowercase words
# remove stop words that carry little semantic weight
# exclude any tokens containing digits (e.g., "1850", "2nd")
# count the frequency of each remaining word
# sort the words in descending order of frequency
corn_context <- hansard_1850 %>% # create a new data frame
filter(str_detect(text, "\\bcorn\\b")) %>% # filter rows with 'corn' as a whole word
unnest_tokens(word, text) %>% # tokenize text
anti_join(stop_words) %>% # remove stop words
filter(!str_detect(word, "[:digit:]")) %>% # remove tokens with digits
count(word) %>% # count word frequency
arrange(desc(n)) # sort by frequency
head(corn_context)
# select the 20 most frequent words from the corn-related word counts
# this subset will be used to visualize the most relevant terms
top_corn <- corn_context %>%
top_n(20)  # top 20 words
# create a horizontal bar plot showing word frequencies
# this plot highlights the most common words in sentences mentioning "corn"
ggplot(data = top_corn) +
geom_col(aes(x = reorder(word, n), y = n), # draw bars sorted by count
fill = "steel blue") + # set bar color
coord_flip() + # flip axes for horizontal bars
labs(title = "Top Words Occurring in Sentences Mentioning Corn", # add title
subtitle = "From the 1850 Hansard debates", # add subtitle
x = "Word", # x-axis label
y = "Count") # y-axis label
# Load the debate metadata for 1850
data("debate_metadata_1850")
head(debate_metadata_1850)
# Merge the hansard_1850 text data with its corresponding metadata
# using a left join to keep all rows from hansard_1850
hansard_1850_with_metadata <- left_join(hansard_1850, debate_metadata_1850)
hansard_1850_with_metadata %>%
head() %>%
mutate(text = str_trunc(text, 120)) %>%   # optional: shorten long lines
kable(format = "latex",
booktabs = TRUE,
caption = "Excerpt from the 1850 Hansard corpus showing sentence-level
text joined with debate-level metadata, including speech dates
and debate titles.") %>%
kable_styling(full_width = FALSE, position = "left") %>%
column_spec(2, width = "6cm") %>%
column_spec(4, width = "4cm")
# Filter the dataset to include only speeches from February 1850
top_month <- hansard_1850_with_metadata %>% # create a new data frame
filter(speechdate >= "1850-02-01", # filter
speechdate <= "1850-02-28")
# load the quanteda package
library("quanteda")
# convert the filtered february 1850 dataset into a quanteda corpus object
# specify that the 'text' column contains the actual speech content
my_corpus <- corpus(top_month, text_field = "text")
# search for instances of the word "corn" in the corpus
# extract 5 words before and after each occurrence
# make the search case-insensitive so it matches "corn", "Corn", etc.
corn_kwic <- kwic(tokens(my_corpus), # tokenize corpus
"corn", # keyword to search
window = 5, # number of words before/after
case_insensitive = TRUE) # match regardless of case
corn_df <- as.data.frame(corn_kwic)
# Wrap long contexts so they don't overflow
corn_df <- corn_df %>%
mutate(pre = str_wrap(pre, width = 40),
keyword = str_wrap(keyword, width = 10),
post = str_wrap(post, width = 40)) %>%
select(-c(docname, from, to, pattern))
corn_df %>%
head(15) %>%
kable(format = "latex",
booktabs = TRUE,
align = "l") %>%
kable_styling(latex_options = c("hold_position", "scale_down"))
# define a custom function to add two numbers
add_numbers <- function(a, b) {
result <- a + b
return(result) }
# use the new function we created to add 2 and 3
print(add_numbers(2, 3))
# define a function to generate a bar plot of top words that co-occur with a given keyword
# filters the dataset for rows containing the keyword as a whole word
# tokenizes the text, removes stop words and numeric tokens
# keeps the top 20 most frequent words and plots them as a horizontal bar chart
generate_word_plot <- function(keyword, hansard_data) {
filtered_hansard_1850 <- hansard_data %>%
filter(str_detect(text, regex(paste0("\\b", keyword, "\\b"),
ignore_case = TRUE)))# filter rows
unnest_tokens(word, text) %>% # tokenize text
anti_join(stop_words) %>% # remove stop words
filter(!str_detect(word, "[:digit:]")) %>% # remove numeric tokens
count(word) %>% # count word frequency
top_n(20) # keep top 20 words
plot <- ggplot(filtered_hansard_1850) + # create a new dataset for the plot
geom_col(aes(x = reorder(word, n), y = n), # bar chart ordered by count
fill = "steel blue") + # set bar color
coord_flip() + # horizontal bars
labs(title = paste("Top Words Occurring in Sentences Mentioning",
toupper(keyword)), # plot title
subtitle = "From the 1850 Hansard debates", # plot subtitle
x = "Word", # x-axis label
y = "Count") # y-axis label
print(plot) } # display the plot
# use our function to create a clean bar plot of the top 20 words in sentences
# mentioning the word "foreign"
generate_word_plot("foreign", hansard_1850)
# use our function to create a clean bar plot of the top 20 words in sentences
# mentioning the word "foreign"
generate_word_plot("foreign", hansard_1850)
# define a function to generate a bar plot of top words that co-occur with a given keyword
# filters the dataset for rows containing the keyword as a whole word
# tokenizes the text, removes stop words and numeric tokens
# keeps the top 20 most frequent words and plots them as a horizontal bar chart
generate_word_plot <- function(keyword, hansard_data) {
filtered_hansard_1850 <- hansard_data %>%
filter(str_detect(text, regex(paste0("\\b", keyword, "\\b"),
ignore_case = TRUE))) %>% # filter rows
unnest_tokens(word, text) %>% # tokenize text
anti_join(stop_words) %>% # remove stop words
filter(!str_detect(word, "[:digit:]")) %>% # remove numeric tokens
count(word) %>% # count word frequency
top_n(20) # keep top 20 words
plot <- ggplot(filtered_hansard_1850) + # create a new dataset for the plot
geom_col(aes(x = reorder(word, n), y = n), # bar chart ordered by count
fill = "steel blue") + # set bar color
coord_flip() + # horizontal bars
labs(title = paste("Top Words Occurring in Sentences Mentioning",
toupper(keyword)), # plot title
subtitle = "From the 1850 Hansard debates", # plot subtitle
x = "Word", # x-axis label
y = "Count") # y-axis label
print(plot) } # display the plot
# use our function to create a clean bar plot of the top 20 words in sentences
# mentioning the word "foreign"
generate_word_plot("foreign", hansard_1850)
??unnest_tokens()
??library
??unnest_tokens()
df_head <- hansard_1850 %>%
slice_head(n = 5)
kable(df_head,
format = "latex",
booktabs = TRUE,
linesep = "",
caption = "Excerpt from the 1850 Hansard corpus showing
sentence-level text with associated sentence ID values.") %>%
kable_styling(latex_options = c("scale_down", "repeat_header"),
table.envir = FALSE) %>%
column_spec(which(colnames(df_head) == "text"),
latex_column_spec = "p{12cm}",
latex_valign = "top")
