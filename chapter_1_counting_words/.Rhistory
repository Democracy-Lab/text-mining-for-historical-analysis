# Compute tf-idf values
tfidf_table <- word_counts2 %>%
bind_tf_idf(term = term, document = document, n = n)
# View top distinctive words for each author
filtered_tfidf <- tfidf_table %>%
filter(term %in% target_words) %>%
arrange(document, desc(tf_idf)) %>%
select(term, tf_idf)
library(knitr)
make_tab <- function(df) {
df %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE, caption = NULL,
col.names = c("Word", "TF-IDF")) %>%
as.character() }
left_tab  <- make_tab(filtered_tfidf[1:6, ])
right_tab <- make_tab(filtered_tfidf[7:12, ])
two_pane_table <- paste0(
"\\noindent
\\begin{minipage}[t]{0.48\\textwidth}
\\raggedright\\textbf{Some Beatrix Potter Words}\\\\[3pt]",
left_tab, "
\\end{minipage}\\hfill
\\begin{minipage}[t]{0.48\\textwidth}
\\raggedright\\textbf{Some Lewis Carroll Words}\\\\[3pt]",
right_tab, "
\\end{minipage}")
asis_output(two_pane_table)
library(kableExtra)
distinctively_carroll <- tfidf_table %>%
filter(document == "Lewis Carroll") %>%
arrange(desc(tf_idf)) %>%
select(term, tf_idf)
kable(head(distinctively_carroll, 15),
format = "latex",
booktabs = TRUE,
caption = "\\textbf{\\large Lewis Carroll's Most Distinctive Words (when \\textit{Alice} is Compared with \\textit{Peter Rabbit})}",
escape = FALSE) %>%
kable_styling(full_width = TRUE)
library(text2vec)             # Load the package
# define a function for jsd
jsd <- function(p, q) {
# Ensure p and q are probability distributions
p <- p / sum(p)
q <- q / sum(q)
m <- 0.5 * (p + q)
# Define a helper function for KL divergence
kl_div <- function(a, b) {
a <- ifelse(a == 0, 1, a)  # Avoid log(0)
b <- ifelse(b == 0, 1, b)
sum(a * log2(a / b))
}
0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)
}
# load Alice in Wonderland
alice_url <- "https://www.gutenberg.org/files/11/11-0.txt"
alice_text <- readLines(alice_url, encoding = "UTF-8", warn = FALSE)
# Combine into one data frame
books <- bind_rows(
tibble(booktitle = "Alice", text = alice_text),
tibble(booktitle = "Peter", text = peter_text),
tibble(booktitle = "LookingGlass", text = looking_glass_text)
)
# Tokenise and count
word_freqs <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$")) %>%
count(booktitle, word) %>%
group_by(booktitle) %>%
mutate(freq = n / sum(n)) %>%
ungroup()
# Create a frequency matrix (book x word)
freq_matrix <- word_freqs %>%
select(booktitle, word, freq) %>%
pivot_wider(names_from = word, values_from = freq, values_fill = 0) %>%
column_to_rownames("booktitle") %>%
as.matrix()
# Compute JSD for each pair of books
book_names <- rownames(freq_matrix)
n_books <- nrow(freq_matrix)
jsd_matrix <- matrix(0, n_books, n_books)
rownames(jsd_matrix) <- book_names
colnames(jsd_matrix) <- book_names
for (i in 1:n_books) {
for (j in 1:n_books) {
jsd_matrix[i, j] <- jsd(freq_matrix[i, ], freq_matrix[j, ])
}
}
# View results
kable(jsd_matrix)
# Load our needed R packages
library(hansardr)
library(tidyverse)
library(lubridate)
library(tidytext)
library(gt)
# Load tidytext's built-in list of stop words for filtering out common words
data("stop_words")
# Load the Hansard debates from the 1830s and 1860s
data("hansard_1830")
data("hansard_1860")
# Add a 'decade' column to hansard_1830 to tag all rows with the value 1830
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
# Add a 'decade' column to hansard_1860 to tag all rows with the value 1860
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
# Combine the two Hansard datasets by stacking their rows into one data frame
# 1830 will be stacked above 1860
hansard_data <- bind_rows(hansard_1830, hansard_1860)
# Inspect the data
hansard_data[,2:3] %>%
sample_n(5) %>%
gt()
# Load our needed R packages
library(hansardr)
library(tidyverse)
library(lubridate)
library(tidytext)
library(gt)
# Load tidytext's built-in list of stop words for filtering out common words
data("stop_words")
# Load the Hansard debates from the 1830s and 1860s
data("hansard_1830")
data("hansard_1860")
# Add a 'decade' column to hansard_1830 to tag all rows with the value 1830
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
# Add a 'decade' column to hansard_1860 to tag all rows with the value 1860
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
# Combine the two Hansard datasets by stacking their rows into one data frame
# 1830 will be stacked above 1860
hansard_data <- bind_rows(hansard_1830, hansard_1860)
# Inspect the data
hansard_data[, 2:3] %>%
sample_n(5) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Sample from Hansard Data") %>%
kable_styling(latex_options = "hold_position")
# keep top 15 tf-idf words per decade
top_hansard_tf_idf <- hansard_tf_idf %>%
group_by(decade) %>% # group by decade
slice_max(tf_idf, n = 15) %>% # keep top 15 words
arrange(decade, desc(tf_idf)) %>%
ungroup()
knitr::opts_chunk$set(echo = TRUE)
# URLs for plain text versions
looking_glass_url <- "https://www.gutenberg.org/files/12/12-0.txt"
peter_url <- "https://www.gutenberg.org/cache/epub/14838/pg14838.txt"
# Extract "Through the Looking-Glass" and save it to RStudio's global environment
looking_glass_text <- readLines(looking_glass_url, encoding = "UTF-8", warn = FALSE)
# Extract "The Tale of Peter Rabbit" and save it to RStudio's global environment
peter_text <- readLines(peter_url, encoding = "UTF-8", warn = FALSE)
# Load required libraries
library(tidyverse)
library(tidytext)
# Convert to data frames
looking_glass_df <- tibble(writer = "Lewis Carroll", text = looking_glass_text)
peter_df <- tibble(writer = "Beatrix Potter", text = peter_text)
# Combine both texts
books <- bind_rows(looking_glass_df, peter_df)
# Tokenize text by spliting it into individual words
tokens <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$"))  # keep only alphabetic words
# Calculate word frequencies per author
word_counts <- tokens %>%
count(writer, word) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0)
# Focus on selected words of interest
target_words <- c("rabbit", "she", "little", "chortled", "shed", "the", "hoe", "slithy", "galumph")
selected <- word_counts %>%
select(writer, any_of(target_words))
# View the table
print(selected)
# Count word frequencies per author (long format)
word_counts2 <- tokens %>%
count(writer, word, sort = TRUE) %>%
rename(document = writer, term = word)
# Compute tf-idf values
tfidf_table <- word_counts2 %>%
bind_tf_idf(term = term, document = document, n = n)
# View top distinctive words for each author
filtered_tfidf <- tfidf_table %>%
filter(term %in% target_words) %>%
arrange(document, desc(tf_idf)) %>%
select(term, tf_idf)
library(knitr)
make_tab <- function(df) {
df %>%
mutate(tf_idf = round(as.numeric(tf_idf), 6)) %>%
kable(format = "latex", booktabs = TRUE, caption = NULL,
col.names = c("Word", "TF-IDF")) %>%
as.character() }
left_tab  <- make_tab(filtered_tfidf[1:6, ])
right_tab <- make_tab(filtered_tfidf[7:12, ])
two_pane_table <- paste0(
"\\noindent
\\begin{minipage}[t]{0.48\\textwidth}
\\raggedright\\textbf{Some Beatrix Potter Words}\\\\[3pt]",
left_tab, "
\\end{minipage}\\hfill
\\begin{minipage}[t]{0.48\\textwidth}
\\raggedright\\textbf{Some Lewis Carroll Words}\\\\[3pt]",
right_tab, "
\\end{minipage}")
asis_output(two_pane_table)
library(kableExtra)
distinctively_carroll <- tfidf_table %>%
filter(document == "Lewis Carroll") %>%
arrange(desc(tf_idf)) %>%
select(term, tf_idf)
kable(head(distinctively_carroll, 15),
format = "latex",
booktabs = TRUE,
caption = "\\textbf{\\large Lewis Carroll's Most Distinctive Words (when \\textit{Alice} is Compared with \\textit{Peter Rabbit})}",
escape = FALSE) %>%
kable_styling(full_width = TRUE)
library(text2vec)             # Load the package
# define a function for jsd
jsd <- function(p, q) {
# Ensure p and q are probability distributions
p <- p / sum(p)
q <- q / sum(q)
m <- 0.5 * (p + q)
# Define a helper function for KL divergence
kl_div <- function(a, b) {
a <- ifelse(a == 0, 1, a)  # Avoid log(0)
b <- ifelse(b == 0, 1, b)
sum(a * log2(a / b)) }
0.5 * kl_div(p, m) + 0.5 * kl_div(q, m) }
# load Alice in Wonderland
alice_url <- "https://www.gutenberg.org/files/11/11-0.txt"
alice_text <- readLines(alice_url, encoding = "UTF-8", warn = FALSE)
# Combine into one data frame
books <- bind_rows(
tibble(booktitle = "Alice", text = alice_text),
tibble(booktitle = "Peter", text = peter_text),
tibble(booktitle = "LookingGlass", text = looking_glass_text))
# Tokenise and count
word_freqs <- books %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "^[a-z']+$")) %>%
count(booktitle, word) %>%
group_by(booktitle) %>%
mutate(freq = n / sum(n)) %>%
ungroup()
# Create a frequency matrix (book x word)
freq_matrix <- word_freqs %>%
select(booktitle, word, freq) %>%
pivot_wider(names_from = word, values_from = freq, values_fill = 0) %>%
column_to_rownames("booktitle") %>%
as.matrix()
# Compute JSD for each pair of books
book_names <- rownames(freq_matrix)
n_books <- nrow(freq_matrix)
jsd_matrix <- matrix(0, n_books, n_books)
rownames(jsd_matrix) <- book_names
colnames(jsd_matrix) <- book_names
for (i in 1:n_books) {
for (j in 1:n_books) {
jsd_matrix[i, j] <- jsd(freq_matrix[i, ], freq_matrix[j, ]) } }
# View results
kable(jsd_matrix)
# Load our needed R packages
library(hansardr)
library(tidyverse)
library(lubridate)
library(tidytext)
library(gt)
# Load tidytext's built-in list of stop words for filtering out common words
data("stop_words")
# Load the Hansard debates from the 1830s and 1860s
data("hansard_1830")
data("hansard_1860")
# Add a 'decade' column to hansard_1830 to tag all rows with the value 1830
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
# Add a 'decade' column to hansard_1860 to tag all rows with the value 1860
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
# Combine the two Hansard datasets by stacking their rows into one data frame
# 1830 will be stacked above 1860
hansard_data <- bind_rows(hansard_1830, hansard_1860)
# Inspect the data
hansard_data[, 2:3] %>%
sample_n(5) %>%
kable(format = "latex", booktabs = TRUE,
caption = "Sample from Hansard Data") %>%
kable_styling(latex_options = "hold_position")
# Code does not show, just output
tokenized_hansard_data <- suppressMessages(
hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[[:digit:]]")))
# tokenize the 'text' column into individual words,
# remove stop words, and filter out tokens that contain digits
tokenized_hansard_data <- hansard_data %>%
unnest_tokens(word, text) %>% # break text into one word per row
anti_join(stop_words) %>% # remove stop words like 'the' or 'and'
filter(!str_detect(word, "[:digit:]")) # remove words with digits
# count how often each word appears per decade,
# sort by frequency, and remove words that appear only once
words_per_decade <- tokenized_hansard_data %>%
count(decade, word, sort = TRUE, name = "word_count_per_decade") %>%  # count word frequency by decade
filter(word_count_per_decade > 1)  # keep words that appear more than once
# Inspect the data
words_per_decade %>%
rename(hansard_word = word, hansard_decade = decade) %>%
sample_n(8) %>%
arrange(desc(word_count_per_decade)) %>%
pivot_wider(names_from = hansard_word, values_from = word_count_per_decade,
values_fill = 0)
# calculate tf-idf (term frequency–inverse document frequency) for each word by decade
# using word frequency and total count
hansard_tf_idf <- words_per_decade %>%
bind_tf_idf(word, decade, word_count_per_decade)  # compute tf-idf \
hansard_tf_idf %>%
sample_n(5) %>%
arrange(desc(tf_idf))
# keep top 15 tf-idf words per decade
top_hansard_tf_idf <- hansard_tf_idf %>%
group_by(decade) %>% # group by decade
slice_max(tf_idf, n = 15) %>% # keep top 15 words
arrange(decade, desc(tf_idf)) %>%
ungroup()
library(dplyr)
library(purrr)
library(knitr)
library(kableExtra)
library(glue)
library(stringr)
# 1) Helper: make a raw LaTeX tabular (no floating table env)
make_kable_tabular <- function(df) {
k <- df %>%
select(word, tf_idf) %>%
mutate(tf_idf = round(tf_idf, 6)) %>%
knitr::kable(
format = "latex",
booktabs = TRUE,
col.names = c("Word", "tf-idf Score"),
align = c("l", "r")
) %>%
kable_styling() %>%
as.character()
# Strip the outer \begin{table}...\end{table} so it's minipage-safe
k <- sub(".*?\\\\begin\\{tabular\\}", "\\\\begin{tabular}", k, perl = TRUE)
k <- sub("\\\\end\\{tabular\\}.*$", "\\\\end{tabular}", k, perl = TRUE)
k
}
# 2) Build a kable tabular per decade
tabs <- top_hansard_tf_idf %>%
group_by(decade) %>%
group_split() %>%
map(function(df) {
list(
decade = unique(df$decade),
tabular = make_kable_tabular(df)
)
})
# 3) Global heading (LaTeX-safe)
knitr::asis_output("\\section*{The Words Most Distinctive of the 1830s and 1860s}\nWhen the decades are compared with each other.\n\n")
# 4) Emit side-by-side in 2 columns using minipage
idx_pairs <- split(seq_along(tabs), ceiling(seq_along(tabs) / 2))
walk(idx_pairs, function(ix) {
left  <- tabs[[ix[1]]]
right <- if (length(ix) == 2) tabs[[ix[2]]] else NULL
left_block <- glue(
"\\begin{{minipage}}[t]{{0.48\\textwidth}}
\\textbf{{Top tf-idf Words — {left$decade}}}\\\\[0.5ex]
{left$tabular}
\\end{{minipage}}"
)
right_block <- if (!is.null(right)) glue(
"\\hfill
\\begin{{minipage}}[t]{{0.48\\textwidth}}
\\textbf{{Top tf-idf Words — {right$decade}}}\\\\[0.5ex]
{right$tabular}
\\end{{minipage}}"
) else ""
knitr::asis_output(paste0(left_block, "\n", right_block, "\n\n"))
})
# Load Hansard data for the 1840s and 1850s
data("hansard_1840")
data("hansard_1850")
# Add a 'decade' column to the 1840s dataset
hansard_1840 <- hansard_1840 %>%
mutate(decade = 1840)
# Add a 'decade' column to the 1850s dataset
hansard_1850 <- hansard_1850 %>%
mutate(decade = 1850)
# Append the 1840s and 1850s data to the existing hansard_data
hansard_data_1830_to_1870 <- hansard_data %>%
bind_rows(hansard_1840) %>% # Add 1840s data
bind_rows(hansard_1850) # Add 1850s data
# define the list of decades to analyze. each value will be used to
# filter the main dataset, enabling decade-by-decade processing.
decades <- c(1830, 1840, 1850, 1860)
# create an empty data frame to hold word frequency counts from each decade.
# this object will be built up by appending results from within the loop.
hansard_words_1830_to_1870 <- data.frame()
# iterate through each specified decade to perform tokenization,
# filtering, and word frequency counting. results are combined into a single table.
for(d in decades) {
# filter the full hansard dataset for just the current decade (d)
new_decade <- hansard_data_1830_to_1870 %>%
filter(decade == d)  # keep only rows from this decade
# tokenize the text column into individual words, remove stop words,
# and exclude any tokens that contain numeric digits (e.g., years)
tokenized_new_decade <- new_decade %>%
unnest_tokens(word, text) %>%  # convert text to one word per row
anti_join(stop_words) %>%      # remove stop words
filter(!str_detect(word, "[:digit:]"))  # remove words with digits
# count the frequency of each remaining word in the current decade
# then filter to retain only words appearing more than 20 times
new_decade_count <- tokenized_new_decade %>%
count(decade, word, sort = TRUE) %>%  # count word frequencies
filter(n > 20)  # keep words with frequency > 20
# append the word counts from this decade to the overall results table
hansard_words_1830_to_1870 <- bind_rows(hansard_words_1830_to_1870, new_decade_count)}  # combine rows
# Inspect the Data
hansard_words_1830_to_1870 %>% group_by(decade) %>%
sample_n(2) %>%
ungroup() %>%
arrange(desc(decade)) %>%
head()
# compute tf-idf values and keep top 15 words by tf-idf for each decade
most_dist_hansard_words <- hansard_words_1830_to_1870 %>% # start with merged word data
bind_tf_idf(word, decade, n) %>% # compute tf-idf for each word by decade
group_by(decade) %>% # group by decade
slice_max(tf_idf, n = 15) # keep top 15 tf-idf words per group
## Tables showing most distinctive words per decade
# Create individual tables per decade
gt_tables <- most_dist_hansard_words %>%
group_split(decade) %>%
map(make_gt_table)
library(dplyr)
library(purrr)
library(knitr)
library(kableExtra)
library(glue)
# ---- Helper: build a raw LaTeX tabular (no float wrapper) ----
make_kable_tabular <- function(df) {
k <- df %>%
select(word, tf_idf) %>%
mutate(tf_idf = round(tf_idf, 6)) %>%
knitr::kable(
format = "latex",
booktabs = TRUE,
col.names = c("Word", "tf-idf Score"),
align = c("l", "r")
) %>%
kable_styling() %>%
as.character()
# Strip surrounding \begin{table}...\end{table} so it can live in a minipage
k <- sub(".*?\\\\begin\\{tabular\\}", "\\\\begin{tabular}", k, perl = TRUE)
k <- sub("\\\\end\\{tabular\\}.*$", "\\\\end{tabular}", k, perl = TRUE)
k
}
# ---- 1) Build one kable tabular per decade ----
tabs <- most_dist_hansard_words %>%
group_split(decade) %>%
map(function(df) {
list(
decade  = unique(df$decade),
tabular = make_kable_tabular(df)
)
})
# ---- 2) Global title & subtitle (LaTeX-safe) ----
knitr::asis_output(
"\\section*{The Words Most Distinctive of the 1830s to the 1870s}
\\noindent\\textit{When the decades are compared with each other}\\\\[1ex]
"
)
# ---- 3) Emit tables in a 2-column LaTeX layout ----
idx_pairs <- split(seq_along(tabs), ceiling(seq_along(tabs) / 2))
walk(idx_pairs, function(ix) {
left  <- tabs[[ix[1]]]
right <- if (length(ix) == 2) tabs[[ix[2]]] else NULL
left_block <- glue(
"\\begin{{minipage}}[t]{{0.48\\textwidth}}
\\textbf{{Top tf-idf Words — {left$decade}}}\\\\[0.5ex]
{left$tabular}
\\end{{minipage}}"
)
right_block <- if (!is.null(right)) glue(
"\\hfill
\\begin{{minipage}}[t]{{0.48\\textwidth}}
\\textbf{{Top tf-idf Words — {right$decade}}}\\\\[0.5ex]
{right$tabular}
\\end{{minipage}}"
) else ""
knitr::asis_output(paste0(left_block, "\n", right_block, "\n\n"))
})
gc()
# chunk options
knitr::opts_chunk$set(
fig.width = 6,
fig.asp = 0.8,
out.width = "80%"
)
source("https://raw.githubusercontent.com/Democracy-Lab/hansardr/master/tools/install_hansardr.R")
# chunk options
knitr::opts_chunk$set(
fig.width = 6,
fig.asp = 0.8,
out.width = "80%"
)
# load the hansardr package
library("hansardr")
