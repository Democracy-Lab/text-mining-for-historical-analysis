knitr::opts_chunk$set(echo = TRUE)
# Filter for just nouns from this same data
nouns <- filter(parsed_hansard_woman, pos == "NOUN")
knitr::opts_chunk$set(echo = TRUE)
# After the first time, this will go quicker:
library(spacyr)
spacy_initialize(model = "en_core_web_sm") # load the English model
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
library(tidyverse)
library(httr)
library(jsonlite)
# Define the JSON API endpoint
url <- "https://www.dallasopendata.com/resource/7h2m-3um5.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
animal_shelter_data <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
# Show just the first five columns
head(animal_shelter_data[, 1:5])
json_list <- fromJSON(json_content)
subset_list <- json_list[1:2, ]
cat(prettify(toJSON(subset_list, auto_unbox = TRUE)))
nrow(json_list)
url <- "https://www.dallasopendata.com/resource/7h2m-3um5.json"
# Pagination settings
limit <- 1000
# Will store the output from each iteration
animal_shelter_data <- tibble()
# Run for two iterations: first with offset = 0, then with offset = 1000.
# This means the code will first collect rows 1–1000,
# and then collect rows 1001–2000 on the second iteration.
for (i in 0:1) {
offset <- i * limit
# Make paginated request
response <- GET(url, query = list(`$limit` = limit, `$offset` = offset))
if (status_code(response) != 200) {
stop("Request failed. Status code: ", status_code(response)) }
# Parse the response
data_subset <- fromJSON(content(response, "text"), flatten = TRUE)
# Save collected data
animal_shelter_data <- bind_rows(animal_shelter_data, as_tibble(data_subset)) }
library(usdoj)
blog_entries <- doj_blog_entries(n_results = 10, search_direction = "DESC")
head(blog_entries$teaser, 5)
library(tidyverse)
library(tidytext)
# Tokenize, remove stop words, and count word frequency
top_words <- blog_entries %>%
select(body) %>%
unnest_tokens(word, body) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 10)
# Plot the top words
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top 10 Words in DOJ Blog Entries",
x = "Word",
y = "Frequency") +
theme_minimal()
library(rvest)
library(tidyverse)
library(xml2)
rss_url <- "https://feeds.bbci.co.uk/news/rss.xml"
rss_page <- read_xml(rss_url)
articles <- tibble(
title = rss_page %>%
xml_find_all("//item/title") %>% xml_text(),
link = rss_page %>%
xml_find_all("//item/link") %>% xml_text())
print(head(articles, 5))
library(httr)
library(fs)
library(pdftools)
library(tidyverse)
# Define the URL with the PDF to be downloaded
url <- "https://sagadb.org/files/pdf/egils_saga.en.pdf"
# Define the directory it will be downloaded to on one's computer
# The file will be downloaded to folder named "tmha_data"
destfile <- path("tmha_data", "egils_saga.en.pdf")
# Create directory if it doesn't exist
dir_create(path_dir(destfile))
# Download the PDF file
GET(url, write_disk(destfile, overwrite = TRUE))
# Extract text from the PDF
pdf_text_raw <- pdf_text(destfile)
# Convert into a tibble with one row per page
tidy_egils_saga <- tibble(page = seq_along(pdf_text_raw),
text = pdf_text_raw)
head(tidy_egils_saga)
tidy_egils_saga %>%
slice(50:60)
View(tidy_egils_saga)
