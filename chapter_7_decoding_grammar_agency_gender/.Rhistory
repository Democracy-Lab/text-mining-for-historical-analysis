library(usdoj)
blog_entries <- doj_blog_entries(n_results = 10, search_direction = "DESC")
head(blog_entries$teaser, 5)
library(tidyverse)
library(tidytext)
# Tokenize, remove stop words, and count word frequency
top_words <- blog_entries %>%
select(body) %>%
unnest_tokens(word, body) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 10)
# Plot the top words
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top 10 Words in DOJ Blog Entries",
x = "Word",
y = "Frequency") +
theme_minimal()
library(rvest)
library(tidyverse)
library(xml2)
rss_url <- "https://feeds.bbci.co.uk/news/rss.xml"
rss_page <- read_xml(rss_url)
articles <- tibble(
title = rss_page %>%
xml_find_all("//item/title") %>% xml_text(),
link = rss_page %>%
xml_find_all("//item/link") %>% xml_text())
print(head(articles, 5))
library(httr)
library(fs)
library(pdftools)
library(tidyverse)
# Define the URL with the PDF to be downloaded
url <- "https://sagadb.org/files/pdf/egils_saga.en.pdf"
# Define the directory it will be downloaded to on one's computer
# The file will be downloaded to folder named "tmha_data"
destfile <- path("tmha_data", "egils_saga.en.pdf")
# Create directory if it doesn't exist
dir_create(path_dir(destfile))
# Download the PDF file
GET(url, write_disk(destfile, overwrite = TRUE))
# Extract text from the PDF
pdf_text_raw <- pdf_text(destfile)
# Convert into a tibble with one row per page
tidy_egils_saga <- tibble(page = seq_along(pdf_text_raw),
text = pdf_text_raw)
head(tidy_egils_saga)
tidy_egils_saga %>%
slice(50:60)
View(tidy_egils_saga)
knitr::opts_chunk$set(echo = TRUE)
# Install reticulate if not already installed
if (!requireNamespace("reticulate", quietly = TRUE)) {
install.packages("reticulate") }
library("reticulate")
# Install Miniconda (cross-platform)
install_miniconda()
# First time only -- installing spacyr on your computer
install.packages("spacyr")
library(spacyr)
spacy_install()
knitr::opts_chunk$set(echo = TRUE)
spacy_download_langmodel("de_core_news_sm") # load the German model
# After the first time, this will go quicker:
library(spacyr)
spacy_download_langmodel("de_core_news_sm") # load the German model
# Install reticulate if not already installed
if (!requireNamespace("reticulate", quietly = TRUE)) {
install.packages("reticulate") }
library("reticulate")
# Install Miniconda (cross-platform)
install_miniconda()
knitr::opts_chunk$set(echo = TRUE)
# Install reticulate if not already installed
if (!requireNamespace("reticulate", quietly = TRUE)) {
install.packages("reticulate") }
library("reticulate")
# Install Miniconda (cross-platform)
install_miniconda()
use_condaenv("r-spacy", required = TRUE)
# Install and load spacyr
if (!requireNamespace("spacyr", quietly = TRUE)) {
install.packages("spacyr") }
library(spacyr)
# Install spaCy and the English model
spacy_install()
library(spacyr)
spacy_install()
reticulate::install_python()
knitr::opts_chunk$set(echo = TRUE)
## ---- setup-spacyr-conda -------------------------------------------
# 1. Make sure spacyr is installed and loaded
if (!requireNamespace("spacyr", quietly = TRUE)) {
install.packages("spacyr")
}
library(spacyr)
# 2. Install spaCy + English model using a conda env managed by spacyr
#    - installs Miniconda if needed
#    - creates a conda env "r-spacy-conda"
#    - installs spaCy + en_core_web_sm in that env
spacy_install(
conda      = "auto",
envname    = "r-spacy-conda",
lang_models = "en_core_web_sm",
prompt     = FALSE
)
spacy_uninstall()
knitr::opts_chunk$set(echo = TRUE)
# After the first time, this will go quicker:
library(spacyr)
spacy_initialize(model = "en_core_web_sm") # load the English model
# Entire startup for future sessions
library(spacyr) # load the library
spacy_initialize(model = "en_core_web_sm") # load the English model
knitr::opts_chunk$set(echo = TRUE)
# After the first time, this will go quicker:
library(spacyr)
spacy_initialize(model = "en_core_web_sm") # load the English model
# Entire startup for future sessions
library(spacyr) # load the library
spacy_initialize(model = "en_core_web_sm") # load the English model
library(kableExtra)
#url <- "https://raw.githubusercontent.com/stephbuon/text-mining-for-historical-analysis/main/images/POS2.png"
#dest <- "POS2.png"
#if (!file.exists(dest)) {
#  download.file(url, dest, mode = "wb")
#}
#knitr::include_graphics(dest)
library(tidyverse)
library(hansardr)
data("hansard_1860")
hansard_woman_1860 <- hansard_1860 %>%
filter(str_detect(text, regex("woman|women", ignore_case = T)))
parsed_hansard_woman <- spacy_parse(hansard_woman_1860$text,
dep = TRUE,
lemma = FALSE,
entity = FALSE)
head(parsed_hansard_woman)
# Filter for just words tagged by spacyr as adjectives
adjectives <- filter(parsed_hansard_woman, pos == "ADJ")
# Count occurrences of each adjective and sort in descending order
top_adjectives <- adjectives %>%
count(token, sort = TRUE)
# View the top adjectives
head(top_adjectives, 30) %>%
kable()
library(tidytext)
data(stop_words)
top_adjectives_clean <- top_adjectives %>%
anti_join(stop_words, by = c("token" = "word"))
head(top_adjectives_clean, 30) %>% kable()
# Filter for just nouns from this same data
nouns <- filter(parsed_hansard_woman, pos == "NOUN")
# Exclude 'woman' and 'women' from the nouns
filtered_nouns <- nouns %>%
filter(!(token %in% c("woman", "women")))
# Count occurrences of each noun and sort in descending order
top_nouns <- filtered_nouns %>%
count(token, sort = TRUE)
# View the top nouns
head(top_nouns, 20)
hansard_man_1860 <- hansard_1860 %>%
filter(str_detect(text, regex("\\b(man|men)\\b", ignore_case = T)))
parsed_hansard_man <- spacy_parse(hansard_man_1860$text,
dep = TRUE,
lemma = FALSE,
entity = FALSE)
nouns <- filter(parsed_hansard_man, pos == "NOUN")
# Exclude 'man' and 'men' from the nouns
filtered_nouns <- nouns %>%
filter(!(token %in% c("man", "men")))
# Count occurrences of each noun and sort in descending order
top_nouns <- filtered_nouns %>%
count(token, sort = TRUE)
library(gridExtra)
# layout as a table
grid.arrange(tableGrob(top_nouns[1:15, ]), tableGrob(top_nouns[16:30, ]), tableGrob(top_nouns[31:45, ]), ncol = 3)
# the notation [1:15, ] tells R to extract the first fifteen rows and all tables from a dataset.  tableGrob() formats them as a table, and grid.arrange() places the tables side-by-side so that we can inspect more of them at once.
# Filter for adjectives and their associated nouns
# Extract adjective-noun pairs where the adjective modifies
# the noun within the same document
get_adjective_noun_pairs <- function(parsed_data, nouns) {
adjective_noun_pairs <- parsed_data %>%
# Process each document separately using a grouping variable
group_by(doc_id) %>%
# Filter for adjectives with the dependency relation
# "amod" (adjective modifying noun)
filter(pos == "ADJ" & dep_rel == "amod") %>%
# Select columns for adjective token, its token ID, and the head
# noun's token ID
select(doc_id, token_id, token, head_token_id) %>%
# Join to match adjectives to their corresponding nouns within
# the same document
inner_join(
parsed_data %>%
select(doc_id, token_id, token),
by = c("doc_id", "head_token_id" = "token_id"),
relationship = "many-to-many",
suffix = c("_adjective", "_noun")) %>%
# Filter for cases where the noun is specified in the 'nouns' argument
filter(token_noun %in% nouns) %>%
# Combine adjectives and nouns into a single string for readability
mutate(adjective_noun_pair = paste(token_adjective, token_noun)) %>%
# Ungroup after processing
ungroup() %>%
select(adjective_noun_pair)
return(adjective_noun_pairs)}
nouns_to_filter <- c("woman", "women")
adjective_noun_pairs_woman <- get_adjective_noun_pairs(parsed_hansard_woman,
nouns_to_filter)
adjective_noun_pair_counts_woman <- adjective_noun_pairs_woman %>%
count(adjective_noun_pair, name = "count") %>%
arrange(desc(count))
# print results as tables
grid.arrange(tableGrob(adjective_noun_pair_counts_woman[1:15, ]), tableGrob(adjective_noun_pair_counts_woman[16:30, ]), tableGrob(adjective_noun_pair_counts_woman[31:45, ]),ncol = 3)
# the notation [1:15, ] tells R to extract the first fifteen rows and all tables from a dataset.  tableGrob() formats them as a table, and grid.arrange() places the tables side-by-side so that we can inspect more of them at once.
woman_context <- hansard_woman_1860 %>%
filter(str_detect(text, regex("Chinese wom", ignore_case = TRUE))) %>%
select(text)
woman_context_show <- woman_context %>%
slice(1:5)
invisible(sapply(woman_context_show$text, function(x) {
cat(paste(strwrap(x, width = 80), collapse = "\n"))
cat("\n\n") })) # separator between entries
nouns_to_filter <- c("man", "men")
adjective_noun_pairs_man <- get_adjective_noun_pairs(parsed_hansard_man,
nouns_to_filter)
adjective_noun_pair_counts_man <- adjective_noun_pairs_man %>%
count(adjective_noun_pair, name = "count") %>%
arrange(desc(count))
grid.arrange(tableGrob(adjective_noun_pair_counts_man[1:15, ]),
tableGrob(adjective_noun_pair_counts_man[15:30, ]),
tableGrob(adjective_noun_pair_counts_man[31:45, ]),
ncol = 3)
library(forcats)
# Vector subtraction: words applied to men vs words applied to women
# extract just the adjectives used to describe men
man_adjectives <- adjective_noun_pair_counts_man %>%
separate(adjective_noun_pair, into = c("adjective", "noun"), sep = " ") %>% # split into two columns
count(adjective, name = "count")
# extract just the adjectives used to describe women
woman_adjectives <- adjective_noun_pair_counts_woman %>%
separate(adjective_noun_pair, into = c("adjective", "noun"), sep = " ") %>% # split into two columns
count(adjective, name = "count")
# join the two into one database and subtract the count of men from the count of women.
comparison <- full_join(man_adjectives, woman_adjectives, by = "adjective", suffix = c("_1", "_2")) %>% #join
mutate( # add a new column
count_1 = replace_na(count_1, 0),
count_2 = replace_na(count_2, 0),
count_diff = count_2 - count_1) %>% # subtract men's count from women's
filter(count_diff!=0) # delete all entries where there is no difference
# In this dataset, there are a large number of adjectives with a count of two. We will therefore "sample" from the results to draw our visualization. This means that every time the visualization below is redrawn, different results will appear.
# take the top adjectives unique to women
positive_sample <- comparison %>%
filter(count_diff > 0) %>% # must show a difference
arrange(desc(count_diff)) %>%
top_n(20) %>%
mutate(Category = "Women")
# take the top adjectives unique to men
# because there are so many with a max of 2, sample some positives
negative_sample <- comparison %>%
filter(count_diff < 0) %>% # must show a difference
sample_n(20) %>% # random sample
mutate(Category = "Men")
filtered_df <- negative_sample %>%
arrange(desc(count_diff)) %>%
bind_rows(positive_sample) %>%
mutate(adjective = fct_reorder(adjective, count_diff))
# Plot
filtered_df %>%
ggplot(aes(x = adjective, y = count_diff, fill = Category)) +
geom_col() +
coord_flip() +
scale_fill_manual(values = c("Women" = "red", "Men" = "blue"),
name = "Phrases used more about") +
theme_minimal() +
labs(
title = "Difference in Adjective-Noun Pair Counts",
x = NULL,
y = "Count Difference",
fill = "Category"
) +
geom_hline(yintercept = 0, color = "black")
#library(gt)
#library(htmltools)
# take the top adjectives unique to men; because there are so many with a max of 2, sample some positives
#positive_sample <- comparison %>%
#  filter(count_diff > 0) %>%
#  arrange(desc(count_diff)) %>%
#  top_n(20) %>%
#  mutate(Category = "Women", Color = "blue")
# take the top adjectives unique to women
#negative_sample <- comparison %>%
#  filter(count_diff < 0) %>%
#  top_n(-20) %>%
#  mutate(Category = "Men", Color = "red")
# combine men and women
#filtered_df <- negative_sample %>%
#  bind_rows(positive_sample) %>%
#  mutate(adjective = fct_reorder(adjective, count_diff)) %>%
#  arrange(desc(count_diff)) %>%
#  select(adjective, count_diff, Category, Color) %>%
#  arrange(desc(abs(count_diff))) %>%
#  mutate(adjective = fct_reorder(adjective, count_diff))
# Function to generate a single gt table
#make_gt_table <- function(data_slice) {
#  data_slice %>%
#    gt() %>%
#    cols_label(
#      adjective = "Adjective",
#      count_diff = "Count Difference",
#      Category = "Used More About"
#    ) %>%
#    text_transform(
#      locations = cells_body(vars(Category)),
#      fn = function(x) {
#        color_map <- ifelse(x == "Women", "blue", "red")
#        mapply(function(txt, col) {
#          HTML(
#            paste0("<span style='color:", col, "'>", txt, "</span>")) }, x, color_map) })}
# Create three slices
#table1 <- make_gt_table(filtered_df %>%
#                          slice(1:200))
#table2 <- make_gt_table(filtered_df %>%
#                          slice(101:200))
#table3 <- make_gt_table(filtered_df %>%
#                          slice(201:300))
#browsable(tagList(tags$div(style = "display: flex; gap: 20px;",
#             tags$div(style = "flex: 1;", table1),
#             tags$div(style = "flex: 1;", table2),
#             tags$div(style = "flex: 1;", table3))))
library(tidyverse)
library(forcats)
library(knitr)
library(kableExtra)
# ---- Build comparison subset ----
# top adjectives used more about women (count_diff > 0)
positive_sample <- comparison %>%
filter(count_diff > 0) %>%
slice_max(order_by = count_diff, n = 20) %>%
mutate(Category = "Women")
# top adjectives used more about men (count_diff < 0)
negative_sample <- comparison %>%
filter(count_diff < 0) %>%
slice_min(order_by = count_diff, n = 20) %>%
mutate(Category = "Men")
# combine, with Women first and all Women rows together
filtered_df <- bind_rows(positive_sample, negative_sample) %>%
mutate(
Category = factor(Category, levels = c("Women", "Men")),
adjective = fct_reorder(adjective, count_diff)
) %>%
arrange(Category, desc(abs(count_diff))) %>%
select(adjective, count_diff, Category)
# ---- Function to generate a single kable table ----
make_kable_table <- function(data_slice) {
data_slice %>%
mutate(
UsedMoreAbout = if_else(
Category == "Women",
cell_spec(Category, format = "latex", color = "blue"),
cell_spec(Category, format = "latex", color = "red")
)
) %>%
select(
Adjective = adjective,
`Count Difference` = count_diff,
`Used More About` = UsedMoreAbout    # Category not shown, just used for color
) %>%
kable(
format = "latex",
booktabs = TRUE,
escape = FALSE
) %>%
kable_styling(full_width = FALSE, position = "center")
}
# ---- Create slices (adjust ranges as needed) ----
# If you have fewer rows, make these smaller (e.g., 1:20, 21:40, etc.)
table1 <- make_kable_table(filtered_df %>% slice(1:200))
table2 <- make_kable_table(filtered_df %>% slice(201:400))
table3 <- make_kable_table(filtered_df %>% slice(401:600))
# In your Rmd, print them in separate chunks, e.g.:
# ```{r}
# table1
# ```
# ```{r}
# table2
# ```
# ```{r}
# table3
# ```
library(tidyverse)
library(hansardr)
data("hansard_1870")
hansard_woman_1870 <- hansard_1870 %>%
filter(str_detect(text, regex("woman|women", ignore_case = T)))
parsed_hansard_woman_1870 <- spacy_parse(hansard_woman_1870$text,
dep = TRUE,
lemma = TRUE,
entity = FALSE)
head(parsed_hansard_woman_1870)
get_simple_lemmatized_svo_triples <- function(parsed_data){
svo_triples <- parsed_data %>%
group_by(doc_id, sentence_id) %>%
# Identify ROOT tokens (usually verbs) by doc and sentence
filter(dep_rel == "ROOT") %>%
select(doc_id,
sentence_id,
root_token_id = token_id,
verb = lemma) %>%
ungroup() %>%
# Join with subject tokens
left_join(
parsed_data %>%
filter(dep_rel %in% c("nsubj", "nsubjpass")) %>%
select(doc_id,
sentence_id,
head_token_id,
subject = lemma),
by = c("doc_id", "sentence_id", "root_token_id" = "head_token_id"),
relationship = "many-to-many") %>%
# Join with object tokens
left_join(
parsed_data %>%
filter(dep_rel %in% c("dobj", "obj", "pobj", "iobj")) %>%
select(doc_id,
sentence_id,
head_token_id,
object = lemma),
by = c("doc_id", "sentence_id", "root_token_id" = "head_token_id"),
relationship = "many-to-many") %>%
# Combine SVO into a single string for readability, skipping NAs
mutate(
svo_triple = paste(
if_else(is.na(subject), "", subject),
if_else(is.na(verb), "", verb),
if_else(is.na(object), "", object),
sep = " "),
# Remove excess white space
svo_triple = str_squish(svo_triple)) %>%
filter(!is.na(subject),
!is.na(verb),
!is.na(object)) %>%
ungroup() %>%
select(doc_id, sentence_id, subject, verb, object, svo_triple)
return(svo_triples) }
simple_svo_triples_1870 <- get_simple_lemmatized_svo_triples(parsed_hansard_woman_1870)
head(simple_svo_triples_1870)
# Find the Most Common Simple SVO Triples
svo_counts_1870 <- simple_svo_triples_1870 %>%
count(svo_triple, sort = TRUE) %>%
slice_head(n = 20)
ggplot(svo_counts_1870,
aes(x = reorder(svo_triple, n),
y = n)) +
geom_col(fill = "steelblue") +
# rotate horizontally for readability
coord_flip() +
labs(x = "SVO triple",
y = "Count",
title = "Top 20 Most Frequent SVO Triples") +
theme_minimal()
svo_triples_context_1870 <- hansard_woman_1870 %>%
filter(str_detect(text,
regex("right|vote",
ignore_case = TRUE))) %>%
select(text)
svo_out1 <- head(svo_triples_context_1870, 5)
gt(svo_out1)
svo_out1 <- head(svo_triples_context_1870, 5)
kable(svo_out1)
# Finding Triples Where Women are the Subject
triples_with_woman_as_subject_1870 <- simple_svo_triples_1870 %>%
filter(str_detect(svo_triple, "^woman"))
top_10_triples_with_woman_as_subject_1870 <- triples_with_woman_as_subject_1870 %>%
count(svo_triple) %>%
arrange(desc(n)) %>%
slice(1:10)
head(top_10_triples_with_woman_as_subject_1870, 10)
# Finding Triples Where Women are the Object
triples_with_woman_as_subject_1870 <- simple_svo_triples_1870 %>%
filter(str_detect(svo_triple, " woman"))
top_10_triples_with_woman_as_object_1870 <- triples_with_woman_as_subject_1870 %>%
count(svo_triple) %>%
arrange(desc(n)) %>%
slice(1:10)
head(top_10_triples_with_woman_as_object_1870, 10)
library(quanteda)
hansard_woman_1870_corpus <- corpus(hansard_woman_1870,
text_field = "text")
vote_kwic <- kwic(tokens(hansard_woman_1870_corpus),
pattern = "vote",
window = 8,
case_insensitive = TRUE)
kwic_df <- as.data.frame(vote_kwic)
gt(kwic_df[ ,4:6])
kwic_df <- as.data.frame(vote_kwic)
kable(kwic_df[ ,4:6])
complex_sentence <- "As to the mode of dealing with the question practically
I have no doubt that the measure which will be brought before Parliament by Her
Majesty's Government will be just to both landlord and tenant, and Her Majesty's
Government ought not to be deterred from bringing in such a measure, although it
may not meet the views of unreasonable individuals."
parsed_complex_sentence <- spacy_parse(complex_sentence,
dep = TRUE,
lemma = FALSE,
entity = FALSE)
parsed_complex_sentence %>%
filter(token == "views")
