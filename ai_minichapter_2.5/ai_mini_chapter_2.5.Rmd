---
title: "AI Mini Chapter: Iteration as Method in Exploratory Data Analysis"
output: pdf_document
header-includes:
  - \usepackage{tcolorbox}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Iteration is a core method in digital humanities (DH) research, positioning inquiry as a cyclical process in which writing code, inspecting data, and interpreting visualizations are repeatedly undergone in response to an analyst's emerging insights. With the introduction of generative AI-based tools, iteration in DH extends beyond the cyclical processes of writing code and inspecting visualizations. Large language models (LLMs) can be used to generate content--such as code or suggestions for brainstorming--requiring a new approach to iteration to incorporate critical reflection on prompt refinement, model behavior, and evaluation of AI-generated outputs.

In this section, iteration is introduced as a method in computational humanities research (Tabak 2017). This approach emphasizes using AI to generate content while performing repeated returns to corpus context, showing how computational analysis and close reading work together to inform and reshape interpretation. Iteration will be paired with Exploratory Data Analysis (EDA), a process through which we construct a high-level overview of the corpus, engage in iterative refinement, and draw upon preliminary findings to inform more focused, in-depth investigations. 

Up to this point we have written our own code without use of generative AI-based tools like a LLM. Working through the process of writing one's own code step-by-step provides knowledge of the underlying concepts and processes behind working with text as data. Understanding the premise behind code syntax, functions, and how they tie back to data processing enables us to translate research questions into code and actualize our analytic goals. With an established foundation knowledge of data processing we can begin to use generative AI-based tools, like AI chatbots, in a productive and critical way. In this "AI minichapter" we use ChatGPT, though a similar approach could be applied to other AI-based chatbots, including open-source models such as Meta's LLaMA.

# Iteration and Exploratory Data Analysis with AI 

In practice, exploratory data analysis often begins with diagnostic step: generating simple summaries that reveal dominant patterns, repetitions, and sources of noise in the corpus before any interpretive claims are made. Here, counting words serves a different purpose than in interpretive text analysis. Rather than being used to support historical interpretation, we will think of word counts as functioning diagnostically, helping us understand the attributes of the dataset and informing later decisions about preprocessing and analysis.

While we have already counted words, here it serves a different purpose. As an initial step in exploratory data analysis, we ask a deliberately simple diagnostic question: Which words appear most frequently in the 1850 Hansard debates? By applying minimal preprocessing, we aim to observe the corpus in an unfiltered state and identify dominant structural patterns rather than interpretive themes.

```{r}
library(tidyverse)
library(tidytext)
library(hansardr)
library(kableExtra)

data(hansard_1850)

tokenized_hansard_1850 <- hansard_1850 %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20)

tokenized_hansard_1850
```

At this exploratory stage, a raw word count is not intended to produce interpretation. Instead, it exposes a central characteristic of the corpus: the abundance of articles and function words. This diagnostic result signals the need for preprocessing steps—such as stop-word removal—to clarify patterns relevant to historical inquiry.

```{r}
data(stop_words)

tokenized_hansard_1850 <- hansard_1850 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20)

tokenized_hansard_1850
```

We can also use AI chatbots in this process by facilitating brainstorming and supporting iterative analysis through the automation of tasks. For instance, while we already removed stop words (e.g., "the" or "and") from our dataset, which significantly improved the clarity of our results. We can also use ChatGPT to generate additional stop words, specifically attuned to the linguistic characteristics of 19th-century Hansard. 

At this exploratory stage, patterns in the data raise new questions rather than provide answers. The prominence of procedural and honorific language suggests that additional preprocessing may be necessary to clarify topical signals. Rather than relying on a fixed or generic stop-word list, we can treat this decision itself as part of the exploratory process.

## Exploratory Exercise: Proposing Corpus-Specific Stop Words

Here we use ChatGPT for brainstorming during exploratory data analysis, not to produce authoritative responses, but to suggest stop words that can then be evaluated, refined, or rejected through our knowledge of the corpus and by reflecting on our research direction. 

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Analyst Prompt]
Generate a list of stopwords to be removed for an analysis of the 19th-century Hansard corpus.
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=ChatGPT-4o Response]
\begin{verbatim}
custom_stopwords <- c("lord", "lords", "hon", "mr", "sir", "gentleman", "gentlemen",
                      "speaker", "right", "honourable", "house", "parliament", 
                      "minister", "bill", "member", "question", "motion", "committee", 
                      "government", "debate", "order", "division", "clause")
\end{verbatim}
\end{tcolorbox}

Before removing the stop words, we can think critically about whether or not the generated results align with our own analytic goals. For example, the word "government" appears frequently in parliamentary debate and may function as procedural language in some contexts that can be safely removed for analysis. However, if the research question involves examining how political authority is framed, contested, or attributed, retaining this word becomes necessary in order to analyze its collocates and surrounding rhetorical patterns.

This reflective step reinforces an important principle of exploratory data analysis: preprocessing decisions are provisional and question-dependent rather than fixed or universal. Words that operate as noise in one analysis may serve as signal in another. AI-generated stop-word lists should therefore be treated as hypotheses to be tested through inspection, rather than rules to be applied automatically.

In practice, this means iteratively revisiting preprocessing choices like stop words removal as analytic focus sharpens. A researcher may begin by removing a broad set of procedural terms, then selectively reintroduce specific words—such as government—to examine their contextual usage, collocates, or shifts in meaning across debates. This back-and-forth movement between diagnostic exploration and focused analysis exemplifies how exploratory data analysis supports interpretive work in digital history.

## Exploratory Exercise: From Overview to Focus

Building on the exploratory data analysis practices introduced above, this exercise demonstrates another way AI chatbots can support analytical workflows during the early stages of research by suggesting topics to further explore. As with diagnostic word counts, the goal here is not interpretation but orientation and exploration of the data. We use AI as a brainstorming tool to help scope potential areas of inquiry by proposing candidate words for further investigation. These suggestions serve as provisional starting points that guide subsequent analysis, rather than as definitive claims about historical significance.

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=User Prompt]
I want to text mine and analyze heated topics from within the 1850 Hansard Parliamentary debates. Suggest several keywords that might be interesting.
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=ChatGPT-4o Response]
When analyzing the 1850 Hansard Parliamentary debates for heated topics, you'll want to focus on issues that were contentious or widely discussed at the time. Here are some keyword suggestions:

\textbf{Corn Laws} – Although the Corn Laws were repealed in 1846, the economic and social impacts continued to be a hot topic of debate.

\textbf{Irish Famine} – The Great Irish Famine (1845–1852) was a significant issue, particularly regarding the British government's response.

\textbf{Chartism} – Though the Chartist movement had declined by 1850, discussions about labor rights, suffrage, and political reform were still prevalent.

\textbf{Poor Law} – The administration and reform of the Poor Laws were highly debated, particularly in relation to poverty and public welfare.

\textbf{Colonial Governance} – Issues around the governance and management of British colonies, particularly in places like India and Ireland.

\textbf{Free Trade} – Discussions around free trade policies, especially in the aftermath of the Corn Laws repeal.

\textbf{Slavery} – Although the British Empire abolished slavery in 1833, discussions about slavery and the slave trade (particularly in relation to other nations) continued.

\textbf{Catholic Emancipation} – Ongoing debates about the rights of Catholics in Britain and Ireland.

\textbf{Industrialization} – Issues related to the rapid industrialization of Britain, including worker rights, factory conditions, and urbanization.

\textbf{Public Health} – Particularly in the context of the cholera outbreaks and the general health of the urban poor.
 
These keywords should help you identify debates that were particularly heated or significant during that time. You can further refine your analysis by focusing on specific phrases or related terms within these broader topics.
\end{tcolorbox}

This response offers a high-level overview of potentially interesting topics within the corpus. Treating these themes as candidate keywords allows us to move from exploratory scoping to more focused analysis. To demonstrate this transition, we use the code introduced earlier in an earlier chapter to examine the lexical context surrounding the term Irish in the 1850 Hansard debates.

```{r, warning=FALSE, message=FALSE}
irish_context <- hansard_1850 %>% 
  filter(str_detect(text, "\\bIrish\\b")) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, "[:digit:]")) %>%
  count(word) %>%
  arrange(desc(n))
```

```{r, warning=FALSE, message=FALSE}
top_irish <- irish_context  %>%
  top_n(20)

ggplot(data = top_irish) +
  geom_col(aes(x = reorder(word, n), 
               y = n),
           fill = "steel blue") +
  coord_flip() +
  labs(title = "Top Words Occuring in Sentences Mentioning Irish",
       subtitle = "From the 1850 Hansard debates",
       x = "Word",
       y = "Count") 
```

[Needs more analysis -- do students have feedback?]

## Exploratory Exercise: Brainstorming Historical Events

We can use AI chatbots to assist with identifying additional historical events for analysis by working within a controlled vocabulary. Rather than generating open-ended interpretations, the chatbot is guided to propose candidate events that align with previously defined analytic criteria. This approach supports iterative exploration while ensuring that the scope of inquiry remains grounded in the analyst's existing interests. 

Here, we prompt the chatbot to suggest events that may be relevant to the study of war-related language in the year 1840. To constrain and focus the response, we provide examples drawn from the an eventslist dataset, which functions as a reference vocabulary for the types of events of interest.

```{r}
data(hansard_1840)
data("events")

eventslist <- events %>% 
distinct(event_name, scholar_assigned_date) %>% 
filter(!scholar_assigned_date > 1840) %>% 
mutate(event_name = tolower(event_name)) %>%
select(event_name, scholar_assigned_date) 

head(eventslist)
```

To structure working with ChatGPT, we provide it with a template specifying the required output format. Providing this constraint in the prompt helps standardize the output, making it easier to inspect. 

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Analyst Prompt] I am providing you with a list of events followed by their year. The purpose of this list is to analyze key events that were mentioned in the 1860 Hansard debates. Name some different events that might be relevant based on the list I provide you. Don't mention the same as provided by the list. Also provide a reason for why the events are relevant. 

Return your answer in this format:

\textbf{Event:} \\
\textbf{Reason:}

These are the events from the list: (copy/paste contents of eventslist) \end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=ChatGPT-4o Response]

\textbf{Event: Abolition of the Slave Trade (1807)} \\
\textbf{Reason:} The abolition of the slave trade would be highly relevant in discussions about moral and economic reforms during the 1860 Hansard debates, particularly in the context of human rights and British colonial policies.

\textbf{Event: Reform Act (1832)} \\ 
\textbf{Reason:} This act, which expanded suffrage and restructured parliamentary representation, would likely influence discussions on electoral reform and representation in 1860.

\textbf{Event: Crimean War (1853–1856)} \\ 
\textbf{Reason:} The Crimean War had significant impacts on British foreign policy, military reforms, and public health improvements, all of which could be relevant topics of debate in 1860.

\textbf{Event: Irish Potato Famine (1845–1852)} \\ 
\textbf{Reason:} The famine’s aftermath would remain a pressing issue, influencing debates about Irish governance, economic policies, and relief efforts.

\textbf{Event: Great Exhibition (1851)} \\ 
\textbf{Reason:} As a symbol of industrial progress and imperial power, the Great Exhibition could be discussed in terms of economic innovation and Britain's global role in the mid-19th century.

\textbf{Event: Indian Rebellion (1857)} \\
\textbf{Reason:} The rebellion was a turning point in British imperial governance, leading to significant debates about military strategy, colonial administration, and the justification of imperial rule.

\textbf{Event: Public Health Act (1848)} \\ 
\textbf{Reason:} This act, which addressed urban sanitation and public health, would be relevant to discussions about improving living conditions and public health policies in 1860.

\textbf{Event: Establishment of the Penny Post (1840)} \\ 
\textbf{Reason:} The reform in postal services, increasing communication efficiency and accessibility, could relate to debates on economic and infrastructural development.

\end{tcolorbox}

Our initial prompt to the AI chatbot successfully guided it to identify and return events similar to the query. However, when we provided the chatbot with the entire contents of eventslist, the results lacked precision and specificity. This outcome suggests that more targeted guidance is necessary to focus on the specific characteristics of interest. To tailor the results to a specific research question, we will refine the prompt further by narrowing in on the events we are most interested in from `eventslist` and instructing the chatbot to return like-events, in this case, other names of meetings that occurred in the 1860s. 

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Analyst Prompt]
I am providing you with a list of events followed by their year. The purpose of this list is to analyze key events that were mentioned in the 1860 Hansard debates. Name some different events that might be relevant based on the list I provide you. Don't mention the same as provided by the list. Also provide a reason for why the events are relevant based on the list I provide. 

Return your answer like this:

\textbf{Event:} \\
\textbf{Date:} \\
\textbf{Reason:}

These are the events from the list: "greek war", "napoleonic wars."
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=ChatGPT-4o Response]
Here are some different events that might be relevant based on your list:

\textbf{Event: Chartist Movements} \\
\textbf{AI Assigned Date:} 1838–1850\\
\textbf{Reason:} The Chartist movements, with their organized public gatherings for political reform, directly relate to the mentioned events like "public meeting" and "recent meeting," illustrating how assemblies have historically been used to drive societal change.

\textbf{Event: Anti-Corn Law League Meetings}\\
\textbf{AI Assigned Date:} 1839–1846\\
\textbf{Reason:} These meetings share a thematic connection to "public meeting" by demonstrating how public gatherings have been pivotal in shaping discourse around economic reform, similar to those discussed in the 1860 debates.

\textbf{Event: Hyde Park Demonstrations}\\
\textbf{AI Assigned Date:} 1855–1860\\
\textbf{Reason:} Hyde Park demonstrations resonate with "public meeting" and "recent meeting," showing how such gatherings functioned as platforms for political and social advocacy, reflecting the period's reliance on public discourse for progress.

\textbf{Event: Meetings on the Reform Act of 1832}\\
\textbf{AI Assigned Date:} 1831–1832\\
\textbf{Reason:} These meetings are akin to "yorkshire meeting" and "clontarf meeting," showcasing historical instances where regional or national public assemblies catalyzed significant legislative reforms, serving as a possible reference point in 1860.

\textbf{Event: Fenian Meetings}\\
\textbf{AI Assigned Date:} 1858–1860\\
\textbf{Reason:} While slightly later, Fenian meetings parallel "clontarf meeting" by highlighting Irish political concerns and illustrating how public gatherings were used to address issues of representation and governance.
\end{tcolorbox}

Now that we have a more specific list of events that aligns with our interests, we can use them in our analysis. To use our new events, we will create a data frame for them. We can prompt the chatbot to return the needed code. Given that we are very familiar with creating data frames at this point, we can also proof the chatbot's results and ensure they match our desired output. 

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Analyst Prompt]
Return this text as a data frame in R with two columns: a column for the event, named event\_name and a column for the date, named AI Assigned Date. Write the events in lower case. 
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=ChatGPT-4o Response]
Here is the R code to create the requested data frame:
  
\begin{verbatim}
# Create the data frame

events_df <- data.frame(
  event_name = c(
    "crimean war",
    "russo-turkish war",
    "congress of vienna",
    "siege of acre",
    "battle of navarino",
    "schleswig-holstein wars"
  ),
  AI_Assigned_Date = c(
    "1853–1856",
    "1828–1829",
    "1814–1815",
    "1799",
    "1827",
    "1848–1852"
  )
)

# Print the data frame
print(events_data)
\end{verbatim}

This will create a data frame in R with two columns: event\_name (containing the event names in lowercase) and AI Assigned Date.
\end{tcolorbox}

We can now test the chatbot's responses by running the code. 

```{r}
# Create the data frame
events_df <- data.frame(
  event_name = c(
    "crimean war",
    "russo-turkish war",
    "congress of vienna",
    "siege of acre",
    "battle of navarino",
    "schleswig-holstein wars"
  ),
  AI_Assigned_Date = c(
    "1853–1856",
    "1828–1829",
    "1814–1815",
    "1799",
    "1827",
    "1848–1852"
  )
)

# Print the data frame
print(events_df)

```


```{r}
data("hansard_1860")
data("debate_metadata_1860")

hansard_1860 <- hansard_1860 %>% 
  left_join(debate_metadata_1860)
```

The following code filters the `hansard_1860` dataset to include rows where the `text` column matches any event name in `events_df$event_name`, using case-insensitive matching. It then adds a new column, `matched_event`, which contains the specific matched event name from the `text`. The result is a filtered data frame with an additional column identifying the matched event for each row.

```{r}
# Match with each event and 
# add a column to capture the matched event
new_matched_events <- hansard_1860 %>%
  filter(str_detect(text,
                    regex(paste0("\\b(", paste(events_df$event_name, collapse = "|"), ")\\b"), 
                          ignore_case = TRUE))) %>%
  mutate(matched_event = str_extract(text,
                                     regex(paste0("\\b(", paste(events_df$event_name, collapse = "|"), ")\\b"), 
                                           ignore_case = TRUE)))
```

```{r, results='asis', warning=FALSE, message=FALSE, echo=FALSE}
# Show top 5 rows
df_head <- new_matched_events %>% 
  slice_head(n = 5)

# Print table with fixed position and word wrap
kbl(df_head, format = "latex", 
    booktabs = TRUE, 
    linesep = "",
    caption = "Excerpt from Hansard 1850") %>%
  kable_styling(latex_options = c("scale_down", "repeat_header"),
                table.envir = FALSE) %>%
  column_spec(which(colnames(df_head) == "text"), 
              latex_column_spec = "p{12cm}", 
              latex_valign = "top")
```

The results show that not all of the events were mentioned word-for-word in the Hansard corpus; however, we successfully matched several, including the Crimean War, Congress of Vienna, and Battle of Navarino. While an AI chatbot can suggest a wider range of related terms to search for--and these terms might spark our curiosity--those terms may not actually appear in the corpus. For experts 19th-century history, it might be expected that some phrases would not be found, but for others, this process can offer a useful way to explore which events were and were not mentioned in the data. This kind of exploration is part of an ongoing, iterative process—where analysts refine their searches by adding or adjusting keywords, either manually or with AI. 

It is also important to note that the AI chatbot did not automatically fix formatting issues in the text, such as inconsistent capitalization. To obtain more reliable and visually meaningful results, the analyst may want to take extra processing steps to clean the data—such as making all text lowercase and fixing other formatting problems—before analysis.

```{r}
event_counts <- new_matched_events %>%
  count(matched_event, name = "count") %>%
  arrange(desc(count))

print(event_counts)
```


# Conclusion 

In this chapter, we operationalized iteration as a core method in digital humanities research by integrating historical theory, code, exploratory data analysis, and artificial intelligence into a single, cyclical workflow. Rather than treating AI as a replacement for scholarly interpretation, we positioned generative AI-based tools as participants in an iterative process that requires repeated inspection of data, returns to corpus context, and critical evaluation of AI-generated outputs.

By pairing iteration with exploratory data analysis, we demonstrated how high-level diagnostic exploration can inform subsequent analytic decisions, and how provisional findings can guide more focused investigations. Throughout the chapter, AI was used to support this process—assisting with brainstorming, proposing preprocessing strategies, and accelerating iteration—while interpretive judgment remained grounded in close reading and historical expertise.

The chapters that follow build on this foundation by introducing more advanced computational techniques and by examining, in greater depth, how generative AI reshapes iterative research practices in the humanities. A central question we pursue is whether—and under what conditions—current AI chatbots can meaningfully contribute to the interpretation of nineteenth-century Parliamentary debates, and how scholars might critically assess those contributions.

While generative AI offers tools for exploratory analysis and iteration, it also introduces new methodological and epistemic risks. Accordingly, we will continue to examine the limitations of AI-assisted approaches to computational text analysis, paying particular attention to the moments when AI-generated suggestions must be interrogated, revised, or rejected in order to sustain historically grounded and responsible interpretation.

New citations for cabot: 

Tabak, Edin. 2017. “A Hybrid Model for Managing Digital Humanities Projects.” Digital Humanities Quarterly 11 (1).
Roopika risam new digital worlds 