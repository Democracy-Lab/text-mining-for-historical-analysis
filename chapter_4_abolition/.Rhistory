knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input)) }
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://data.cityofnewyork.us/resource/wewp-mm3p.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
service_requests <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
# Show just the first five columns
head(service_requests[, 1:5])
json_list <- fromJSON(json_content)
subset_list <- json_list[1:2, ]
cat(prettify(toJSON(subset_list, auto_unbox = TRUE)))
nrow(json_list)
url <- "https://data.cityofnewyork.us/resource/wewp-mm3p.json"
# Pagination settings
limit <- 1000
# Will store the output from each iteration
service_request_data <- tibble()
# Run for two iterations: first with offset = 0, then with offset = 1000.
# This means the code will first collect rows 1–1000,
# and then collect rows 1001–2000 on the second iteration.
for (i in 0:1) {
offset <- i * limit
# Make paginated request
response <- GET(url, query = list(`$limit` = limit, `$offset` = offset))
if (status_code(response) != 200) {
stop("Request failed. Status code: ", status_code(response)) }
# Parse the response
data_subset <- fromJSON(content(response, "text"), flatten = TRUE)
# Save collected data
service_requests <- bind_rows(service_requests, as_tibble(data_subset)) }
library("usdoj")
blog_entries <- doj_blog_entries(n_results = 10, search_direction = "DESC")
head(blog_entries$teaser, 5)
library("tidyverse")
library("tidytext")
# Tokenize, remove stop words, and count word frequency
top_words <- blog_entries %>%
select(body) %>%
unnest_tokens(word, body) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 10)
# Plot the top words
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top 10 Words in DOJ Blog Entries",
x = "Word",
y = "Frequency") +
theme_minimal()
library("rvest")
library("tidyverse")
library("xml2")
rss_url <- "https://feeds.bbci.co.uk/news/rss.xml"
rss_page <- read_xml(rss_url)
articles <- tibble(
title = rss_page %>%
xml_find_all("//item/title") %>%
xml_text(),
link = rss_page %>%
xml_find_all("//item/link") %>%
xml_text())
print(head(articles, 5))
library("httr")
library("fs")
library("pdftools")
# Define the URL with the PDF to be downloaded
url <- "https://sagadb.org/files/pdf/egils_saga.en.pdf"
# Define the directory it will be downloaded to on one's computer
# The file will be downloaded to folder named "tmha_data"
destfile <- path("tmha_data", "egils_saga.en.pdf")
# Create directory if it doesn't exist
dir_create(path_dir(destfile))
# Download the PDF file
GET(url, write_disk(destfile, overwrite = TRUE))
# Extract text from the PDF
pdf_text_raw <- pdf_text(destfile)
# Convert into a tibble with one row per page
tidy_egils_saga <- tibble(page = seq_along(pdf_text_raw),
text = pdf_text_raw)
tidy_egils_saga %>%
slice(50:60)
library("tidyverse")
library("tidytext")
data(stop_words)
# Unnest tokens, remove stop words, count words, and select the top 20
top_words <- tidy_egils_saga %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 20)
# Visualize top 20 words
top_words %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col() +
coord_flip() +
labs(title = "Top 20 Words in Egil's Saga (after removing stop words)",
x = "Word",
y = "Count") +
theme_minimal()
tidy_egils_saga_with_metadata <- tidy_egils_saga %>%
mutate(author = "Snorri Sturluson",
book = "Egil's Saga",
date = "1893")
library("gt")
gt(head(tidy_egils_saga_with_metadata))
url <- "https://github.com/stephbuon/text-mining-for-historical-analysis"
page <- read_html(url)
# Convert to character (raw HTML), split into lines, and preview a few lines
html_preview <- page %>%
as.character() %>%
str_split("\n") %>%
unlist() %>%
str_trim() %>%
.[1:5]
# Optionally truncate long lines to fit the page
html_preview <- str_trunc(html_preview, width = 100)
# Print nicely
cat(paste(html_preview, collapse = "\n"))
# Step 1: Read the webpage
url <- "https://example.com"
page <- read_html(url)
# Step 2: Select and extract all <p> tags using a CSS selector
paragraphs <- page %>%
html_elements("p") %>%      # CSS selector
html_text(trim = TRUE)      # extract clean text
print(paragraphs)
library("hansardr")
data("speaker_metadata_1840")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0051P0_728")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0051P0_18193")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194")
speaker_metadata_1840 %>%
filter(sentence_id == "S3V0060P0_2194") %>%
mutate(speaker = str_replace_all(speaker, "'", ""))
