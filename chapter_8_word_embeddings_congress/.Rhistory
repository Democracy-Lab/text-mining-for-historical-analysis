knitr::opts_chunk$set(echo = TRUE)
# ---- Libraries ----
library("text2vec")
library("dplyr")
library("tibble")
# ---- Helper: load embeddings as a matrix with tokens as rownames ----
load_embeddings <- function(path) {
# If the first line is "N D" (e.g., word2vec text), skip it
first_line <- tryCatch(readLines(path, n = 1), error = function(e) "")
skip <- if (grepl("^\\s*\\d+\\s+\\d+\\s*$", first_line)) 1 else 0
# Read as a data.frame: first column = token (rownames), rest = numeric dims
df <- read.table(
file = path,
header = FALSE,
row.names = 1,
check.names = FALSE,
stringsAsFactors = FALSE,
as.is = TRUE,
skip = skip
)
# Coerce to numeric (protects against any stray non-numeric parsing)
df[] <- lapply(df, function(col) suppressWarnings(as.numeric(col)))
as.matrix(df)
}
# ---- Paths (adjust if needed) ----
path_2001 <- "~/Downloads/word_embeddings/congress_climate_2001_2001.txt"
path_2021 <- "~/Downloads/word_embeddings/congress_climate_2021_2021.txt"
# ---- Load matrices ----
congress_climate_1 <- load_embeddings(path_2001)  # rows = tokens, cols = dims
library("text2vec")
library("dplyr")
library("tibble")
#---------------------------------------------------
# 1. Sample text (replace with your own corpus)
#---------------------------------------------------
text <- c(
"Climate change is a global challenge for humanity.",
"Government policies on climate mitigation vary widely.",
"Environmental issues are increasingly central to public debate.",
"The climate crisis requires bold and sustained action."
)
#---------------------------------------------------
# 2. Tokenize
#---------------------------------------------------
tokens <- text %>%
tolower() %>%
word_tokenizer()
#---------------------------------------------------
# 3. Create vocabulary
#---------------------------------------------------
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
# Optional: prune vocabulary
vocab <- prune_vocabulary(vocab, term_count_min = 1)
#---------------------------------------------------
# 4. Vectorize into term-co-occurrence matrix (TCM)
#---------------------------------------------------
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
#---------------------------------------------------
# 5. Train a word2vec-like model (GloVe approximation is common)
#---------------------------------------------------
glove <- GlobalVectors$new(rank = 50, x_max = 10)
wv_main <- glove$fit_transform(tcm, n_iter = 30)
# GloVe returns main + context embeddings; add them
wv_context <- glove$components
word_embeddings <- wv_main + t(wv_context)
#---------------------------------------------------
# 6. Inspect embeddings
#---------------------------------------------------
dim(word_embeddings)  # rows = words, cols = embedding dims
head(word_embeddings)
#---------------------------------------------------
# 7. Find most similar words
#---------------------------------------------------
keyword <- "climate"
kw_vec <- word_embeddings[keyword, , drop = FALSE]
cos_sim <- sim2(word_embeddings, kw_vec, method = "cosine", norm = "l2")[,1]
tibble(
word = names(cos_sim),
similarity = cos_sim
) %>%
filter(word != keyword) %>%
arrange(desc(similarity)) %>%
slice_head(n = 10)
